### Prerequisites
1. k get events
    ![get events](../images/debug/get-events.png "get events")
    å€Ÿç”±k get eventsæŒ‡ä»¤æª¢æŸ¥æ˜¯å¦æœ‰Warningçš„events, ä¸¦ä¸”æŸ¥çœ‹warning reason

    Q: You received reports of recent issues in the cluster. Investigate by using the appropriate command to view the events happening in the cluster. What is the most recent event?

    option1: RegisterdNode
    option2: Failed
    option3: NodeAllocatebleEnforced
    option4: InvalidDiskCapacity

    controlplane ~ âœ  k get events
    LAST SEEN   TYPE      REASON                    OBJECT              MESSAGE
    56m         Normal    NodeHasSufficientMemory   node/controlplane   Node controlplane status is now: NodeHasSufficientMemory
    56m         Normal    NodeHasNoDiskPressure     node/controlplane   Node controlplane status is now: NodeHasNoDiskPressure
    56m         Normal    NodeHasSufficientPID      node/controlplane   Node controlplane status is now: NodeHasSufficientPID
    56m         Normal    NodeAllocatableEnforced   node/controlplane   Updated Node Allocatable limit across pods
    56m         Normal    Starting                  node/controlplane   Starting kubelet.
    ...
    33s (x4 over 114s)   Warning   Failed                    Pod/webserver       Error: ErrImagePull
    8s (x6 over 113s)    Normal    BackOff                   Pod/webserver       Back-off pulling image "busybox1"
    8s (x6 over 113s)    Warning   Failed                    Pod/webserver       Error: ImagePullBackOff
    ...

    OR:
    controlplane ~ âœ  k get events --sort-by=.metadata.creationTimestamp| tail -n 1
    2m22s       Warning   Failed      pod/webserver   Error: ImagePullBackOff

    OR:
    controlplane ~ âœ  **k get events | tail -n 1**
    2m31s       Warning   Failed      pod/webserver   Error: ImagePullBackOff

    Answer: option2 Failed

2. k port-forward


3. k auth can-i
    ![auth whoami](../images/debug/auth-whoami.png "auth whoami")

    ç”±æ–¼æœªæŒ‡å®šç•¶å‰çš„userç‚ºä½•ï¼Œä½¿ç”¨auth can-i å»é¡¯ç¤ºyes, å‰‡é€ék auth whoamiæŒ‡ä»¤æŸ¥çœ‹ç•¶ä¸‹æ¸¬è©¦çš„user

    ![auth sa](../images/debug/auth-serviceaccount.png "auth sa")
    --as æŒ‡å®šserviceaccount
    k auth can-i get pods **--as=system:serviceaccount:default:default**


4. k top: (åƒ…é™æ–¼ç”¨åœ¨podsè·Ÿnodes)
   ![k top](../images/debug/top.png "k top")

5. k expain --recursive: åŠ ä¸Šrecursiveåƒæ•¸ï¼Œç›´æ¥å±•é–‹æ‰€æœ‰å­—æ®µ(é€™å°±ä¸ç”¨k explainä¸€è·¯æŸ¥æ‰¾è‡³æœ€åº•å±¤å­—æ®µ)
    ![k explain with recursive](../images/debug/explain.png "k explain with recursive")

    You just learned thereâ€™s a lifecycle spec defined in one of the pod manifests. Use the right command to learn more about that lifecycle spec. Write the output out to a file lifecycle.txt
    
    Hint: lifecycle spec is under containers

    controlplane ~ âœ  kubectl explain pod.spec.containers.lifecycle > lifecycle.txt

    controlplane ~ âœ  cat lifecycle.txt 
    KIND:       Pod
    VERSION:    v1

    FIELD: lifecycle <Lifecycle>


    DESCRIPTION:
    ...

    controlplane ~ âœ  kubectl explain pod.spec.containers.lifecycle > lifecycle.txt

6. k diff: æª¢æŸ¥æœ¬åœ°ç¾æœ‰çš„yamlé…ç½®æ–‡ä»¶èˆ‡å·²å»ºç«‹çš„Resourceä¹‹é–“çš„å·®ç•°
    æ­¤å‘½ä»¤æœƒæ¯”è¼ƒ my-deployment.yaml æ–‡ä»¶ä¸­çš„è³‡æºå’Œé›†ç¾¤ä¸­çš„ç¾æœ‰è³‡æºï¼Œä¸¦è¼¸å‡ºå·®ç•°
    ![k diff](../images/debug/diff.png "k diff")

    You've made changes to the deployment configuration in the nginx-deployment.yml file, but you want to review the differences before applying them. Use the command to show the difference between local and cluster configurations. What has changed?

    option1: replicas
    option2: labels

    controlplane ~ âœ  k diff -f nginx-deployment.yml 
    diff -u -N /tmp/LIVE-1547354458/apps.v1.Deployment.app.nginx-deployment /tmp/MERGED-679631904/apps.v1.Deployment.app.nginx-deployment
    --- /tmp/LIVE-1547354458/apps.v1.Deployment.app.nginx-deployment        2025-03-08 05:22:15.080460346 +0000
    +++ /tmp/MERGED-679631904/apps.v1.Deployment.app.nginx-deployment       2025-03-08 05:22:15.080460346 +0000
    @@ -6,7 +6,7 @@
        kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"nginx"},"name":"nginx-deployment","namespace":"app"},"spec":{"replicas":3,"selector":{"matchLabels":{"app":"nginx"}},"template":{"metadata":{"labels":{"app":"nginx"}},"spec":{"containers":[{"image":"nginx:1.14.2","name":"nginx","ports":[{"containerPort":80}]}]}}}}
    creationTimestamp: "2025-03-08T05:21:24Z"
    -  generation: 1
    +  generation: 2
    labels:
        app: nginx
    name: nginx-deployment
    @@ -15,7 +15,7 @@
    uid: e17ef8e6-3d54-476d-a15f-cb7de79a4ce3
    spec:
    progressDeadlineSeconds: 600
    -  replicas: 3
    +  replicas: 2
    revisionHistoryLimit: 10
    selector:
        matchLabels:

    Answer: option1

7. How many pods are one the node01?

    controlplane ~ âœ  k get nodes
    NAME           STATUS   ROLES           AGE   VERSION
    controlplane   Ready    control-plane   69m   v1.32.0
    node01         Ready    <none>          68m   v1.32.0

    controlplane ~ âœ  ssh node01
    Welcome to Ubuntu 22.04.4 LTS (GNU/Linux 5.4.0-1106-gcp x86_64)

    * Documentation:  https://help.ubuntu.com
    * Management:     https://landscape.canonical.com
    * Support:        https://ubuntu.com/pro

    This system has been minimized by removing packages and content that are
    not required on a system that users do not log into.

    To restore this content, you can run the 'unminimize' command.

    node01 ~ âœ  k get pods -A
    E0308 05:24:04.427912   19038 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: the server could not find the requested resource"
    E0308 05:24:04.430511   19038 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: the server could not find the requested resource"
    E0308 05:24:04.432887   19038 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: the server could not find the requested resource"
    E0308 05:24:04.435062   19038 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: the server could not find the requested resource"
    E0308 05:24:04.438207   19038 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: the server could not find the requested resource"
    Error from server (NotFound): the server could not find the requested resource

    æ”¹åŸ·è¡Œ:
    node01 ~ âœ– exit
    logout
    Connection to node01 closed.

    controlplane ~ âœ  k get pod -A **-o wide --field-selector spec.nodeName=node01**
    NAMESPACE     NAME                                READY   STATUS             RESTARTS   AGE     IP             NODE     NOMINATED NODE   READINESS GATES
    app           myapp                               0/1     Completed          0          9m40s   10.244.192.4   node01   <none>           <none>
    app           nginx-deployment-647677fc66-4mxzz   1/1     Running            0          5m41s   10.244.192.4   node01   <none>           <none>
    app           nginx-deployment-647677fc66-k4dwd   1/1     Running            0          5m41s   10.244.192.5   node01   <none>           <none>
    app           nginx-deployment-647677fc66-zgtt9   1/1     Running            0          5m41s   10.244.192.6   node01   <none>           <none>
    app           python-app                          1/1     Running            0          9m41s   10.244.192.3   node01   <none>           <none>
    default       backend                             1/1     Running            0          9m41s   10.244.192.2   node01   <none>           <none>
    default       webserver                           0/1     ImagePullBackOff   0          18m     10.244.192.1   node01   <none>           <none>
    kube-system   kube-proxy-vkdxd                    1/1     Running            0          71m     192.2.122.3    node01   <none>           <none>
    kube-system   weave-net-hbjxz                     2/2     Running            0          71m     192.2.122.3    node01   <none>           <none>

8. You are investigating an incident that happened on the pod called myapp and need to ensure logs display timestamps for accurate analysis. Write the logs to a file timestamps.txt


    Deployment can be in any of the namespaces

    controlplane ~ âœ  k logs -n app myapp --timestamps > timestamps.txt

    controlplane ~ âœ  cat timestamps.txt 
    2025-03-08T05:17:32.679262501Z map[id:2 message:message 2]
    2025-03-08T05:17:32.779410834Z map[id:1 message:message 1]
    2025-03-08T05:17:32.879673388Z map[id:2 message:message 2]
    2025-03-08T05:17:32.979954690Z map[id:1 message:message 1]
    2025-03-08T05:17:33.079151695Z map[id:3 message:message 3]
    2025-03-08T05:17:33.179393468Z map[id:1 message:message 1]
    2025-03-08T05:17:33.279638912Z map[id:3 message:message 3]
    2025-03-08T05:17:33.379843720Z map[id:2 message:message 2]
    2025-03-08T05:17:33.479028037Z map[id:2 message:message 2]
    2025-03-08T05:17:33.579274593Z map[id:1 message:message 1]
    2025-03-08T05:17:33.679566556Z map[id:2 message:message 2]
    2025-03-08T05:17:33.779797509Z map[id:3 message:message 3]
    2025-03-08T05:17:33.880039911Z map[id:3 message:message 3]
    2025-03-08T05:17:33.979322062Z map[id:1 message:message 1]
    2025-03-08T05:17:34.079547541Z map[id:3 message:message 3]

9. ä½¿ç”¨k debugæ›¿ä»£ k execï¼ŒåŸå› ç‚º:   (**k exec é©ç”¨æ–¼å·²é‹è¡Œçš„ Pod; k debug ä¸»è¦ç”¨æ–¼å•é¡Œæ’æŸ¥**)
    A. æ¸›å°‘ Pod ä¸­æ–· (minimize pod disruptions)
    kubectl exec ç›´æ¥åœ¨ç¾æœ‰çš„ Pod å…§åŸ·è¡Œå‘½ä»¤ï¼Œç•¶ä½ é€²è¡Œèª¿è©¦æ™‚ï¼Œå¦‚æœæ“ä½œä¸æ…ï¼ˆä¾‹å¦‚åˆªé™¤æ–‡ä»¶ã€ä¿®æ”¹è¨­å®šæˆ–å½±éŸ¿é‹è¡Œçš„é€²ç¨‹ï¼‰ï¼Œå¯èƒ½æœƒå°è‡´ Pod å´©æ½°ï¼Œå½±éŸ¿æ¥­å‹™é‹è¡Œã€‚
    kubectl debug å‰‡å¯ä»¥å‰µå»ºä¸€å€‹æ–°çš„è‡¨æ™‚ Debug å®¹å™¨ï¼Œä¸å½±éŸ¿åŸæœ¬çš„æ‡‰ç”¨ã€‚ä¾‹å¦‚ï¼š
        
        kubectl debug pod/my-pod -it --image=busybox --target=my-container
    
    é€™æ¨£æœƒåœ¨åŸæœ‰çš„ Pod å…§éƒ¨æ–°å¢ä¸€å€‹ busybox å®¹å™¨ï¼Œä½†ä¸æœƒå½±éŸ¿ my-container çš„é‹è¡Œï¼Œç¢ºä¿æ‡‰ç”¨ä¸ä¸­æ–·ã€‚
    èª¿è©¦å®Œç•¢å¾Œï¼Œå¯ä»¥åˆªé™¤ Debug å®¹å™¨ï¼Œè€Œä¸å½±éŸ¿åŸæœ¬çš„ Podã€‚
    âœ… é©ç”¨å ´æ™¯

    éœ€è¦æ·±å…¥åˆ†æä½†ä¸æƒ³å½±éŸ¿æ‡‰ç”¨é‹è¡Œçš„æƒ…å¢ƒï¼Œå¦‚èª¿è©¦ç•°å¸¸æµé‡ã€è¨˜éŒ„åˆ†æã€æ¸¬è©¦æŒ‡ä»¤ç­‰ã€‚

    B. Distroless æ˜ åƒ (distroless images)
    ä»€éº¼æ˜¯ Distroless?

    **Distroless æ˜¯ä¸€ç¨®æ¥µç°¡åŒ–çš„å®¹å™¨æ˜ åƒ**ï¼Œå®ƒä¸åŒ…å« bashã€shã€apt æˆ–å…¶ä»–å¸¸è¦‹çš„å·¥å…·ï¼Œåƒ…åŒ…å«æ‡‰ç”¨æ‰€éœ€çš„æœ€å°‘çµ„ä»¶ï¼Œé€™æœ‰åŠ©æ–¼æé«˜å®‰å…¨æ€§å’Œæ¸›å°‘æ”»æ“Šé¢ã€‚
    ä¾‹å¦‚ï¼Œå®˜æ–¹çš„ gcr.io/distroless/base æˆ– gcr.io/distroless/static æ˜ åƒæ²’æœ‰ shellï¼Œä¸èƒ½ç›´æ¥ kubectl exec -it my-pod -- sh é€²å…¥å®¹å™¨ã€‚
    ç‚ºä½• kubectl debug æ›´å¥½ï¼Ÿ

    kubectl exec å¤±æ•ˆï¼š åœ¨ Distroless å®¹å™¨ä¸­ï¼Œç”±æ–¼æ²’æœ‰ shellï¼ŒåŸ·è¡Œä»¥ä¸‹æŒ‡ä»¤å¯èƒ½æœƒå¤±æ•—ï¼š
    
        kubectl exec -it my-pod -- sh
    
    æœƒè¿”å›é¡ä¼¼ exec /bin/sh: no such file or directory çš„éŒ¯èª¤ã€‚
    kubectl debug æä¾›å®Œæ•´å·¥å…·ï¼š ä½ å¯ä»¥ç”¨ kubectl debug æ·»åŠ ä¸€å€‹å¸¶æœ‰ bash æˆ– sh çš„èª¿è©¦å®¹å™¨ï¼Œä¾‹å¦‚ï¼š
        
        kubectl debug pod/my-pod -it --image=ubuntu
    
    é€™æ¨£å³ä½¿åŸæœ¬çš„ Pod æ˜¯ Distrolessï¼Œèª¿è©¦å®¹å™¨ä»ç„¶æœ‰å®Œæ•´çš„ Linux ç’°å¢ƒï¼Œå¯ç”¨ä¾†è¨ºæ–·å•é¡Œã€‚
    âœ… é©ç”¨å ´æ™¯

    ç•¶ Pod ä½¿ç”¨ Distroless æˆ–æ¥µç°¡åŒ–æ˜ åƒæ™‚ï¼Œkubectl exec å¯èƒ½ç„¡æ³•åŸ·è¡Œï¼Œä½† kubectl debug å¯å¹«åŠ©ä½ æ·»åŠ ä¸€å€‹å®Œæ•´çš„èª¿è©¦ç’°å¢ƒã€‚


    ![k debug](../images/debug/debug.png "k debug")
    ![k debug](../images/debug/debug02.png "k debug")


    1ï¸âƒ£ kubectl exec é©ç”¨æ–¼å·²é‹è¡Œçš„ Pod
    kubectl exec ç”¨æ–¼åŸ·è¡Œå‘½ä»¤åˆ° æ­£åœ¨é‹è¡Œä¸­çš„å®¹å™¨ï¼Œé€™é©ç”¨æ–¼ï¼š

    éœ€è¦åœ¨å®¹å™¨å…§åŸ·è¡ŒæŸå€‹æŒ‡ä»¤ï¼Œå¦‚ /bin/bashã€shã€lsã€cat /var/log/...ã€‚
    éœ€è¦å­˜å–æ‡‰ç”¨ç¨‹å¼å…§éƒ¨çš„æ—¥èªŒæˆ–ç’°å¢ƒï¼Œè€Œä¸éœ€è¦é¡å¤–æ–°å¢ Debug å®¹å™¨ã€‚
    
    ç¯„ä¾‹ï¼š
    kubectl exec -it webapp -- /bin/bash
    é€™è¡ŒæŒ‡ä»¤çš„ç›®çš„æ˜¯å˜—è©¦é€²å…¥ webapp Pod å…§çš„å®¹å™¨ï¼ŒåŸ·è¡Œ /bin/bash æŒ‡ä»¤ã€‚

    2ï¸âƒ£ kubectl debug ä¸»è¦ç”¨æ–¼å•é¡Œæ’æŸ¥
    kubectl debug çš„ç”¨é€”ï¼š

    ç”¨æ–¼ç•¶ kubectl exec ä¸èƒ½è§£æ±ºå•é¡Œæ™‚ï¼Œä¾‹å¦‚ï¼š
    å®¹å™¨ç¼ºå°‘ shellï¼ˆé€™é¡Œçš„éŒ¯èª¤å°±æ˜¯ /bin/bash æ‰¾ä¸åˆ°ï¼‰ã€‚
    **å®¹å™¨å´©æ½° (CrashLoopBackOff)ï¼Œå°è‡´ kubectl exec ç„¡æ³•ä½¿ç”¨**ã€‚
    å®¹å™¨å½±åƒæ˜¯ distrolessï¼Œç¼ºå°‘ debug å·¥å…·ï¼Œå¦‚ curlã€psã€netstatã€‚
    æƒ³è¦ä½¿ç”¨ Ephemeral Container ä¾†é™„åŠ é¡å¤–çš„åµéŒ¯å·¥å…·ã€‚
    
    å¦‚æœé€™é¡Œå…è¨±ä½¿ç”¨ kubectl debugï¼Œå¯ä»¥ç”¨ï¼š
    kubectl debug -it webapp --image=busybox --target=webapp
    
    é€™æœƒé™„åŠ ä¸€å€‹æ–°çš„ ephemeral debug containerï¼Œä¸¦ä¸”åŒ…å« shellï¼Œè®“æˆ‘å€‘èƒ½å¤ é€²è¡Œé€²ä¸€æ­¥çš„åµéŒ¯ã€‚



    Which of the following actions can be performed using kubectl debug? Choose all valid answers:

    A. Running a new pod in the nodeâ€™s host namespaces with access to its filesystem.
    B. Attaching a container to a running pod without restarting the pod.
    C. Copy files from a local machine to a pod.
    D. Create a new pod with a modified container image for debugging.

    æ­£ç¢ºç­”æ¡ˆæ˜¯ **Aã€B å’Œ D**ï¼Œä»¥ä¸‹æ˜¯å„é¸é …çš„è§£é‡‹ï¼Œä»¥åŠç‚ºä»€éº¼ **C é¸é …ä¸æ­£ç¢º**ï¼š


    /### **A) åœ¨ç¯€é»çš„ä¸»æ©Ÿå‘½åç©ºé–“ (host namespaces) ä¸­é‹è¡Œä¸€å€‹æ–°çš„ Podï¼Œä¸¦èƒ½è¨ªå•å…¶æ–‡ä»¶ç³»çµ±ã€‚âœ…**
    - **kubectl debug** å¯ç”¨ä¾†åœ¨ **ç¯€é»çš„ä¸»æ©Ÿå‘½åç©ºé–“** ä¸­å•Ÿå‹•ä¸€å€‹æ–°çš„åµéŒ¯ Podï¼Œé€™å°æ–¼æ’æŸ¥ç¯€é»ç´šåˆ¥çš„å•é¡Œéå¸¸æœ‰ç”¨ã€‚
    - å¯ä»¥ä½¿ç”¨ `--hostpid`ã€`--hostnetwork` å’Œ `--hostipc` ä¾†è®“åµéŒ¯å®¹å™¨é‹è¡Œæ–¼ä¸»æ©Ÿå‘½åç©ºé–“ã€‚
    - **ç¤ºä¾‹**ï¼š
    ```sh
    kubectl debug node/<node-name> -it --image=busybox --hostpid --hostnetwork
    ```


    /### **B) åœ¨ä¸é‡å•Ÿ Pod çš„æƒ…æ³ä¸‹ï¼Œé™„åŠ ä¸€å€‹å®¹å™¨åˆ°æ­£åœ¨é‹è¡Œçš„ Podã€‚âœ…**
    - **kubectl debug** å…è¨±åœ¨ **ä¸é‡æ–°å•Ÿå‹• Pod** çš„æƒ…æ³ä¸‹ï¼Œ**æ–°å¢ä¸€å€‹è‡¨æ™‚çš„åµéŒ¯å®¹å™¨**ã€‚
    - é€™åœ¨åŸæœ¬çš„å®¹å™¨ä¸­ç¼ºå°‘ä¸€äº›åµéŒ¯å·¥å…·æ™‚ç‰¹åˆ¥æœ‰ç”¨ã€‚
    - **ç¤ºä¾‹**ï¼š
    ```sh
    kubectl debug -it <pod-name> --image=busybox --target=<container-name>
    ```
    - `--target=<container-name>` åƒæ•¸è®“åµéŒ¯å®¹å™¨é‹è¡Œæ–¼ç›¸åŒçš„ç¶²è·¯èˆ‡å‘½åç©ºé–“ã€‚

    /### **C) å¾æœ¬åœ°æ©Ÿå™¨è¤‡è£½æ–‡ä»¶åˆ° Podã€‚âŒï¼ˆéŒ¯èª¤ï¼‰**
    - **kubectl debug ç„¡æ³•ç”¨ä¾†åœ¨æœ¬åœ°æ©Ÿå™¨èˆ‡ Pod ä¹‹é–“å‚³è¼¸æ–‡ä»¶**ã€‚
    - é€™é …æ“ä½œæ‡‰è©²ä½¿ç”¨ **kubectl cp** å‘½ä»¤ä¾†å®Œæˆã€‚
    - **æ­£ç¢ºçš„æ–‡ä»¶è¤‡è£½æ–¹å¼**ï¼š
    ```sh
    kubectl cp /local/path my-pod:/container/path
    ```
    - `kubectl debug` çš„ç”¨é€”æ˜¯ **åµéŒ¯èˆ‡è¨ºæ–· Pod**ï¼Œè€Œä¸æ˜¯æ–‡ä»¶å‚³è¼¸ï¼Œå› æ­¤ `kubectl cp` æ‰æ˜¯æ­£ç¢ºçš„é¸æ“‡ã€‚


    /### **D) å‰µå»ºä¸€å€‹å¸¶æœ‰ä¿®æ”¹å¾Œå®¹å™¨æ˜ åƒ (image) çš„æ–° Pod ä¾†é€²è¡ŒåµéŒ¯ã€‚âœ…**
    - **kubectl debug** å…è¨±åŸºæ–¼åŸæœ¬çš„ Pod **å‰µå»ºä¸€å€‹æ–° Podï¼Œä¸¦ä½¿ç”¨ä¸åŒçš„é¡åƒä¾†åµéŒ¯**ã€‚
    - é€™å°æ–¼éœ€è¦ä½¿ç”¨å¸¶æœ‰è¨ºæ–·å·¥å…·çš„æ˜ åƒä¾†æ’æŸ¥å•é¡Œæ™‚ç‰¹åˆ¥æœ‰ç”¨ã€‚
    - **ç¤ºä¾‹**ï¼š
    ```sh
    kubectl debug --image=busybox --copy-to=my-debug-pod my-existing-pod
    ```
    - `--copy-to` åƒæ•¸è®“æ–°çš„ Pod åŸºæ–¼åŸ Pod å‰µå»ºï¼Œä½†å…è¨±ä¿®æ”¹ï¼Œä¾‹å¦‚æ›´æ›å®¹å™¨æ˜ åƒã€‚


    Answer: A,D



10. Pod å¿…é ˆè™•æ–¼ Running ç‹€æ…‹ï¼Œæ‰èƒ½ä½¿ç”¨ Ephemeral containers!
    
    When are ephemeral(çŸ­æš«çš„) containers useful?

    A) Container is starting up
    B) When using distroless images
    C) Debugging a pending pod
    D) Images doesnâ€™t contain debugging utilities
    E) Debugging container in a crashloop

    ä»€éº¼æ˜¯ Ephemeral Containersï¼ˆçŸ­æš«å®¹å™¨ï¼‰ï¼Ÿ
    Ephemeral containers æ˜¯ Kubernetes å…è¨±ç”¨ä¾† æš«æ™‚ é™„åŠ åˆ°ä¸€å€‹å·²ç¶“é‹è¡Œçš„ Pod å…§çš„ç‰¹æ®Šé¡å‹å®¹å™¨ï¼Œä¸»è¦ç”¨æ–¼ åµéŒ¯ã€‚
    ç‰¹é»ï¼š
    **ä¸èƒ½ å®šç¾© resourcesï¼ˆCPUã€å…§å­˜é™åˆ¶ï¼‰**ã€‚
    **ä¸èƒ½ åƒèˆ‡ PodSpecï¼Œå³ä¸æœƒå½±éŸ¿ Pod æ­£å¸¸é‹è¡Œçš„å®¹å™¨é…ç½®**ã€‚
    **ä¸æœƒè‡ªå‹•é‡å•Ÿï¼Œç•¶å®ƒçµæŸæ™‚å°±æœƒè¢«åˆªé™¤**ã€‚

    A) Container is starting upï¼ˆå®¹å™¨æ­£åœ¨å•Ÿå‹•ï¼‰âŒ
    éŒ¯èª¤ï¼Œå› ç‚º Ephemeral containers **åªèƒ½é™„åŠ åˆ°å·²é‹è¡Œçš„ Pod**ï¼Œç„¡æ³•å½±éŸ¿å•Ÿå‹•éç¨‹ã€‚
    å¦‚æœå®¹å™¨å°šæœªå•Ÿå‹•å®Œæˆï¼ŒEphemeral containers ä¹Ÿç„¡æ³•æ’å…¥ã€‚

    B) When using distroless imagesï¼ˆä½¿ç”¨ Distroless æ˜ åƒï¼‰âœ…
    æ­£ç¢ºï¼Œå› ç‚º distroless æ˜ åƒé€šå¸¸ä¸åŒ…å« shell æˆ–èª¿è©¦å·¥å…·ï¼Œç„¡æ³•ç›´æ¥åŸ·è¡Œ bash æˆ– sh ä¾†åµéŒ¯ã€‚
    ä½¿ç”¨ ephemeral containersï¼Œå¯ä»¥ **å‹•æ…‹é™„åŠ  ä¸€å€‹å¸¶æœ‰ sh æˆ– bash çš„èª¿è©¦å®¹å™¨ï¼Œå¦‚ busybox æˆ– debug-toolsï¼Œä¾†é€²è¡Œè¨ºæ–·**ã€‚
    
    ç¤ºä¾‹ï¼š
    kubectl debug -it mypod --image=busybox --target=mycontainer

    C) Debugging a pending podï¼ˆåµéŒ¯è™•æ–¼ pending ç‹€æ…‹çš„ Podï¼‰âŒ
    éŒ¯èª¤ï¼Œ**å› ç‚º Pod å¿…é ˆè™•æ–¼ Running ç‹€æ…‹ï¼Œæ‰èƒ½ä½¿ç”¨ Ephemeral containers**ã€‚
    Pending ç‹€æ…‹çš„ Pod å¯èƒ½é‚„æ²’æœ‰è¢«æŒ‡æ´¾ (scheduled) åˆ°ç¯€é»ï¼Œç”šè‡³é‚„æ²’æœ‰æ‹‰å–æ˜ åƒï¼Œé€™æ™‚å€™ç„¡æ³•é™„åŠ ä»»ä½•å®¹å™¨ã€‚
    
    æ­£ç¢ºçš„æ–¹æ³•ï¼š
    ä½¿ç”¨ kubectl describe pod <pod-name> æª¢æŸ¥ç‚ºä½• Pod è™•æ–¼ Pending ç‹€æ…‹ï¼ˆå¦‚è³‡æºä¸è¶³ã€èª¿åº¦å¤±æ•—ï¼‰ã€‚
    ä½¿ç”¨ kubectl get events ä¾†æŸ¥çœ‹ç›¸é—œäº‹ä»¶ã€‚

    D) Images doesnâ€™t contain debugging utilitiesï¼ˆæ˜ åƒä¸åŒ…å«åµéŒ¯å·¥å…·ï¼‰âœ…
    æ­£ç¢ºï¼Œé€™èˆ‡ B é¸é …ç›¸ä¼¼ï¼Œè¨±å¤šç”Ÿç”¢ç’°å¢ƒçš„ Docker æ˜ åƒ ç‚ºäº†å®‰å…¨æ€§ï¼Œä¸æœƒåŒ…å« curlã€viã€ps ç­‰å·¥å…·ã€‚
    **Ephemeral containers å…è¨±å‹•æ…‹åŠ å…¥ä¸€å€‹åŒ…å«é€™äº›å·¥å…·çš„å®¹å™¨ï¼Œä»¥ä¾¿é€²è¡Œè¨ºæ–·å’ŒåµéŒ¯**ã€‚
    
    ç¤ºä¾‹ï¼š
    kubectl debug -it mypod --image=busybox --target=mycontainer


    E) Debugging container in a crashloopï¼ˆåµéŒ¯é™·å…¥ CrashLoop çš„å®¹å™¨ï¼‰âœ…
    æ­£ç¢ºï¼Œ**å› ç‚º CrashLoopBackOff ç‹€æ…‹çš„å®¹å™¨æœƒä¸æ–·å´©æ½°ä¸¦é‡å•Ÿï¼Œå°è‡´é›£ä»¥é€²è¡Œæ¨™æº–åµéŒ¯**ã€‚
    **Ephemeral containers å…è¨±é™„åŠ åˆ° Pod è€Œä¸å½±éŸ¿åŸæœ¬çš„å®¹å™¨é‡å•Ÿè¡Œç‚ºï¼Œæä¾›ä¸€å€‹ç¨ç«‹çš„è¨ºæ–·ç’°å¢ƒ**ã€‚

    ç¤ºä¾‹ï¼š
    kubectl debug -it mypod --image=busybox --target=mycontainer
    é€™æ¨£å¯ä»¥åœ¨å®¹å™¨å´©æ½°æ™‚ï¼Œä½¿ç”¨ ps, top, cat /var/log/... ä¾†é€²ä¸€æ­¥èª¿æŸ¥å•é¡Œã€‚
    
    Answer: B,~~C~~,E æ˜¯ **B,D,E**!


    Which of the following are **incorrect statements**? Choose all valid answers

    A) The ephemeral container is guaranteed to not eat up the podâ€™s resources. (X)
    B) Ephemeral containers can be restarted after exiting. (X)
    C) The --copy-to flag creates a pod that is not owned by the original workload
    D) The pid namespace of all containers in a pod is shared with the ephemeral container by default. (X é™¤éè¨­ç½®--target!)

    C) ç‚ºä½• C æ˜¯æ­£ç¢ºçš„ï¼Ÿ
    --copy-to æ——æ¨™ (kubectl debug --copy-to) ç¢ºå¯¦æœƒå‰µå»ºä¸€å€‹æ–°çš„ Podï¼Œä¸”**é€™å€‹ Pod ä¸å±¬æ–¼åŸæœ¬çš„ workload**ï¼ˆä¾‹å¦‚ Deploymentã€StatefulSetï¼‰ã€‚
    é€™å€‹æ–°çš„ Pod ä¸æœƒå—åŸå§‹æ§åˆ¶å™¨ï¼ˆå¦‚ Deployment æˆ– ReplicaSetï¼‰ç®¡ç†ï¼Œå®ƒåªæ˜¯åŸºæ–¼åŸæœ¬çš„ Pod å‰µå»ºä¸€å€‹è¤‡è£½ç‰ˆæœ¬ï¼Œä¸¦å…è¨±ä¿®æ”¹é¡åƒã€ç’°å¢ƒè®Šæ•¸ç­‰è¨­å®šã€‚
    
    ç¤ºä¾‹ï¼š
    kubectl debug mypod --copy-to=mypod-debug --image=busybox
    é€™æ¨£æœƒå‰µå»ºä¸€å€‹ mypod-debugï¼Œä½†å®ƒ ä¸æœƒ å±¬æ–¼åŸæœ¬çš„ Deploymentï¼Œå› æ­¤ Kubernetes ä¸æœƒè‡ªå‹•ç¸®æ”¾æˆ–èª¿æ•´é€™å€‹æ–°çš„ Podã€‚

    D) The pid namespace of all containers in a pod is shared with the ephemeral container by default. âŒï¼ˆéŒ¯èª¤ âœ…ï¼‰
    éŒ¯èª¤åŸå› ï¼š

    é è¨­æƒ…æ³ä¸‹ï¼ŒEphemeral Containers ä¸¦ä¸æœƒèˆ‡ Pod å…§çš„å…¶ä»–å®¹å™¨å…±äº« PID namespaceã€‚
    å¦‚æœè¦è®“ Ephemeral Container å­˜å–å…¶ä»–å®¹å™¨çš„é€²ç¨‹ï¼ˆprocessï¼‰ï¼Œéœ€è¦æ‰‹å‹•æŒ‡å®š --targetã€‚
    
    ç¤ºä¾‹ï¼š
    kubectl debug -it mypod --image=busybox --target=mycontainer
    --target=mycontainer æœƒè®“ Ephemeral Container å…±äº« mycontainer çš„ PID namespaceï¼Œå¦å‰‡å®ƒæœƒæœ‰è‡ªå·±çš„ç¨ç«‹ namespaceã€‚
    
    ç‚ºä½•é€™æ˜¯éŒ¯èª¤çš„æ•˜è¿°ï¼Ÿ
    **Kubernetes è¨­è¨ˆä¸Šä¸æœƒè®“æ‰€æœ‰å®¹å™¨è‡ªå‹•å…±äº« PID namespaceï¼Œé¿å…å½±éŸ¿å®‰å…¨æ€§**ã€‚
    **é è¨­æƒ…æ³ä¸‹ï¼ŒEphemeral Containers åªèƒ½å­˜å–è‡ªå·±çš„é€²ç¨‹ï¼Œå¦‚æœæ²’æœ‰ --targetï¼Œå‰‡çœ‹ä¸åˆ° Pod å…§å…¶ä»–å®¹å™¨çš„é€²ç¨‹**ã€‚


    Answer: A,B,D 


    Your application webapp is running in the default namespace. It seems to be encountering some issues. WITHOUT the use of a debug container, attempt to find the cause of the issue. What error are you running into?

    option1: Error: command not found: kubectl exec 
    option2: Pod is not running or ready
    option3: OCI runtime exec failed: exec failed: unable to start container process: exec: "/bin/bash" : stat /bin/bash: no such file or directory

    Hint: run k exec command

    controlplane ~ âœ– k exec -it webapp -- /bin/bash
    error: Internal error occurred: Internal error occurred: error executing command in container: failed to exec in container: failed to start exec "6aed24fc83383c463df4fe462c4400050bc95731b1305b4a305c2023adc935a3": OCI runtime exec failed: exec failed: unable to start container process: exec: "/bin/bash": stat /bin/bash: no such file or directory: unknown


    Answer: option3

    

    è¦‹ç¬¬9. **k exec é©ç”¨æ–¼å·²é‹è¡Œçš„ Pod; k debug ä¸»è¦ç”¨æ–¼å•é¡Œæ’æŸ¥**

    1ï¸âƒ£ kubectl exec é©ç”¨æ–¼å·²é‹è¡Œçš„ Pod
    kubectl exec ç”¨æ–¼åŸ·è¡Œå‘½ä»¤åˆ° æ­£åœ¨é‹è¡Œä¸­çš„å®¹å™¨ï¼Œé€™é©ç”¨æ–¼ï¼š
    **éœ€è¦åœ¨å®¹å™¨å…§åŸ·è¡ŒæŸå€‹æŒ‡ä»¤ï¼Œå¦‚ /bin/bash**ã€shã€lsã€cat /var/log/...ã€‚
    éœ€è¦å­˜å–æ‡‰ç”¨ç¨‹å¼å…§éƒ¨çš„æ—¥èªŒæˆ–ç’°å¢ƒï¼Œè€Œä¸éœ€è¦é¡å¤–æ–°å¢ Debug å®¹å™¨ã€‚
    
    ç¯„ä¾‹ï¼š
    kubectl exec -it webapp -- /bin/bash
    é€™è¡ŒæŒ‡ä»¤çš„ç›®çš„æ˜¯å˜—è©¦é€²å…¥ webapp Pod å…§çš„å®¹å™¨ï¼ŒåŸ·è¡Œ /bin/bash æŒ‡ä»¤ã€‚

    2ï¸âƒ£ kubectl debug ä¸»è¦ç”¨æ–¼å•é¡Œæ’æŸ¥
    kubectl debug çš„ç”¨é€”ï¼š

    ç”¨æ–¼ç•¶ kubectl exec ä¸èƒ½è§£æ±ºå•é¡Œæ™‚ï¼Œä¾‹å¦‚ï¼š
    å®¹å™¨ç¼ºå°‘ shellï¼ˆé€™é¡Œçš„éŒ¯èª¤å°±æ˜¯ /bin/bash æ‰¾ä¸åˆ°ï¼‰ã€‚
    **å®¹å™¨å´©æ½° (CrashLoopBackOff)ï¼Œå°è‡´ kubectl exec ç„¡æ³•ä½¿ç”¨**ã€‚
    å®¹å™¨å½±åƒæ˜¯ distrolessï¼Œç¼ºå°‘ debug å·¥å…·ï¼Œå¦‚ curlã€psã€netstatã€‚
    æƒ³è¦ä½¿ç”¨ Ephemeral Container ä¾†é™„åŠ é¡å¤–çš„åµéŒ¯å·¥å…·ã€‚
    
    å¦‚æœé€™é¡Œå…è¨±ä½¿ç”¨ kubectl debugï¼Œå¯ä»¥ç”¨ï¼š
    kubectl debug -it webapp --image=busybox --target=webapp
    
    é€™æœƒé™„åŠ ä¸€å€‹æ–°çš„ ephemeral debug containerï¼Œä¸¦ä¸”åŒ…å« shellï¼Œè®“æˆ‘å€‘èƒ½å¤ é€²è¡Œé€²ä¸€æ­¥çš„åµéŒ¯ã€‚


    Add a debugging container to the ephemeral pod, use the busybox image. Whatâ€™s the first message you see ?

    option1: failed to generate container "5f7b9313beae753..."
    optpion2: Targeting container "ephermeral". If you don't see process from this conatiner it may be because the conainer runtime doesn't support this feature

    controlplane ~ âœ  k debug ephermeral --image=busybox --target ephermeral
    Targeting container "ephermeral". If you don't see processes from this container it may be because the container runtime doesn't support this feature.
    --profile=legacy is deprecated and will be removed in the future. It is recommended to explicitly specify a profile, for example "--profile=general".
    Error from server (NotFound): pods "ephermeral" not found

    ç‚ºä½• kubectl debug æŒ‡ä»¤éœ€è¦ --targetï¼Ÿ
    kubectl debug é è¨­æœƒå‰µå»ºä¸€å€‹æ–°çš„ ephemeral containerï¼Œä½†å¦‚æœæˆ‘å€‘å¸Œæœ›é€™å€‹å®¹å™¨é™„åŠ åˆ°ç‰¹å®šçš„ç›®æ¨™å®¹å™¨ï¼Œå°±å¿…é ˆä½¿ç”¨ --target åƒæ•¸ã€‚

    ä¸»è¦åŸå› 
    ç¢ºä¿ Ephemeral Container èƒ½å¤ å…±äº«ç›®æ¨™å®¹å™¨çš„ Namespace

    é€™æ¨£å¯ä»¥å­˜å–è©²å®¹å™¨çš„ processesã€networkã€filesystemã€‚
    è‹¥æ²’æœ‰ --targetï¼Œå‰‡ Kubernetes ä¸æœƒçŸ¥é“è¦é™„åŠ åˆ°å“ªå€‹å®¹å™¨ï¼Œå°¤å…¶æ˜¯ Pod å…§éƒ¨æœ‰å¤šå€‹å®¹å™¨æ™‚ã€‚
    è§£æ±º ps, top, netstat ç­‰æŒ‡ä»¤çœ‹ä¸åˆ°å…¶ä»–é€²ç¨‹çš„å•é¡Œ

    å¦‚æœ --target æœªæŒ‡å®šï¼ŒEphemeral Container å¯èƒ½ç„¡æ³•çœ‹åˆ°åŸæœ¬å®¹å™¨çš„é€²ç¨‹ï¼Œå› ç‚ºå®ƒæœƒå•Ÿå‹•ä¸€å€‹ç¨ç«‹çš„ PID å‘½åç©ºé–“ã€‚
    æœ‰ --target æ™‚ï¼ŒEphemeral Container æœƒå…±äº« ç›®æ¨™å®¹å™¨çš„ PID namespaceï¼Œè®“ ps auxã€top å¯ä»¥é¡¯ç¤ºåŸæœ¬çš„é€²ç¨‹ã€‚
    ä½¿ç”¨ --target å¯ä»¥é¿å…èª¤é™„åŠ åˆ°éŒ¯èª¤çš„å®¹å™¨

    åœ¨æŸäº›æƒ…æ³ä¸‹ï¼ŒPod å…§éƒ¨å¯èƒ½åŒ…å«å¤šå€‹æ‡‰ç”¨ç¨‹å¼å®¹å™¨ï¼Œä¸æŒ‡å®š --target å¯èƒ½æœƒé™„åŠ åˆ°éŒ¯èª¤çš„å®¹å™¨ã€‚
    --target ephermeral ç¢ºä¿æˆ‘å€‘çš„ busybox Debug å®¹å™¨é™„åŠ åˆ° åç‚º "ephermeral" çš„å®¹å™¨ï¼Œè€Œä¸æ˜¯å…¶ä»–ä¸ç›¸é—œçš„å®¹å™¨ã€‚

    Targeting container "ephermeral". If you don't see processes from this container it may be because the container runtime doesn't support this feature.
    
    é€™å€‹è¨Šæ¯çš„æ„æ€æ˜¯ï¼š
    Ephemeral Container æœƒå˜—è©¦å…±äº« "ephermeral" å®¹å™¨çš„ PID Namespaceã€‚
    ä½†æ˜¯ï¼Œæœ‰äº›å®¹å™¨é‹è¡Œæ™‚ï¼ˆContainer Runtimeï¼Œä¾‹å¦‚ Docker æˆ– containerdï¼‰å¯èƒ½ä¸æ”¯æ´æ­¤åŠŸèƒ½ã€‚
    å¦‚æœ ps aux æˆ– top æŒ‡ä»¤åŸ·è¡Œå¾Œ çœ‹ä¸åˆ°åŸæœ¬å®¹å™¨çš„é€²ç¨‹ï¼Œå¯èƒ½æ˜¯å› ç‚ºï¼š
    Runtime ä¸æ”¯æ´ PID Namespace å…±äº«ã€‚
    --target å®¹å™¨å…§æ²’æœ‰é‹è¡Œä¸­çš„é€²ç¨‹ã€‚

    Answer: option2

    
    Use a debug container to inspect backend, what is the linux version used?

    controlplane ~ âœ– k get pod
    NAME                       READY   STATUS    RESTARTS      AGE
    backend-7ddfdc4fdc-vxvkc   2/2     Running   1 (15m ago)   32m
    ephemeral                  1/1     Running   0             10m
    webapp                     1/1     Running   0             32m


    First Try:
    controlplane ~ âœ  k debug pod/backend --target backend
    error: you must specify --image when not using --copy-to.
    (kubectl debug é è¨­ä¸æœƒè‡ªå‹•é¸æ“‡ debug å®¹å™¨çš„æ˜ åƒï¼Œæ‰€ä»¥æˆ‘å€‘å¿…é ˆæ‰‹å‹•æŒ‡å®š --imageï¼Œå¦å‰‡ Kubernetes ç„¡æ³•å‰µå»º Ephemeral Containerã€‚)


    Second Try:
    controlplane ~ âœ– k debug pod/backend --target backend --image=busybox
    Targeting container "backend". If you don't see processes from this container it may be because the container runtime doesn't support this feature.
    --profile=legacy is deprecated and will be removed in the future. It is recommended to explicitly specify a profile, for example "--profile=general".
    Error from server (NotFound): pods "backend" not found

    --target backend æŒ‡å®šçš„ backend å®¹å™¨ä¸å­˜åœ¨ã€‚
    å¯èƒ½çš„åŸå› ï¼š
    backend-7ddfdc4fdc-vxvkc Pod å…§çš„å®¹å™¨åç¨±å¯èƒ½ä¸æ˜¯ backendï¼Œè€Œæ˜¯å…¶ä»–åç¨±ï¼ˆå¦‚ app æˆ– mainï¼‰ã€‚
    
    å¯ä»¥ç”¨ä»¥ä¸‹æŒ‡ä»¤æª¢æŸ¥ Pod å…§çš„å®¹å™¨åç¨±ï¼š
    kubectl get pod backend-7ddfdc4fdc-vxvkc -o jsonpath='{.spec.containers[*].name}'

    è‹¥ backend å®¹å™¨åç¨±éŒ¯èª¤ï¼Œæ‡‰è©²ä½¿ç”¨æ­£ç¢ºçš„å®¹å™¨åç¨±ï¼š
    k debug backend-7ddfdc4fdc-vxvkc --target <æ­£ç¢ºçš„å®¹å™¨åç¨±> --image=busybox


    Thrid Try:
    controlplane ~ âœ– k debug pod/backend-7ddfdc4fdc-vxvkc --target backend --image=busybox
    Targeting container "backend". If you don't see processes from this container it may be because the container runtime doesn't support this feature.
    --profile=legacy is deprecated and will be removed in the future. It is recommended to explicitly specify a profile, for example "--profile=general".
    Defaulting debug container name to debugger-4q7kw.
    The Pod "backend-7ddfdc4fdc-vxvkc" is invalid: spec.ephemeralContainers[0].targetContainerName: Not found: "backend"

    (æ­¤è™•çš„targetè¦è·Ÿpod nameä¸€æ¨£ï¼Œæ‡‰ç‚º: backend-7ddfdc4fdc-vxvkc )

    Forth Try: æˆåŠŸ
    controlplane ~ âœ– k debug backend-7ddfdc4fdc-vxvkc -it -c debugger --image=busybox
    --profile=legacy is deprecated and will be removed in the future. It is recommended to explicitly specify a profile, for example "--profile=general".
    If you don't see a command prompt, try pressing enter.
    / # **uname -a**
    Linux backend-7ddfdc4fdc-vxvkc 5.4.0-1106-gcp #115~18.04.1-Ubuntu SMP Mon May 22 20:46:39 UTC 2023 x86_64 GNU/Linux
    / # 


    Answer: Linux verion 5.4.0-1106-gcp

    <è£œå……>: ç‚ºä½•æˆ‘é‡æ–°é€²å…¥åˆ°debugg container ä»–ç›´æ¥é€€å‡ºäº†?? 
    controlplane ~ âœ  kubectl debug -it -c debugger --image=busybox backend-7ddfdc4fdc-vxvkc
    --profile=legacy is deprecated and will be removed in the future. It is recommended to explicitly specify a profile, for example "--profile=general".
    / # uname -a
    Linux backend-7ddfdc4fdc-vxvkc 5.4.0-1106-gcp #115~18.04.1-Ubuntu SMP Mon May 22 20:46:39 UTC 2023 x86_64 GNU/Linux
    / # exit

    controlplane ~ âœ  

    **ç‚ºä½•é‡æ–°é€²å…¥ Debug å®¹å™¨å¾Œï¼Œå®ƒç›´æ¥é€€å‡ºï¼Ÿ**

    ç•¶ä½ ä½¿ç”¨ `kubectl debug` ä¾†å•Ÿå‹•ä¸€å€‹ **Ephemeral Container**ï¼Œå®ƒçš„ **è¡Œç‚ºèˆ‡æ™®é€šçš„å®¹å™¨ä¸åŒ**ã€‚ä»¥ä¸‹æ˜¯å°è‡´å®¹å™¨é€€å‡ºçš„åŸå› ï¼š

    **ğŸ” åŸå›  1: Ephemeral Container åœ¨é€€å‡ºå¾Œä¸æœƒè‡ªå‹•é‡å•Ÿ**
    - **Ephemeral Containers æ˜¯ä¸€æ¬¡æ€§çš„ (one-time use)**ï¼Œç•¶ä½ åŸ·è¡Œ `exit` æ™‚ï¼Œå®ƒæœƒç›´æ¥çµ‚æ­¢ä¸¦ä¸” **ä¸æœƒé‡æ–°å•Ÿå‹•**ã€‚
    - Kubernetes **ä¸æœƒç®¡ç† Ephemeral Container çš„ç”Ÿå‘½é€±æœŸ**ï¼Œå®ƒä¸åƒæ™®é€šçš„ Pod å…§éƒ¨çš„å®¹å™¨å¯ä»¥è‡ªå‹•é‡å•Ÿã€‚

    - **è­‰æ“šï¼š**
    ```
    Session ended, the ephemeral container will not be restarted but may be reattached using 'kubectl attach backend-7ddfdc4fdc-vxvkc -c debugger -i -t' if it is still running
    ```
    - **é—œéµå­—**: `the ephemeral container will not be restarted`
    - é€™æ˜ç¢ºè¡¨ç¤º **Ephemeral å®¹å™¨ä¸æœƒè‡ªå‹•é‡æ–°å•Ÿå‹•**ï¼Œä½†å¦‚æœå®ƒä»ç„¶åœ¨é‹è¡Œï¼Œå‰‡å¯ä»¥ç”¨ `kubectl attach` é‡æ–°é€£æ¥ã€‚

    
    **ğŸ” åŸå›  2: `kubectl debug` æœƒæ¯æ¬¡éƒ½å‰µå»ºä¸€å€‹æ–°çš„ Ephemeral Container**
    - **ä½ åŸ·è¡Œ `kubectl debug` æ™‚ï¼ŒKubernetes æœƒé‡æ–°å‰µå»ºä¸€å€‹æ–°çš„ Ephemeral Container**ã€‚
    - **ä½†å¦‚æœä¸Šæ¬¡çš„ Debug å®¹å™¨å·²ç¶“é€€å‡ºï¼Œå®ƒä¸æœƒè¢«é‡æ–°å•Ÿå‹•**ï¼Œæ–°çš„ Ephemeral Container ä¹Ÿæœƒç«‹å³çµ‚æ­¢ï¼Œå› ç‚ºå®ƒ **æ²’æœ‰ä»»ä½•é€²ç¨‹åœ¨é‹è¡Œ**ã€‚

    **ğŸ”¹ å¦‚ä½•è­‰æ˜ Kubernetes å‰µå»ºäº†ä¸€å€‹æ–° Debug å®¹å™¨ï¼Ÿ**
    ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹ Ephemeral Containersï¼š
    ```sh
    kubectl get pod backend-7ddfdc4fdc-vxvkc -o jsonpath='{.spec.ephemeralContainers[*].name}'
    ```
    å¦‚æœæ¯æ¬¡åŸ·è¡Œ `kubectl debug`ï¼Œé€™å€‹åˆ—è¡¨æœƒä¸æ–·æ–°å¢ä¸åŒçš„ `debugger-*` å®¹å™¨åç¨±ã€‚

    **ğŸ” ç‚ºä½•æ–°çš„ Debug å®¹å™¨ç«‹å³é€€å‡ºï¼Ÿ**
    - ç•¶ Kubernetes å‰µå»ºä¸€å€‹æ–°çš„ **Ephemeral Debug å®¹å™¨**ï¼Œå¦‚æœæ²’æœ‰çµ¦å®šæŒçºŒåŸ·è¡Œçš„å‘½ä»¤ï¼Œå®ƒæœƒ**ç«‹å³é€€å‡º**ã€‚
    - é è¨­æƒ…æ³ä¸‹ï¼Œ`busybox` å®¹å™¨çš„ `ENTRYPOINT` æ˜¯ `sh`ï¼Œä½†å¦‚æœæ²’æœ‰ `-it` æˆ– `sleep`ï¼Œå®ƒæœƒ**ç›´æ¥çµæŸ**ã€‚

    **ğŸ”¹ å¦‚ä½•æ¸¬è©¦ï¼Ÿ**
    åŸ·è¡Œï¼š
    ```sh
    kubectl debug backend-7ddfdc4fdc-vxvkc -it -c debugger --image=busybox -- sh
    ```
    å¦‚æœ `sh` è¢«é—œé–‰ï¼ŒEphemeral å®¹å™¨å°±æœƒé€€å‡ºã€‚

    **âœ… è§£æ±ºæ–¹æ¡ˆ**
    /### **æ–¹æ³• 1: ä½¿ç”¨ `kubectl attach` é‡æ–°é€£æ¥å·²é‹è¡Œçš„ Debug å®¹å™¨**
    å¦‚æœ Debug å®¹å™¨ä»ç„¶é‹è¡Œï¼Œä½ å¯ä»¥ä½¿ç”¨ï¼š
    ```sh
    kubectl attach backend-7ddfdc4fdc-vxvkc -c debugger -i -t
    ```
    é€™æ¨£å°±å¯ä»¥é‡æ–°é€²å…¥ä¹‹å‰çš„ Ephemeral å®¹å™¨ï¼Œè€Œä¸æœƒå‰µå»ºæ–°çš„ã€‚

    
    /### **æ–¹æ³• 2: è®“ Debug å®¹å™¨ä¿æŒå­˜æ´»**
    å¦‚æœä½ æƒ³è®“ Debug å®¹å™¨ **ä¸æœƒç«‹å³é€€å‡º**ï¼Œä½ å¯ä»¥è®“å®ƒåŸ·è¡Œ `sleep`ï¼š
    ```sh
    kubectl debug backend-7ddfdc4fdc-vxvkc -it -c debugger --image=busybox -- sh -c "sleep 3600"
    ```
    é€™æ¨£ï¼Œå®ƒæœƒé‹è¡Œ `sleep 3600`ï¼Œä¸¦ä¸”æŒçºŒå­˜æ´» 1 å°æ™‚ï¼Œä½ å¯ä»¥å¤šæ¬¡ç”¨ `kubectl attach` é€²å…¥å®ƒã€‚

    /### **æ–¹æ³• 3: ä½¿ç”¨ `kubectl run` ä¾†å•Ÿå‹•ç¨ç«‹çš„ Debug Pod**
    å¦‚æœä½ éœ€è¦ä¸€å€‹é•·æ™‚é–“é‹è¡Œçš„ Debug å®¹å™¨ï¼Œè€Œä¸æ˜¯ Ephemeral Containerï¼Œä½ å¯ä»¥é€™æ¨£åšï¼š
    ```sh
    kubectl run debug-container --rm -it --image=busybox -- /bin/sh
    ```
    é€™æ¨£ï¼Œå®ƒæœƒå‰µå»ºä¸€å€‹æ–°çš„ **ç¨ç«‹ Pod**ï¼Œä½†ç•¶ä½  `exit` æ™‚ï¼Œå®ƒæœƒè¢«åˆªé™¤ã€‚


    æ³¨æ„!! Kubernetes ä¸å…è¨±ä¿®æ”¹å·²ç¶“å‰µå»ºçš„ Ephemeral Containerï¼Œæ‰€ä»¥ kubectl debug ä¸èƒ½è®Šæ›´ debugger å®¹å™¨çš„ Command æˆ– Argsã€‚
    
    ä½ ä¹‹å‰å·²ç¶“å‰µå»ºäº† debugger å®¹å™¨ï¼Œç¾åœ¨å˜—è©¦ä¿®æ”¹å®ƒï¼Œå› æ­¤è¢«æ‹’çµ•ã€‚
    å¦‚ä½•è§£æ±ºï¼Ÿ
    âœ… æ–¹æ¡ˆ 1ï¼šåˆªé™¤ Pod ä¸¦é‡æ–°å‰µå»º Debug å®¹å™¨ï¼ˆå¦‚æœå¯ä»¥åˆªé™¤ï¼‰
    âœ… æ–¹æ¡ˆ 2ï¼šä½¿ç”¨ä¸åŒåç¨±çš„ Ephemeral å®¹å™¨ï¼Œä¾‹å¦‚ debugger2
    âœ… æ–¹æ¡ˆ 3ï¼šå¦‚æœ Debug å®¹å™¨é‚„åœ¨é‹è¡Œï¼Œä½¿ç”¨ kubectl attach æˆ– kubectl exec é€²å…¥å®ƒ

    å€˜è‹¥ä¸èƒ½åˆªé™¤podçš„è©±ï¼Œå‰‡æ¡ç”¨æ–¹æ¡ˆ2ï¼Œæ–°å»ºç«‹ä¸€å€‹debugger2å®¹å™¨ï¼Œç„¶å¾Œè¨­ç½®sleep ä½¿å…¶èƒ½é‹è¡Œä¹…ä¸€é»ï¼Œæ¥è‘—k execé€²å…¥è©²å®¹å™¨

    controlplane ~ âœ– kubectl debug backend-7ddfdc4fdc-vxvkc -it -c debugger2 --image=busybox -- s
    h -c "sleep 3600"
    --profile=legacy is deprecated and will be removed in the future. It is recommended to explicitly specify a profile, for example "--profile=general".
    If you don't see a command prompt, try pressing enter.

    ^C



    ^C^C^C^C

    (æ–°é–‹è¦–çª—ï¼Œä½¿ç”¨k exec ä¾¿å¯é€²å…¥æ–°å»ºç«‹çš„å®¹å™¨)

    controlplane ~ âœ  kubectl exec -it backend-7ddfdc4fdc-vxvkc -c debugger2 -- sh
    / # 



    What happens when you exit the shell from the debug container?
    option1: The debug container is terminated
    option2: The pod restarts

    Answer: option1

11. k9s CLI 
    
    command line è¼¸å…¥: k9sé€²å…¥k9sä»‹é¢
    ![k9S CLI](../images/debug/k9s.png "k9S CLI")

    k9s visulization:
    ![k9S visualization](../images/debug/k9s-pulse "k9S Visualization")

### Troubleshooting Scenario
!! å€˜è‹¥k describe deploy/pods ä¸‹çš„events æ²’æœ‰è¼¸å‡ºä»»ä½•æœ‰ç”¨æç¤ºï¼Œå‰‡å–„ç”¨k get eventsæŒ‡ä»¤ ä¾†ç²å–ç›¸é—œçš„events!!

1. Imageg Pull Errors

    image pull çš„å¹¾ç¨®æƒ…æ³:
    i. image name typo
    ii. Pod Events å…§é¡¯ç¤º: 401 Unauthorizedï¼Œ å¯èƒ½æœ‰secrets ä½†æ˜¯podæ²’æœ‰å®šç¾©imagePullSecretåƒæ•¸

    ![ImagePullError: 401](../images/debug/imagePull.png "ImagePullError: 401")

    ![ImagePullError: 401](../images/debug/imagePull02.png "ImagePullError: 401")

    iii. Pod Events å…§é¡¯ç¤º: no such hostï¼Œå‰‡ä½¿ç”¨nslookup æŒ‡ä»¤æª¢æŸ¥hostæ˜¯å¦å­˜åœ¨(æ­¤é¡ŒCKADæ‡‰ä¸æœƒè€ƒ)

    ![ImagePullError: 401](../images/debug/imagePull03.png "ImagePullError: 401")

    ![ImagePullError: 401](../images/debug/imagePull04.png "ImagePullError: 401")


    What does the ErrImagePull error indicate in a Kubernetes environment?
    Answer: Kubernetes is unable to pull the container image from the registry.


    What is the primary purpose of using imagePullSecrets in a pod definition file?
    Answer: To provide Kubernets with the credentials to pull a private container image.


    Youâ€™re in a production environment investigating an issue with the app pod. What seems to be the problem?
    Pods can be in any of the namespaces

    controlplane ~ âœ  k get pod -A| grep app
    production    app                                    0/1     ErrImagePull   0             34s
    production    webapp-75f4b589fd-95sgk                1/1     Running        0             80s

    controlplane ~ âœ  k describe pod -n production app 
    ...
    Events:
    Type     Reason     Age                From               Message
    ----     ------     ----               ----               -------
    Normal   Scheduled  59s                default-scheduler  Successfully assigned production/app to node01
    Normal   BackOff    27s (x2 over 57s)  kubelet            Back-off pulling image "docker.io/nicholasaaronbrady/testnode:latest"
    Warning  Failed     27s (x2 over 57s)  kubelet            Error: ImagePullBackOff
    Normal   Pulling    16s (x3 over 58s)  kubelet            Pulling image "docker.io/nicholasaaronbrady/testnode:latest"
    Warning  Failed     16s (x3 over 57s)  kubelet            Failed to pull image "docker.io/nicholasaaronbrady/testnode:latest": failed to pull and unpack image "docker.io/nicholasaaronbrady/testnode:latest": failed to resolve reference "docker.io/nicholasaaronbrady/testnode:latest": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed
    Warning  Failed     16s (x3 over 57s)  kubelet            Error: ErrImagePull


    Answer: Pulling a private image without credentials


    We have just identified a pod misconfigured-pod attempting to pull an image nginx:latest . Identify and fix the problem.
    Pods can be in any of the namespaces

    controlplane ~ âœ  k get pod 
    NAME                READY   STATUS         RESTARTS   AGE
    misconfigured-pod   0/1     ErrImagePull   0          13s

    controlplane ~ âœ  k describe pod misconfigured-pod
    ...
    Events:
    Type     Reason     Age                From               Message
    ----     ------     ----               ----               -------
    Normal   Scheduled  25s                default-scheduler  Successfully assigned default/misconfigured-pod to node01
    Normal   BackOff    23s                kubelet            Back-off pulling image "ngninx:latest"
    Warning  Failed     23s                kubelet            Error: ImagePullBackOff
    Normal   Pulling    10s (x2 over 24s)  kubelet            Pulling image "ngninx:latest"
    Warning  Failed     10s (x2 over 24s)  kubelet            Failed to pull image "ngninx:latest": failed to pull and unpack image "docker.io/library/ngninx:latest": failed to resolve reference "docker.io/library/ngninx:latest": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed
    
    controlplane ~ âœ  vim misconfigured-pod.yaml 
    (æ›´æ”¹image typo)

    controlplane ~ âœ– k replace -f misconfigured-pod.yaml --force
    pod "misconfigured-pod" deleted
    pod/misconfigured-pod replaced

    controlplane ~ âœ  k get pod
    NAME                READY   STATUS              RESTARTS   AGE
    misconfigured-pod   0/1     ContainerCreating   0          3s

    controlplane ~ âœ  k get pod
    NAME                READY   STATUS    RESTARTS   AGE
    misconfigured-pod   1/1     Running   0          10s


    In a production environment, you have an existing application deployment webapp with kodekloud/webapp-color:v1 pod running smoothly in your cluster. However, during a routine update to kodekloud/webapp-color:v2, you encounter an ErrImagePull error when trying to deploy the updated version. Investigate and resolve the issue.

    Manifest file for webapp deployment is present at /root/webapp-deployment.yaml
    Deployment can be in any of the namespaces

    controlplane ~ âœ  k get pod -n production 
    NAME                      READY   STATUS             RESTARTS   AGE
    webapp-75f4b589fd-95sgk   1/1     Running            0          8m3s
    webapp-79cc5d9d98-6b9bt   0/1     ImagePullBackOff   0          22s

    controlplane ~ âœ  k get deploy -n production 
    NAME     READY   UP-TO-DATE   AVAILABLE   AGE
    webapp   1/1     1            1           8m15s

    controlplane ~ âœ  k describe pod -n production webapp-79cc5d9d98-6b9bt 
    ...
    Events:
    Type     Reason     Age                 From               Message
    ----     ------     ----                ----               -------
    Normal   Scheduled  117s                default-scheduler  Successfully assigned production/webapp-79cc5d9d98-6b9bt to node01
    Normal   Pulling    28s (x4 over 116s)  kubelet            Pulling image "docker.io/kodekloud/webapp-color:vv2"
    Warning  Failed     28s (x4 over 116s)  kubelet            Failed to pull image "docker.io/kodekloud/webapp-color:vv2": rpc error: code = NotFound desc = failed to pull and unpack image "docker.io/kodekloud/webapp-color:vv2": failed to resolve reference "docker.io/kodekloud/webapp-color:vv2": docker.io/kodekloud/webapp-color:vv2: not found
    Warning  Failed     28s (x4 over 116s)  kubelet            Error: ErrImagePull
    Normal   BackOff    5s (x7 over 115s)   kubelet            Back-off pulling image "docker.io/kodekloud/webapp-color:vv2"
    Warning  Failed     5s (x7 over 115s)   kubelet            Error: ImagePullBackOff


    controlplane ~ âœ  vim webapp-deployment.yaml  (ä¿®æ”¹vv2 ç‚ºv2)

    controlplane ~ âœ  k replace -f webapp-deployment.yaml --force
    deployment.apps "webapp" deleted
    deployment.apps/webapp replaced

    controlplane ~ âœ  k get deploy -n production 
    NAME     READY   UP-TO-DATE   AVAILABLE   AGE
    webapp   0/1     1            0           2s



    Once again, youâ€™re in a production environment investigating an issue with the api pod. What seems to be the problem this time?
    Pods can be in any of the namespaces

    option1: Image Repository does not exist
    option2: Pod cannot reach container registry

    controlplane ~ âœ  k get pod -n production 
    ...
    Events:
    Type     Reason     Age                From               Message
    ----     ------     ----               ----               -------
    Normal   Scheduled  35s                default-scheduler  Successfully assigned production/api to node01
    Normal   Pulling    16s (x2 over 30s)  kubelet            Pulling image "gitlab.kodekloud.com:5050/root/webapp:v4"
    Warning  Failed     16s (x2 over 30s)  kubelet            Failed to pull image "gitlab.kodekloud.com:5050/root/webapp:v4": **failed to pull and unpack image** "gitlab.kodekloud.com:5050/root/webapp:v4": failed to resolve reference "gitlab.kodekloud.com:5050/root/webapp:v4": **failed to do request**: Head "https://gitlab.kodekloud.com:5050/v2/root/webapp/manifests/v4": dial tcp: lookup gitlab.kodekloud.com on 172.25.0.1:53: **no such host**
    Warning  Failed     16s (x2 over 30s)  kubelet            Error: ErrImagePull
    Normal   BackOff    2s (x2 over 29s)   kubelet            Back-off pulling image "gitlab.kodekloud.com:5050/root/webapp:v4"
    Warning  Failed     2s (x2 over 29s)   kubelet            Error: ImagePullBackOff

    Answer: option2


    failed to resolve referenceï¼šç„¡æ³•è§£æ Image Repository ä½å€ã€‚
    failed to do request: Headï¼šå˜—è©¦å¾ gitlab.kodekloud.com:5050 ä¸‹è¼‰ imageï¼Œä½†ç™¼ç”ŸéŒ¯èª¤ã€‚
    dial tcp: lookup gitlab.kodekloud.com on 172.25.0.1:53: no such hostï¼š
    **é€™ä»£è¡¨ Kubernetes å˜—è©¦è§£æ gitlab.kodekloud.com é€™å€‹ç¶²åŸŸæ™‚ï¼ŒDNS ç„¡æ³•æ‰¾åˆ°é€™å€‹ä½å€**ã€‚
    
    é€™é€šå¸¸æ„å‘³è‘—ç¶²è·¯å•é¡Œï¼Œä¾‹å¦‚ï¼š
    Pod ç„¡æ³•é€£ç·šåˆ°å¤–éƒ¨çš„ container registry (gitlab.kodekloud.com:5050)ã€‚
    DNS ä¼ºæœå™¨ç„¡æ³•è§£æè©²ç¶²å€ã€‚
    
    ç‚ºä½•ä¸æ˜¯ option1 (Image Repository does not exist)?
    å¦‚æœ image repository ä¸å­˜åœ¨ï¼Œé€šå¸¸æœƒå‡ºç¾éŒ¯èª¤é¡ä¼¼ï¼š

    "manifest unknown" æˆ– "repository not found"
    404 Not Found
    failed to pull and unpack image ... manifest unknown
    é€™äº›éŒ¯èª¤è¡¨æ˜ è©² repository æ²’æœ‰è©² imageï¼Œä½† DNS è§£ææ‡‰è©²ä»ç„¶æˆåŠŸï¼Œä¸æœƒå‡ºç¾ "lookup ... no such host" é€™é¡éŒ¯èª¤ã€‚

    ä½†åœ¨é€™æ¬¡çš„éŒ¯èª¤è¨Šæ¯ä¸­ï¼Œé—œéµå•é¡Œæ˜¯ **Pod ç„¡æ³•è§£æ Container Registry çš„ DNS ä½å€**ï¼Œé€™æ›´ç¬¦åˆ option2 (Pod cannot reach container registry)ã€‚


2. Crashing Pods (**é‡è¦!!! å¸¸è€ƒ!!**) (èˆ‡Containerå…§éƒ¨é…ç½®å¦‚:Probe, Volumes, env varaibles æœ‰é—œ)

    å°è‡´CrashLoopBackOff çš„å¹¾ç¨®æƒ…æ³:
    i. **æŸ¥æ‰¾ä¸åˆ°env variablesï¼Œå°‡ä½¿podå´©æ½°ä¸¦ä¸æ–·å˜—è©¦é‡å•Ÿ**
    ![Crash](../images/debug/crash.png "Crash")

    ii. podç„¡æ³•execé€²å…¥å®¹å™¨ï¼Œå°è‡´podå´©æ½°
    å¦‚: unable to start container process : exec: "/script.sh" : permission denied: unknown
    ![Crash 2](../images/debug/crash02.png "Crash 02")

    ä½¿ç”¨docker imagesæª¢æŸ¥è©²é¡åƒï¼Œdocker run -it --image=<é¡åƒ> shï¼Œæ‰‹å‹•å»ºç«‹ä¸€å€‹å…·æœ‰è©²é¡åƒçš„podä¸¦é€²å…¥åˆ°å®¹å™¨å…§ï¼Œæª¢æŸ¥æ–‡ä»¶
    ![Crash 3](../images/debug/crash03.png "Crash 03")

    ç™¼ç¾è©²script.shæ–‡ä»¶ä¸å…·æœ‰åŸ·è¡Œçš„æ¬Šé™ï¼Œchmodä¿®æ”¹
    ![Crash 4](../images/debug/crash04.png "Crash 04")

    ä¿®æ”¹ä¸¦é‡å•Ÿpodä¹‹å¾Œï¼Œä¾¿å¯çœ‹åˆ°ç‹€æ…‹ç‚ºrunning

    iii. no such file or directory
    ![Crash 5](../images/debug/crash05.png "Crash 05")

    æª¢æŸ¥deployment æ˜¯å¦æœ‰å®šç¾©configmapï¼Œç™¼ç¾æ²’æœ‰ï¼Œç™¼ç¾containerå…§æ²’æœ‰å®šç¾©volumeMount
    ![Crash 6](../images/debug/crash06.png "Crash 06")

    k edit deploymentå¾Œï¼Œæª¢æŸ¥podå·²é‡å•ŸæˆåŠŸä¸¦running

    iv. **OOMKilled: Pod çš„Memoryä½¿ç”¨é‡è¶…éäº† limit**ï¼Œå°è‡´å®¹å™¨è¢«ç³»çµ±å¼·åˆ¶çµ‚æ­¢ (Killed by the Out-Of-Memory Killer)
    ![Crash 7](../images/debug/crash07.png "Crash 07")

    å°‡pod limitè¨­ç½®é«˜æ–¼requestå¾Œé‡å•Ÿpod,ä¾¿å¯é‡å•ŸæˆåŠŸ

    v. Probe æœ‰å•é¡Œ
    ![Crash 8](../images/debug/crash08.png "Crash 08")
    ![Crash 9](../images/debug/crash09.png "Crash 09")

    vi. connection refused: è¡¨ç¤ºæ‡‰ç”¨ç¨‹åºé‚„æ²’redayï¼Œ æœ‰å¯èƒ½æ˜¯ **LivenessProbeæ¢æ¸¬æ™‚é–“å¤ªçŸ­**! 
    connection refused è¡¨ç¤º æ‡‰ç”¨ç¨‹å¼é‚„æ²’æº–å‚™å¥½ï¼Œä½† livenessProbe å·²ç¶“é–‹å§‹æ¢æ¸¬ã€‚
    é€™é€šå¸¸ç™¼ç”Ÿåœ¨ï¼š
    æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•æ™‚é–“è¼ƒé•·ï¼ˆå¦‚éœ€è¼‰å…¥å¤§é‡è³‡æ–™ã€é€£æ¥è³‡æ–™åº«ç­‰ï¼‰ã€‚
    æ¢æ¸¬æ™‚é–“è¨­å®šéçŸ­ï¼Œå°è‡´æ‡‰ç”¨ç¨‹å¼é‚„æ²’æº–å‚™å¥½å°±è¢«èª¤æ®ºã€‚
    
    Pod çš„ Liveness Probe åœ¨æ‡‰ç”¨ç¨‹å¼é‚„æ²’å®Œå…¨å•Ÿå‹•æ™‚å°±é–‹å§‹æª¢æŸ¥ï¼Œå°è‡´æ‡‰ç”¨ç¨‹å¼ä¸æ–·è¢« Kubernetes æ®ºæ­»ä¸¦é‡æ–°å•Ÿå‹• (Back-off restarting failed container)
    ![Crash 10](../images/debug/crash10.png "Crash 10")
    
    initialDelaySeconds: 1
    Pod å•Ÿå‹• 1ç§’å¾Œ å°±é–‹å§‹é€²è¡Œå¥åº·æª¢æŸ¥ (livenessProbe)ã€‚
    ä½†å¤§éƒ¨åˆ†æ‡‰ç”¨ç¨‹å¼éœ€è¦ è¼ƒé•·æ™‚é–“ä¾†å•Ÿå‹• (ç‰¹åˆ¥æ˜¯ Spring Bootã€Node.jsã€Python Flask ç­‰å¾Œç«¯æ‡‰ç”¨)ã€‚
    å¦‚æœæ‡‰ç”¨ç¨‹å¼å°šæœªæº–å‚™å¥½ï¼ŒKubernetes æœƒéŒ¯èª¤åœ°èªç‚ºå®ƒç•¶æ©Ÿï¼Œé€²è€Œ æ®ºæ‰ä¸¦é‡å•Ÿå®¹å™¨ã€‚
    (ä¿®æ”¹ç‚º20ç§’)

    periodSeconds: 1
    æ¯ 1 ç§’ é€²è¡Œä¸€æ¬¡å¥åº·æª¢æŸ¥ï¼Œé€™å°æ–¼è¨±å¤šæ‡‰ç”¨ç¨‹å¼ä¾†èªªéæ–¼é »ç¹ï¼Œå®¹æ˜“å°è‡´ä¸å¿…è¦çš„é‡å•Ÿã€‚
    (ä¿®æ”¹ç‚º10ç§’)
    ![Crash 11](../images/debug/crash11.png "Crash 11")

    ä¹Ÿå¯ä»¥å¤šè¨­ç½®: **failureThreshold**: 3
    è‹¥æ¢æ¸¬å¤±æ•— 3 æ¬¡ æ‰åˆ¤å®š Pod ç•¶æ©Ÿï¼Œè€Œä¸æ˜¯ç«‹åˆ»é‡å•Ÿï¼Œé¿å…èª¤æ®ºã€‚


    What does exit code 1 indicate?
    Exit code 1 generally indicates that the application inside the container has encountered an error.

    option1: Abnormal Termination (SIGABRT)
    option2: Application Error
    option3: Purposely Stopped

    Answerï¼šoption3

    
    Which exit code indicates the application tried to access a non-existent file?

    livenessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - |
              pg_isready -d mydatabase -h localhost -U myuser -t 1
          failureThreshold: 3
          initialDelaySeconds: 30
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: postgres
        ports:
        - containerPort: 5432
          protocol: TCP
        readinessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - |
              pg_isready -d mydatabase -h localhostt -U myuser -t 1 |# æœ‰éŒ¯å­—
          failureThreshold: 3
          initialDelaySeconds: 20
          periodSeconds: 10


    Answer: readinessProbe is falling

    Fix the previous issue with the cart-api deployment.

    controlplane ~ âœ  k edit deploy cart-api 
    deployment.apps/cart-api edited

    controlplane ~ âœ  k get deploy
    NAME             READY   UP-TO-DATE   AVAILABLE   AGE
    cart-api         1/1     1            1           9m30s
    data-processor   0/1     1            0           10m


    You are a seasoned Kubernetes application developer overseeing a critical production environment. You observe a web-server pod encountering a serious issue after a junior developer deployed a quick fix. What seems to be the problem?

    controlplane ~ âœ  k get pod
    NAME                              READY   STATUS             RESTARTS        AGE
    cart-api-6df899869b-zcb2w         1/1     Running            0               2m14s
    data-processor-55d57797b8-fxxds   0/1     CrashLoopBackOff   7 (3m51s ago)   11m
    web-server                        1/1     Running            2 (38s ago)     45s

    controlplane ~ âœ  k logs web-server 
    /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
    /docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
    /docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
    10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf
    10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf
    /docker-entrypoint.sh: Sourcing /docker-entrypoint.d/15-local-resolvers.envsh
    /docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
    /docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh
    /docker-entrypoint.sh: Configuration complete; ready for start up
    2025/03/09 01:47:46 [emerg] 1#1: open() "/etc/nginx/nginx.conf" failed (2: No such file or directory)
    nginx: [emerg] open() "/etc/nginx/nginx.conf" failed (2: No such file or directory)

    spec:
        containers:
        - image: rakshithraka/custom-nginx:latest
            imagePullPolicy: Always
            name: nginx
            ports:
            - containerPort: 80
            name: http-server
            protocol: TCP
            resources: {}
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
              - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
              name: kube-api-access-vgz6x
              readOnly: true

    Answer: Volume needed by app not mounted


    Your company sales are booming and the developers are rolling out new releases every day to production. One day, you notice a problem with api-alpha deployments. What seems to be the problem here?

    controlplane ~ âœ  k get pod
    NAME                              READY   STATUS             RESTARTS        AGE
    cart-api-6df899869b-zcb2w         1/1     Running            0               16m
    data-processor-55d57797b8-fxxds   0/1     CrashLoopBackOff   12 (11s ago)    26m

    controlplane ~ âœ  k describe pod api-alpha-cf697c9fc-74j68 
    ...
    Containers:
    memory-demo-2-ctr:
        Container ID:  containerd://23128db7bfad7ac5e153e43162e6463ade5a72ea901068ad797845eb303fc1e4
        Image:         polinux/stress
        Image ID:      docker.io/polinux/stress@sha256:b6144f84f9c15dac80deb48d3a646b55c7043ab1d83ea0a697c09097aaad21aa
        Port:          <none>
        Host Port:     <none>
        Command:
        stress
        Args:
        --vm
        1
        --vm-bytes
        250M
        --vm-hang
        1
        State:          Waiting
        Reason:       CrashLoopBackOff
        Last State:     Terminated
        Reason:       **OOMKilled**
        Exit Code:    1
        Started:      Sun, 09 Mar 2025 01:57:21 +0000
        Finished:     Sun, 09 Mar 2025 01:57:21 +0000
        Ready:          False
        Restart Count:  6
        Limits:
        memory:  100Mi
        Requests:
        memory:     50Mi
        Environment:  <none>
        Mounts:
        /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-k2lfb (ro)
    Conditions:
    Type                        Status
    PodReadyToStartContainers   True 
    Initialized                 True 
    Ready                       False 
    ContainersReady             False 
    PodScheduled                True 
    Volumes:
    kube-api-access-k2lfb:
        Type:                    Projected (a volume that contains injected data from multiple sources)
        TokenExpirationSeconds:  3607
        ConfigMapName:           kube-root-ca.crt
        ConfigMapOptional:       <nil>
        DownwardAPI:             true
    QoS Class:                   Burstable
    Node-Selectors:              <none>
    Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                                node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
    Events:
    Type     Reason     Age                  From               Message
    ----     ------     ----                 ----               -------
    Normal   Scheduled  10m                  default-scheduler  Successfully assigned default/api-alpha-cf697c9fc-74j68 to node01
    Normal   Pulled     10m                  kubelet            Successfully pulled image "polinux/stress" in 677ms (677ms including waiting). Image size: 4041495 bytes.
    Normal   Pulled     10m                  kubelet            Successfully pulled image "polinux/stress" in 158ms (158ms including waiting). Image size: 4041495 bytes.
    Normal   Pulled     10m                  kubelet            Successfully pulled image "polinux/stress" in 159ms (159ms including waiting). Image size: 4041495 bytes.
    Normal   Pulled     9m32s                kubelet            Successfully pulled image "polinux/stress" in 150ms (150ms including waiting). Image size: 4041495 bytes.
    Normal   Pulled     8m51s                kubelet            Successfully pulled image "polinux/stress" in 161ms (161ms including waiting). Image size: 4041495 bytes.
    Normal   Created    7m22s (x6 over 10m)  kubelet            Created container: memory-demo-2-ctr
    Normal   Pulled     7m22s                kubelet            Successfully pulled image "polinux/stress" in 138ms (138ms including waiting). Image size: 4041495 bytes.
    Normal   Started    7m21s (x6 over 10m)  kubelet            Started container memory-demo-2-ctr
    Normal   Pulling    4m37s (x7 over 10m)  kubelet            Pulling image "polinux/stress"
    Normal   Pulled     4m37s                kubelet            Successfully pulled image "polinux/stress" in 200ms (200ms including waiting). Image size: 4041495 bytes.
    Warning  BackOff    13s (x48 over 10m)   kubelet            Back-off restarting failed container memory-demo-2-ctr in pod api-alpha-cf697c9fc-74j68_default(d07632b6-7b2c-49ce-8e1a-7acff00947c1)

    **é›–ç„¶yaml fileè¨­ç½®request æ˜¯50Mi å°æ–¼ limitsæ˜¯100Miï¼Œä½†æ˜¯æ³¨æ„commandè¨­ç½®äº†**:
    --vm 1ï¼šå•Ÿå‹• 1 å€‹è¨˜æ†¶é«”åˆ†é…å¯¦ä¾‹ï¼ˆthreadï¼‰ã€‚
    --vm-bytes 250Mï¼šæ¯å€‹åŸ·è¡Œç·’æœƒåˆ†é… 250MiB è¨˜æ†¶é«”ã€‚
    ğŸš¨ å•é¡Œï¼šå®¹å™¨çš„ limits.memory è¨­å®šç‚º 100Miï¼Œä½†æ‡‰ç”¨ç¨‹å¼å˜—è©¦åˆ†é… 250Miï¼Œé€™å¤§å¹…è¶…éé™åˆ¶ï¼Œå› æ­¤è¢« OOMKilledï¼

    æ‰€ä»¥
    Answer: The new version takes more moemory than the previous version and gets OOM killed


    Identify and fix the problem with the data-processordeployment
    controlplane ~ âœ  k get pod
    NAME                              READY   STATUS             RESTARTS       AGE
    api-alpha-cf697c9fc-74j68         0/1     CrashLoopBackOff   8 (14s ago)    16m
    cart-api-6df899869b-zcb2w         1/1     Running            0              22m
    data-processor-55d57797b8-fxxds   0/1     CrashLoopBackOff   14 (15s ago)   32m
    web-server                        0/1     CrashLoopBackOff   6 (14s ago)    6m34s

    controlplane ~ âœ  k logs data-processor-55d57797b8-fxxds 

    controlplane ~ âœ  k describe pod data-processor-55d57797b8-fxxds 
    ...
    Events:
    Type     Reason     Age                    From               Message
    ----     ------     ----                   ----               -------
    Normal   Scheduled  32m                    default-scheduler  Successfully assigned default/data-processor-55d57797b8-fxxds to node01
    Normal   Pulled     32m                    kubelet            Successfully pulled image "registry.k8s.io/busybox" in 398ms (398ms including waiting). Image size: 1144547 bytes.
    Normal   Pulled     32m                    kubelet            Successfully pulled image "registry.k8s.io/busybox" in 153ms (153ms including waiting). Image size: 1144547 bytes.
    Normal   Pulled     31m (x2 over 31m)      kubelet            Successfully pulled image "registry.k8s.io/busybox" in 158ms (158ms including waiting). Image size: 1144547 bytes.
    Normal   Created    30m (x5 over 32m)      kubelet            Created container: liveness
    Normal   Started    30m (x5 over 32m)      kubelet            Started container liveness
    Normal   Pulled     30m                    kubelet            Successfully pulled image "registry.k8s.io/busybox" in 143ms (143ms including waiting). Image size: 1144547 bytes.
    Normal   Killing    7m45s (x12 over 32m)   kubelet            Container liveness failed liveness probe, will be restarted
    Warning  Unhealthy  7m13s (x13 over 32m)   kubelet            **Liveness probe failed: cat: can't open '/tmp/healthy': No such file or directory**
    Warning  BackOff    2m33s (x108 over 30m)  kubelet            **Back-off restarting failed container liveness in pod** data-processor-55d57797b8-fxxds_default(53ca4e0c-5fb6-4517-ab83-5a8a757664b0)
    Normal   Pulling    100s (x15 over 32m)    kubelet            Pulling image "registry.k8s.io/busybox"
    

    controlplane ~ âœ  k edit deploy data-processor 
    ...
    spec:
    containers:
    - args:
        - /bin/sh
        - -c    
        - **sleep 30**; touch /tmp/healthy; sleep 3600
        image: registry.k8s.io/busybox
        imagePullPolicy: Always
        livenessProbe:
        exec:
            command:
            - cat
            - /tmp/healthy
        **failureThreshold: 3 #1  # å¤šå˜—è©¦2æ¬¡!**
        **initialDelaySeconds: ~~10~~ #2  # Container é è¨­å…ˆsleep 30ç§’ä¹‹å¾Œæ‰é‹è¡Œï¼Œæ‰€ä»¥å»ºè­°initialDelaySecondsè¦è¨­ç½®å¤§æ–¼ç­‰æ–¼30ç§’!**
        **periodSeconds: 10 #1**
        successThreshold: 1
        timeoutSeconds: 1
        name: liveness
        resources: {}

    deployment.apps/data-processor edited

    controlplane ~ âœ  k get pod
    NAME                              READY   STATUS             RESTARTS        AGE
    cart-api-6df899869b-zcb2w         1/1     Running            0               25m
    data-processor-55d57797b8-4cg78   1/1     Running            0               18s
    

3. Pending Pods (è·ŸPodèª¿åº¦æœ‰é—œ)
    i. ç•¶cluster ä¸Šå·²ç¶“æ²’æœ‰å……è¶³çš„è³‡æºå¯ä»¥åˆ†é…çµ¦podæ™‚ï¼Œå°‡æœƒä½¿podç‹€æ…‹ç‚ºpending
    ![Pending](../images/debug/pending03.png "Pending")
    ![Pending](../images/debug/pending.png "Pending")
    æŸ¥çœ‹ç•¶å‰nodesè³‡æº:
    ![Pending 2](../images/debug/pending02.png "Pending 02")

    å¾åœ–ç‰‡ä¸­æˆ‘å€‘å¯ä»¥çœ‹åˆ° Kubernetes å¢é›†ä¸­å…©å€‹ Node (`controlplane` å’Œ `node01`) çš„ **CPU ä½¿ç”¨ç‹€æ³**ï¼Œé€²è€Œè¨ˆç®—å¯ä¾›æ–° Pod èª¿åº¦çš„ CPU è³‡æºã€‚

    /### **ğŸ” 1ï¸âƒ£ è§£æ `CPU` è³‡è¨Š**
    | Node         | ç¸½ CPU | å·²ä½¿ç”¨ CPU | %CPU | å‰©é¤˜å¯ç”¨ CPU |
    |-------------|--------|-----------|------|------------|
    | controlplane | **8**  | **5%** (0.05 * 8 = 0.4) | 5% | **7.6** |
    | node01      | **3**  | **1%** (0.01 * 3 = 0.03) | 1% | **2.97** |

    /### **ğŸ”¢ 2ï¸âƒ£ è¨ˆç®—å¯ç”¨ CPU**
    å¯ç”¨ CPU = `ç¸½ CPU - å·²ä½¿ç”¨ CPU`
    - **`controlplane`**: `8 - 0.4 = 7.6`
    - **`node01`**: `3 - 0.03 = 2.97`


    /### **âœ… 3ï¸âƒ£ çµè«–**
    - `controlplane` **é‚„æœ‰** **7.6 CPU** å¯ä¾›æ–° Pod ä½¿ç”¨ã€‚
    - `node01` **é‚„æœ‰** **2.97 CPU** å¯ä¾›æ–° Pod èª¿åº¦ã€‚

    é€™æ„å‘³è‘—ï¼š
    - `controlplane` **èƒ½èª¿åº¦æ›´å¤šéœ€è¦é«˜ CPU è³‡æºçš„ Pod**ã€‚
    - `node01` ç”±æ–¼ CPU åªæœ‰ **3 é¡†**ï¼Œä½†ç•¶å‰ä½¿ç”¨ç‡å¾ˆä½ï¼Œ**ä»ç„¶å¯ä»¥èª¿åº¦ä¸€äº›å°å‹ Pod**ã€‚

    å¦‚æœè¦ç¢ºä¿æ–° Pod èƒ½é †åˆ©è¢«èª¿åº¦åˆ°é©åˆçš„ Nodeï¼Œå¯ä»¥ä½¿ç”¨ï¼š
    ```sh
    kubectl describe node controlplane
    kubectl describe node node01
    ```
    ä¾†æŸ¥çœ‹æ›´è©³ç´°çš„ `Allocatable CPU` å’Œ `Requests`ã€‚

    node01çš„å‰©é¤˜å¯ç”¨CPUæ•¸é‡ç‚º2.97ï¼Œå€˜è‹¥è¦å°‡data-processor podpendingè¢«èª¿åº¦åˆ°node01ä¸Šï¼Œ
    **åœ¨æœ‰é™çš„node01è³‡æºä¸Šæƒ³èª¿åº¦pod, åªèƒ½é™ä½podçš„request cpué‡!!**

    æª¢æŸ¥deploymenté…ç½®:
    ![Pending 05](../images/debug/pending05.png "Pending 05")
    
    è®Šæ›´å¾Œæª¢æŸ¥podå·²ç¶“è¢«èª¿åº¦
    ![Pending 06](../images/debug/pending06.png "Pending 06")


    ii. 1 node didtn't match Pod's node affinity or selecctor (podè·Ÿnodeä¸Šçš„labelsä¸ä¸€è‡´)
    podä¸Šçš„labelsæ²’æœ‰åœ¨nodeä¸Šçš„labelsä¸­
    ![Pending 07](../images/debug/pending07.png "Pending 07")

    æª¢æŸ¥node01 çš„labels: ç™¼ç¾æ²’æœ‰è¨­ç½®type=gpu
    ![Pending 08](../images/debug/pending08.png "Pending 08")

    ç”±æ–¼pod è¨­ç½®äº†nodeä¸Šæ²’æœ‰çš„labels, ç‚ºäº†èƒ½ä½¿mlapi podè¢«èª¿åº¦åˆ°node01ä¸Šï¼Œå‰‡æ‡‰åœ¨node01ä¸Šæ·»åŠ type=gpu labels!

    ![Pending 09](../images/debug/pending09.png "Pending 09")

    iii. 1 node has untolerated taint (podæ²’æœ‰è¨­ç½®toleration)
    ![Pending 04](../images/debug/pending04.png "Pending 04")

    æª¢æŸ¥node01ä¸Šçš„taints:
    ![Pending 10](../images/debug/pending10.png "Pending 10")

    åœ¨deploymentè¨­ç½®pod toleration:
    ![Pending 11](../images/debug/pending11.png "Pending 11")

    è¨­ç½®å¾Œä¾¿å¯çœ‹åˆ°podè¢«æˆåŠŸèª¿åº¦




4. Missing Pods (**CKAD1.32æ–°è§€å¿µ!! æœƒè€ƒ!!**) (è·ŸResourceQuota, serviceaccountæœ‰é—œ)
    i. MinimumReplicasUnavailable:  nodeä¸Šå¯èƒ½å®šç¾©äº†resource quotaå°è‡´podç„¡æ³•å…¨éƒ¨é…ç½®åœ¨nodeä¸Š
    deploymentå®šç¾©replicasæ‡‰ç‚º5å€‹pod,æœ€çµ‚å»åªç”Ÿæˆ2å€‹pod
    ![Missing 01](../images/debug/missing01.png "Missing 01")

    é€ék describe deploy æŒ‡ä»¤æŸ¥çœ‹eventsä¸¦ç„¡ç•°æ¨£:
    ![Missing 02](../images/debug/missing02.png "Missing 02")

    æ”¹ä½¿ç”¨: k get events -n staging æŸ¥çœ‹æ‰€æœ‰æ­·å²events
    ![Missing 03](../images/debug/missing03.png "Missing 03")

    æŸ¥çœ‹node01: ç™¼ç¾è¨­ç½®äº†Usedè·ŸHard Podæ•¸é‡
    Used = 5ï¼šç›®å‰ staging Namespace ä¸­å·²ç¶“æœ‰ 5 å€‹ Podã€‚
    Hard = 5ï¼šstaging æœ€å¤šå…è¨± 5 å€‹ Podï¼Œé€™è¡¨ç¤º æ–°çš„ Pod ä¸èƒ½è¢«èª¿åº¦ã€‚
    æ­¤æ™‚ï¼Œå¦‚æœå†å‰µå»ºæ–°çš„ Podï¼ŒKubernetes æœƒè¿”å›ï¼š
    Error from server (Forbidden): exceeded quota: pod-quota, requested: pods=1, used: 5, limited: 5
    é€™æ˜¯å› ç‚º Hard é™åˆ¶ç‚º 5ï¼ŒKubernetes ç„¡æ³•å†èª¿åº¦æ–°çš„ Podã€‚
    ![Missing 04](../images/debug/missing04.png "Missing 04")

    ä¿®æ”¹resource quotaçš„hardå€¼ (k edit resourcequota) ä¸¦ä¸”é‡å•Ÿdeployment (k rollout restart deploy -n staging api):
    staging Namespace ç¾åœ¨å…è¨±çš„ æœ€å¤§ Pod æ•¸é‡å¾ 5 å¢åŠ åˆ° 10ã€‚
    é€™ä¸€æ­¥å®Œæˆå¾Œï¼Œæ–°çš„ Pod å°±å¯ä»¥è¢«èª¿åº¦ï¼Œä½† Kubernetes éœ€è¦ä¸€å€‹è§¸ç™¼æ©Ÿåˆ¶ä¾†å¯¦éš›åŸ·è¡Œ Pod æ“´å±•
    ![Missing 05](../images/debug/missing05.png "Missing 05")

    ä¿®æ”¹å¾Œk get pod --watch ç™¼ç¾podæ•¸é‡è¿½åŠ è‡³æ»¿5å€‹:
    kubectl rollout restart deployment -n staging api
    é€™å€‹æŒ‡ä»¤çš„ä½œç”¨ï¼š
    å¼·åˆ¶ Deployment æ»¾å‹•æ›´æ–° (rollout restart)ï¼Œè®“æ‰€æœ‰ Pod é‡æ–°å‰µå»ºã€‚
    å› ç‚º ResourceQuota ç¾åœ¨å…è¨±æœ€å¤š 10 å€‹ Podï¼Œæ–°çš„ Pod å¯ä»¥æˆåŠŸå•Ÿå‹•ã€‚
    Kubernetes æœƒæ ¹æ“š replicas è¨­å®šä¾†å•Ÿå‹•æ–° Podã€‚
    ![Missing 06](../images/debug/missing06.png "Missing 06")

    
    â“ ç‚ºä½• Pod æ•¸é‡è®Šç‚º 5 è€Œä¸æ˜¯ 10ï¼Ÿ
    **ä¿®æ”¹ ResourceQuota åªæ±ºå®š Namespace å…è¨±çš„æœ€å¤§ Pod æ•¸é‡ï¼Œä½†å¯¦éš›é‹è¡Œå¤šå°‘ Pod å–æ±ºæ–¼ Deployment çš„ replicas è¨­å®š**ã€‚

    ä½ å¯ä»¥æª¢æŸ¥ Deploymentï¼š
    kubectl get deployment -n staging api -o yaml
    **å¦‚æœ replicas: 5ï¼Œå³ä½¿ ResourceQuota å…è¨± 10 å€‹ Podï¼Œä¹Ÿåªæœƒå•Ÿå‹• 5 å€‹ Podã€‚**


    ii. æ²’æœ‰è¨­ç½®service account:
    ![Missing 08](../images/debug/missing08.png "Missing 08")

    k describe pod ä¸‹çš„eventsä¸¦ç„¡ä»»ä½•æç¤ºï¼Œå‰‡ä½¿ç”¨k get events -n staging æŒ‡ä»¤ä¾†æŸ¥çœ‹:

    ![Missing 09](../images/debug/missing09.png "Missing 09")

    å‰µå»ºserviceaccountä¹‹å¾Œï¼Œk rollout restart deploy apiä¾¿å¯çœ‹åˆ°podæˆåŠŸå‰µå»º:
    ![Missing 10](../images/debug/missing10.png "Missing 10")    

        


5. SchrÃ¶dinger's Deployment è–›ä¸æ ¼éƒ¨ç½²: ç„¡æ³•ç¢ºå®šä¸€å€‹æ‡‰ç”¨ç¨‹å¼æ˜¯å¦çœŸæ­£æˆåŠŸéƒ¨ç½²ä¸¦é‹è¡Œï¼Œé™¤éä½ è¦ªè‡ªæª¢æŸ¥ã€‚ (å–„ç”¨k get endpointsæŒ‡ä»¤)
    æƒ…å¢ƒå¦‚: ä½ åŸ·è¡Œäº†éƒ¨ç½²ï¼Œä½†ä¸ç¢ºå®šå®ƒæ˜¯å¦æˆåŠŸ, éƒ¨ç½²éç¨‹æ²’æœ‰å ±éŒ¯ï¼Œä½†æ‡‰ç”¨ç¨‹å¼å¯èƒ½ç„¡æ³•æ­£å¸¸é‹è¡Œ, åœ¨æ¸¬è©¦ç’°å¢ƒä¸­ä¸€åˆ‡æ­£å¸¸ï¼Œä½†éƒ¨ç½²åˆ°æ­£å¼ç’°å¢ƒå¾Œå¯èƒ½å£æ‰,æ²’æœ‰é©ç•¶çš„æ—¥èªŒè¨˜éŒ„ã€ç›£æ§æˆ–è­¦å ±ç³»çµ±ï¼Œè®“åœ˜éšŠç„¡æ³•ç¢ºå®šéƒ¨ç½²çš„ç‹€æ…‹...etc
    
    **æŸ¥çœ‹ endpoint å¯ä»¥ç”¨ä¾†æ’æŸ¥ SchrÃ¶dinger's Deployment**
    åœ¨ Kubernetes å…§ï¼ŒService é€é selector æ‰¾åˆ°ç›¸æ‡‰çš„ Podï¼Œä¸¦ç”Ÿæˆ endpointsã€‚å¦‚æœ Service çš„ selector éŒ¯èª¤æˆ–ä¸åŒ¹é…ï¼Œå¯èƒ½æœƒå°è‡´æµé‡æ²’æœ‰å°å‘æ­£ç¢ºçš„ Podï¼Œé€ æˆæ‡‰ç”¨éƒ¨ç½²å¾Œç„¡æ³•ä½¿ç”¨çš„æƒ…æ³ï¼ˆå³ SchrÃ¶dinger's Deploymentï¼Œæ—¢å¯èƒ½é‹è¡Œï¼Œä¹Ÿå¯èƒ½ä¸é‹è¡Œï¼‰ã€‚

        a. æª¢æŸ¥ Service æ˜¯å¦å°‡æµé‡å°å‘æ­£ç¢ºçš„ Pod
        kubectl get endpoints å¯ä»¥é¡¯ç¤º Service ç¶å®šçš„ Pod IPã€‚
        è‹¥ endpoints åˆ—è¡¨ç‚ºç©ºï¼Œè¡¨ç¤º Service æ²’æœ‰åŒ¹é…åˆ°ä»»ä½• Podï¼Œé€™å¯èƒ½æ˜¯å› ç‚º label selector éŒ¯èª¤ã€‚
        
        b. æ’æŸ¥ Service selector å•é¡Œ
        kubectl describe svc <service-name> å¯æª¢æŸ¥ selector è¨­å®šã€‚
        kubectl get pods --show-labels æª¢æŸ¥ Pod çš„ labelsï¼Œç¢ºä¿èˆ‡ Service çš„ selector ç›¸åŒ¹é…ã€‚
        
        c. ç¢ºä¿æ‡‰ç”¨ç¨‹åºçš„ Pod æ­£åœ¨é‹è¡Œ
    
    kubectl get pods ç¢ºä¿ Pod ç‹€æ…‹ç‚º Runningï¼Œä¸” Ready=1/1ã€‚
    é€é kubectl get endpointsï¼Œä½ å¯ä»¥å¿«é€Ÿæª¢æŸ¥ï¼š
    Service æ˜¯å¦æœ‰ç¶å®š Podï¼Ÿ
    Pod æ˜¯å¦æœ‰è¢«éŒ¯èª¤çš„ Service ä»£ç†ï¼Ÿ
    æ˜¯å¦æœ‰ label é…ç½®éŒ¯èª¤å°è‡´çš„ Service è·¯ç”±éŒ¯èª¤ï¼Ÿ

    
    blue-service æœ€åˆçš„ selector éæ–¼å¯¬é¬†(åªæœ‰ {version: v1})ï¼ŒåŒ…å«äº† green çš„ Podï¼Œå°è‡´ æœå‹™æµé‡éŒ¯èª¤å°å‘ã€‚
    ä¿®æ­£ blue-service çš„ selector å¾Œ(è®Šæ›´ç‚º: {version: v1, app: blue} )ï¼Œgreen-service çš„ endpoints ä¹Ÿéš¨ä¹‹æ­£ç¢ºæ›´æ–°ï¼Œè§£æ±ºäº†æµé‡éŒ¯èª¤åˆ†ç™¼çš„å•é¡Œã€‚
    
    é€™æ˜¯ä¸€å€‹ç¶“å…¸çš„ Kubernetes Service selector é…ç½®éŒ¯èª¤å°è‡´ æµé‡æ··äº‚ çš„æ¡ˆä¾‹ï¼Œç¶“é kubectl get endpoints æ’æŸ¥ï¼ŒæˆåŠŸè§£æ±ºäº† SchrÃ¶dinger's Deployment å•é¡Œï¼ 


6. Create Container Errors (CreateContainerError & RunContaimerError -> sleepæŒ‡ä»¤è§£æ±ºä»¥å»¶é•·podå£½å‘½ä¸¦ä¸”åˆ©æ–¼åŸ·è¡Œk execé€²ä¸€æ­¥æ’æŸ¥)
    
    ![Container Error Types](../images/debug/container-error.png "Container Error Types")    

    i. Pull Image (è¦‹ä¸Šæ–¹1. Image Pull Errors)


    ii. Generate Container Configuration - CreateContainerConfigError: **é…ç½®éŒ¯èª¤ï¼Œå°è‡´å®¹å™¨ç„¡æ³•ç”Ÿæˆ**
    (éŒ¯èª¤ç™¼ç”Ÿæ–¼ Pod.spec åº•ä¸‹é…ç½®éŒ¯èª¤å¦‚: Volumeä¸­ConfigMap / Secret ä¸å­˜åœ¨, æˆ–æ˜¯æ‰¾ä¸åˆ°ç’°å¢ƒè®Šæ•¸)

    ç¯„ä¾‹1: è¦‹ä¸Šæ–¹2.iç¯„ä¾‹
    (éŒ¯èª¤ç™¼ç”Ÿæ–¼æ‰¾ä¸åˆ°ç’°å¢ƒè®Šæ•¸)
    | **éŒ¯èª¤ç·¨è™Ÿ** | **éŒ¯èª¤é¡å‹** | **åŸå› åˆ†æ** | **å°æ‡‰éŒ¯èª¤åˆ†é¡** |
    |------------|------------|------------|----------------|
    | **2-i. æŸ¥æ‰¾ä¸åˆ°ç’°å¢ƒè®Šæ•¸ (`env variables` ä¸å­˜åœ¨ï¼Œå°è‡´ Pod å´©æ½°ä¸¦é‡å•Ÿ)** | **ç’°å¢ƒè®Šæ•¸å•é¡Œ** | Pod éœ€è¦çš„ç’°å¢ƒè®Šæ•¸ï¼ˆ`env`ï¼‰æœªè¨­ç½®ï¼Œå¯èƒ½å› ç‚º `ConfigMap` æˆ– `Secret` ç¼ºå¤±ï¼Œå°è‡´æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•å¤±æ•—ã€‚ | **CreateContainerConfigError** |

    ç¯„ä¾‹2: 
    (éŒ¯èª¤ç™¼ç”Ÿæ–¼æ‰¾ä¸åˆ°secrets)
    k get events é¡¯ç¤ºæ‰¾ä¸åˆ°secrets
    ![Container Error Types](../images/debug/container-error06.png "Container Error Types")  
    æª¢æŸ¥podæ–‡ä»¶ï¼Œç™¼ç¾è©²secretsä¸å­˜åœ¨ï¼Œæ‰‹å‹•å»ºç«‹secrets:
    ![Container Error Types](../images/debug/container-error07.png "Container Error Types")  
    å»ºç«‹secretså¾Œï¼Œé‡å•Ÿpodï¼Œä½¿podæˆåŠŸrunning:
    ![Container Error Types](../images/debug/container-error08.png "Container Error Types")  



    iii. Create Container - CreateContainerError: **å®¹å™¨å·²å‰µå»ºï¼Œä½†é‹è¡Œç’°å¢ƒæœ‰å•é¡Œ**

    ç¯„ä¾‹1: è¦‹ä¸Šæ–¹2.iiiç¯„ä¾‹
    (éŒ¯èª¤ç™¼ç”Ÿæ–¼æ‰¾ä¸åˆ°volumeæ›è¼‰éŒ¯èª¤ï¼Œåƒæ˜¯æ‰¾ä¸åˆ°volumeMonut)
    | **éŒ¯èª¤ç·¨è™Ÿ** | **éŒ¯èª¤é¡å‹** | **åŸå› åˆ†æ** | **å°æ‡‰éŒ¯èª¤åˆ†é¡** |
    |------------|------------|------------|----------------|
    | **2-iii. `no such file or directory`** | **ConfigMap / Volume æ›è¼‰éŒ¯èª¤** | `Deployment` æœªæ›è¼‰ `ConfigMap` æˆ– `VolumeMount`ï¼Œå°è‡´æ‡‰ç”¨ç¨‹å¼æ‰¾ä¸åˆ°å¿…è¦çš„æ–‡ä»¶ | **CreateContainerError** |

    ç¯„ä¾‹2: æ·»åŠ sleep 3600ä¾†è§£æ±ºpodå› ç‚ºã€Œæ²’æœ‰åŸ·è¡Œçš„æŒ‡ä»¤ã€è€Œç«‹å³é€€å‡ºï¼Œå°è‡´éŒ¯èª¤
    (éŒ¯èª¤ç™¼ç”Ÿæ–¼ Kubelet å˜—è©¦å‰µå»ºå®¹å™¨ä½†ç™¼ç¾æ²’æœ‰å¯åŸ·è¡Œçš„å‘½ä»¤)
    ![Container Error Types](../images/debug/container-error02.png "Container Error Types")  
    æ·»åŠ æŒ‡ä»¤å¾Œå§‹podå…·æœ‰ä¸€å€‹æœ‰æ•ˆçš„å‘½ä»¤ (entry command)
    ![Container Error Types](../images/debug/container-error03.png "Container Error Types") 
    ä¿®æ”¹å®Œdeploymenté…ç½®å¾Œä¾¿å¯çœ‹åˆ°podç‹€æ…‹ç‚ºrunning
    ![Container Error Types](../images/debug/container-error09.png "Container Error Types") 

    è¦‹ä¸‹æ–¹[èªªæ˜]ã€‚


    iv. Start Container - RunContainerError: **å®¹å™¨æˆåŠŸå‰µå»ºï¼Œä½†é‹è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤**

    
    ç¯„ä¾‹1: è¦‹ä¸Šæ–¹2.iiç¯„ä¾‹
    | **éŒ¯èª¤ç·¨è™Ÿ** | **éŒ¯èª¤é¡å‹** | **åŸå› åˆ†æ** | **å°æ‡‰éŒ¯èª¤åˆ†é¡** |
    |------------|------------|------------|----------------|
    | **2-ii. Pod ç„¡æ³•åŸ·è¡Œ `exec` é€²å…¥å®¹å™¨ (`permission denied`)** | **æ¬Šé™å•é¡Œ** | å®¹å™¨å…§çš„ `script.sh` æ²’æœ‰åŸ·è¡Œæ¬Šé™ (`chmod +x` æœªè¨­ç½®)ï¼Œå°è‡´ç„¡æ³•å•Ÿå‹•æ‡‰ç”¨ç¨‹å¼ | **RunContainerError** |

    
    ç¯„ä¾‹2: è¦‹ä¸Šæ–¹2.ivç¯„ä¾‹
    | **éŒ¯èª¤ç·¨è™Ÿ** | **éŒ¯èª¤é¡å‹** | **åŸå› åˆ†æ** | **å°æ‡‰éŒ¯èª¤åˆ†é¡** |
    |------------|------------|------------|----------------|
    | **2-iv. `OOMKilled`: è¨˜æ†¶é«”è¶…é™ (Out-Of-Memory, OOM Killer)** | **è³‡æºä¸è¶³ (Memory é™åˆ¶è¶…å‡º)** | å®¹å™¨ä½¿ç”¨çš„ `Memory` è¶…é `limit`ï¼Œå°è‡´ç³»çµ±å¼·åˆ¶çµ‚æ­¢ | **RunContainerError** |
    
    ç¯„ä¾‹3: è¦‹ä¸Šæ–¹2.vç¯„ä¾‹
    | **éŒ¯èª¤ç·¨è™Ÿ** | **éŒ¯èª¤é¡å‹** | **åŸå› åˆ†æ** | **å°æ‡‰éŒ¯èª¤åˆ†é¡** |
    | **2-v. `Probe` æ¢é‡éŒ¯èª¤ï¼Œå°è‡´ Pod åè¦†é‡å•Ÿ** | **Liveness / Readiness æ¢æ¸¬å¤±æ•—** | `LivenessProbe` æˆ– `ReadinessProbe` è¨­ç½®éŒ¯èª¤ï¼Œå°è‡´å®¹å™¨å•Ÿå‹•å¾Œç«‹å³è¢«åˆ¤æ–·ç‚ºä¸å¥åº·ä¸¦é‡æ–°å•Ÿå‹• | **RunContainerError** |


    ç¯„ä¾‹4: è¦‹ä¸Šæ–¹2.viç¯„ä¾‹
    | **éŒ¯èª¤ç·¨è™Ÿ** | **éŒ¯èª¤é¡å‹** | **åŸå› åˆ†æ** | **å°æ‡‰éŒ¯èª¤åˆ†é¡** |
    | **2-vi. `connection refused`: LivenessProbe æ¢æ¸¬æ™‚é–“å¤ªçŸ­** | **æ‡‰ç”¨å•Ÿå‹•æ™‚é–“è¼ƒé•·ï¼Œæ¢æ¸¬æ™‚é–“éçŸ­** | `LivenessProbe` æ¢æ¸¬é–“éš”éçŸ­ï¼Œæ‡‰ç”¨é‚„æ²’æº–å‚™å¥½ | **RunContainerError** |


    ç¯„ä¾‹5:
    (éŒ¯èª¤ç™¼ç”Ÿæ–¼ å®¹å™¨æˆåŠŸå‰µå»ºä½†åŸ·è¡Œæ™‚æ‰¾ä¸åˆ°æŒ‡å®šçš„å‘½ä»¤)
    ![Container Error Types](../images/debug/container-error04.png "Container Error Types")  

    ![Container Error Types](../images/debug/container-error10.png "Container Error Types")

    ![Container Error Types](../images/debug/container-error05.png "Container Error Types")  
    ä¿®æ”¹å®Œdeploymenté…ç½®å¾Œä¾¿å¯çœ‹åˆ°podç‹€æ…‹ç‚ºrunning    

    è¦‹ä¸‹æ–¹èªªæ˜ã€‚

    [èªªæ˜]
    ç‚ºä½• CreateContainerErrorï¼ˆiii. ç¯„ä¾‹3ï¼‰èˆ‡ RunContainerErrorï¼ˆiv. ç¯„ä¾‹5ï¼‰çš„è™•ç†æ–¹æ³•éƒ½æ˜¯æ·»åŠ  command: ["sleep", "3600"]ï¼Ÿ
    åœ¨ iii. CreateContainerError å’Œ iv. RunContainerError çš„éŒ¯èª¤å ´æ™¯ä¸­ï¼ŒPod éƒ½ç„¡æ³•é †åˆ©å•Ÿå‹•ï¼Œè€Œè§£æ±ºæ–¹æ¡ˆéƒ½æ˜¯æ·»åŠ  command: ["sleep", "3600"]ã€‚é€™æ˜¯å› ç‚ºï¼š

    æ ¹æœ¬å•é¡Œï¼šå®¹å™¨æ²’æœ‰é è¨­çš„å•Ÿå‹•æŒ‡ä»¤

    åœ¨ iii. CreateContainerError ä¸­ï¼ŒéŒ¯èª¤ç™¼ç”Ÿæ–¼ Kubelet å˜—è©¦å‰µå»ºå®¹å™¨ä½†ç™¼ç¾æ²’æœ‰å¯åŸ·è¡Œçš„å‘½ä»¤ã€‚
    åœ¨ iv. RunContainerError ä¸­ï¼ŒéŒ¯èª¤ç™¼ç”Ÿæ–¼ å®¹å™¨æˆåŠŸå‰µå»ºä½†åŸ·è¡Œæ™‚æ‰¾ä¸åˆ°æŒ‡å®šçš„å‘½ä»¤ã€‚
    **Kubernetes éœ€è¦æœ‰ã€Œå¯åŸ·è¡Œçš„å‘½ä»¤ã€ï¼Œå¦å‰‡å®¹å™¨æœƒå¤±æ•—**

    å¦‚æœ**å®¹å™¨æ˜ åƒæœ¬èº«æ²’æœ‰ ENTRYPOINT æˆ– CMDï¼Œæˆ–è€…æŒ‡å®šçš„ command ç„¡æ³•åŸ·è¡Œï¼Œå‰‡å®¹å™¨æœƒå› ç‚ºã€Œæ²’æœ‰åŸ·è¡Œçš„æŒ‡ä»¤ã€è€Œç«‹å³é€€å‡ºï¼Œå°è‡´éŒ¯èª¤**ã€‚
    sleep 3600 æ˜¯æœ€ç°¡å–®çš„ä½”ä½å‘½ä»¤ï¼Œèƒ½å¤ ä¿æŒå®¹å™¨é‹è¡Œ

    sleep 3600 æœƒè®“å®¹å™¨åŸ·è¡Œ sleep å‘½ä»¤ï¼Œä½¿å…¶ä¿æŒé‹è¡Œ 3600 ç§’ï¼ˆ1 å°æ™‚ï¼‰ã€‚
    é€™æ¨£ï¼ŒPod æœƒé€²å…¥ Running ç‹€æ…‹ï¼Œè€Œä¸æœƒ CrashLoopBackOff æˆ– Exit 1ã€‚
    **é€™å°æ–¼ æ¸¬è©¦ã€é™¤éŒ¯ï¼ˆkubectl exec é€²å…¥å®¹å™¨ï¼‰å¾ˆæœ‰å¹«åŠ©**ã€‚


    /### **ç¸½çµ**
    | Kubernetes æ­¥é©Ÿ | éŒ¯èª¤é¡å‹ | å¯èƒ½çš„éŒ¯èª¤åŸå›  | æ’æŸ¥æ–¹å¼ |
    |----------------|------------------------|-----------------|---------------------|
    | **Generate Container Configuration** | `CreateContainerConfigError` | - PodSpec é…ç½®éŒ¯èª¤<br>- ConfigMap / Secret ä¸å­˜åœ¨<br>- Volume é…ç½®éŒ¯èª¤ | `kubectl describe pod`<br>`kubectl get configmap` |
    | **Create Container** | `CreateContainerError` | - è³‡æºä¸è¶³<br>- æ¬Šé™å•é¡Œ<br>- Storage æ›è¼‰å¤±æ•—<br>- Image ä¸å­˜åœ¨ | `kubectl describe pod`<br>`kubectl get pvc` |
    | **Start Container** | `RunContainerError` | - ç¨‹å¼å´©æ½° (Exit Code â‰  0)<br>- å•Ÿå‹•å‘½ä»¤éŒ¯èª¤<br>- Liveness / Readiness æ¢é‡å¤±æ•—<br>- ç„¡æ³•å­˜å–å¤–éƒ¨è³‡æº | `kubectl logs`<br>`kubectl describe pod` |

    é€éé€™äº›æ–¹æ³•ï¼Œä½ å¯ä»¥æœ‰æ•ˆæ’æŸ¥ Kubernetes çš„å®¹å™¨å•Ÿå‹•å•é¡Œï¼Œç¢ºä¿æ‡‰ç”¨éƒ¨ç½²é †åˆ©ï¼ ğŸš€


7. Config Out of Date

    ç¯„ä¾‹1: è®Šæ›´ConfigMap çš„data ä½†Podæ²’æœ‰æ›´æ–°ç²å–æ–°çš„envå€¼å€¼
    åœ¨ Kubernetes ä¸­ï¼Œç•¶ä½ ä¿®æ”¹äº† ConfigMap è£¡çš„ dataï¼Œä½†ç™¼ç¾é€²å…¥ pod å¾Œç’°å¢ƒè®Šæ•¸ (env) æ²’æœ‰è®Šæ›´ï¼Œé€™æ˜¯å› ç‚º ConfigMap çš„è®Šæ›´ä¸æœƒè‡ªå‹•æ›´æ–°å·²ç¶“é‹è¡Œä¸­çš„ Podã€‚é€™èˆ‡ ConfigMap çš„æ›è¼‰æ–¹å¼æœ‰é—œã€‚
    ![Config Out Of Date](../images/debug/config-outofdate.png "Config Out Of Date")

    ç‚ºä»€éº¼ä¿®æ”¹äº† ConfigMapï¼ŒPod è£¡çš„ç’°å¢ƒè®Šæ•¸æ²’æœ‰è®Šæ›´ï¼Ÿ
  
    ConfigMap å¯ä»¥è¢«æ›è¼‰åˆ° Pod ä¸­æœ‰å…©ç¨®æ–¹å¼ï¼š
    ä½œç‚ºç’°å¢ƒè®Šæ•¸ (envFrom or env)
    ä½œç‚º Volume æ›è¼‰ (volumeMounts)

    å¦‚æœ ConfigMap æ˜¯é€šéç’°å¢ƒè®Šæ•¸å‚³é (env) åˆ° Pod è£¡ï¼š
    ç’°å¢ƒè®Šæ•¸çš„å€¼åªæœƒåœ¨ Pod å•Ÿå‹•æ™‚åˆå§‹åŒ–ï¼Œä¹‹å¾Œå³ä½¿ ConfigMap æ›´æ–°äº†ï¼Œå·²é‹è¡Œçš„ Pod å…§éƒ¨ç’°å¢ƒè®Šæ•¸ä¸æœƒè®Šæ›´ã€‚
   
    è§£æ±ºæ–¹å¼ï¼šå¿…é ˆé‡å•Ÿ Podï¼Œè®“æ–°çš„ ConfigMap å€¼ç”Ÿæ•ˆã€‚
    
    å¦‚æœ ConfigMap æ˜¯ä½œç‚º Volume æ›è¼‰ï¼š
    Kubernetes æœƒè‡ªå‹•æª¢æ¸¬ ConfigMap è®Šæ›´ï¼Œä¸¦æ›´æ–° Volume å…§å®¹ï¼ˆé€šå¸¸æœƒæœ‰å¹¾ç§’é˜çš„å»¶é²ï¼‰ã€‚
    ä½†é€™åªé©ç”¨æ–¼æ–‡ä»¶é¡å‹çš„ ConfigMap æ›è¼‰ï¼Œè€Œéç’°å¢ƒè®Šæ•¸ã€‚

    rollout restart deployment -n production web-app æœƒåŸ·è¡Œ Deployment æ»¾å‹•é‡å•Ÿï¼š
    Kubernetes æœƒé€æ­¥çµ‚æ­¢èˆŠçš„ Podï¼Œä¸¦å‰µå»ºæ–°çš„ Podã€‚
    æ–°çš„ Pod æœƒç²å–æ›´æ–°å¾Œçš„ ConfigMap å€¼ä½œç‚ºç’°å¢ƒè®Šæ•¸ã€‚

    OR:
    å€˜è‹¥ä½¿ç”¨k reload -f deployment.yaml:
    **åƒ…æœƒæ›¿æ› Deployment ç‰©ä»¶ï¼Œä¸æœƒå½±éŸ¿æ­£åœ¨é‹è¡Œçš„ Podï¼Œé™¤é .spec.template æœ‰è®Šæ›´!!**ã€‚
    Pod å…§çš„ç’°å¢ƒè®Šæ•¸ä¸æœƒè¢«åˆ·æ–°ï¼Œä»ç„¶æ˜¯èˆŠçš„ ConfigMap å€¼
    é™¤éå…ˆ æ‰‹å‹•åˆªé™¤æ‰€æœ‰podä¹‹å¾Œï¼Œå†åŸ·è¡Œreload

    çµè«–
    ConfigMap è®Šæ›´å¾Œä¸æœƒè‡ªå‹•æ›´æ–°ç’°å¢ƒè®Šæ•¸ï¼Œå› ç‚ºç’°å¢ƒè®Šæ•¸æ˜¯ Pod å•Ÿå‹•æ™‚è¨­å®šçš„ã€‚

    è§£æ±ºæ–¹å¼
    æ¨è–¦æ–¹æ³•ï¼škubectl rollout restart deployment -n production web-app
    æ›¿ä»£æ–¹æ³•ï¼šæ‰‹å‹•åˆªé™¤ Pod (kubectl delete pod ...)ï¼Œè®“ Deployment è‡ªå‹•å‰µå»ºæ–° Podã€‚
    æœ€ä½³å¯¦è¸ï¼šä½¿ç”¨ ConfigMap Volume æ›è¼‰ï¼Œè€Œéç’°å¢ƒè®Šæ•¸ï¼Œè®“è®Šæ›´è‡ªå‹•ç”Ÿæ•ˆï¼ˆä½†å‰ææ˜¯æ‡‰ç”¨è¦èƒ½è®€å–æ–‡ä»¶è®Šæ›´ï¼‰ã€‚
    å¦‚æœä½ çš„æ‡‰ç”¨ç¨‹å¼èƒ½å¤ ç›£è½ ConfigMap è®Šæ›´ï¼Œå¯ä»¥ç”¨ Volume æ›è¼‰ï¼Œé¿å…æ¯æ¬¡æ”¹ ConfigMap éƒ½è¦é‡å•Ÿ Podï¼


    ç¯„ä¾‹2: Secretså€¼ decodeå¾Œæ­£ç¢ºä½†podç²å–äº†éŒ¯èª¤çš„ç’°å¢ƒè®Šæ•¸ï¼Œå‰‡é‡å•Ÿè©²podä½¿podè®€å–æ­£ç¢ºçš„secretså€¼

    readinessProbeæ‰¾ä¸åˆ°ä¸»æ©Ÿåç¨±:
    ![Config Out Of Date](../images/debug/config-outofdate02.png "Config Out Of Date")
    æª¢æŸ¥ç™¼ç¾hosté…ç½®æ–¼secretsï¼Œæª¢æŸ¥secretsé…ç½®
    ![Config Out Of Date](../images/debug/config-outofdate03.png "Config Out Of Date")
    æª¢æŸ¥secretsè£¡é¢çš„MYSQL_HOSTï¼Œdecodeä¹‹å¾Œçš„å€¼ç‚ºmysql, ä¸¦ç„¡éŒ¯èª¤:
    MYSQL_HOST: bXlzcWw= 
    echo "bxLzcWw=" |base -d (-d: decode)
    ![Config Out Of Date](../images/debug/config-outofdate04.png "Config Out Of Date")
    åŸ·è¡Œrollout restart deployä¹‹å¾Œï¼Œå¯çœ‹åˆ°PODç‹€æ…‹è·Ÿevnetså‡æ­£å¸¸:
    ![Config Out Of Date](../images/debug/config-outofdate05.png "Config Out Of Date")

8. Endlessly Terminating Pods (æ­é…--forceä½¿ç”¨)
    
    å¦‚ä¸‹ï¼Œå˜—è©¦çµ‚æ­¢podæ™‚å»ç„¡æ³•å®Œå…¨åˆªé™¤è©²pod
    ![Endlessly Terminating](../images/debug/endless.png "Endlessly Terminating")
    ç•¶åŸ·è¡Œ kubectl delete pod shipping-api-57cdd984bc-grq7g æ™‚ï¼ŒPod é€²å…¥ Terminating ç‹€æ…‹ï¼Œä½† æŒçºŒåœç•™åœ¨ Terminatingï¼Œæ²’æœ‰çœŸæ­£åˆªé™¤ï¼Œé€™é€šå¸¸æ˜¯ Pod å¡åœ¨çµ‚æ­¢ç‹€æ…‹ (endlessly terminating)ï¼Œå¯èƒ½æœ‰ä»¥ä¸‹å¹¾ç¨®åŸå› ï¼š
    æ‡‰ç”¨æ²’æœ‰æ­£ç¢ºè™•ç† SIGTERMï¼Œå°è‡´ç„¡æ³•å„ªé›…é—œé–‰ã€‚
    Pod æœ‰ Finalizerï¼Œé˜»æ­¢åˆªé™¤ã€‚
    Pod ä»ç„¶æŒæœ‰ PVCï¼Œç„¡æ³•åˆªé™¤ã€‚
    ç¶²çµ¡ (CNI) å•é¡Œå°è‡´ Pod å¡ä½

    é‡åˆ°ä»¥ä¸Šå•é¡Œæ™‚ï¼Œä½¿ç”¨: k delete pod <pod-name> --force è·³éæ­£å¸¸å„ªé›…é—œé–‰æµç¨‹ï¼Œå¼·åˆ¶ç§»é™¤pod

    ä»¥ä¸‹å±•ç¤ºè¼ƒå¸¸è¦‹çš„"Finalizer"å•é¡Œ:
    https://kubernetes.io/docs/concepts/overview/working-with-objects/finalizers/

    æª¢æŸ¥podæ˜¯å¦å…·æœ‰Finalizer:
    ![Endlessly Terminating](../images/debug/endless02.png "Endlessly Terminating")
    ![Endlessly Terminating](../images/debug/endless03.png "Endlessly Terminating")
    ç§»é™¤Finalizerå¾Œï¼Œä¾¿å¯æˆåŠŸåˆªé™¤è©²pod:
    ![Endlessly Terminating](../images/debug/endless04.png "Endlessly Terminating")

    æª¢æŸ¥namespaceæ˜¯å¦å…·æœ‰Finalizer:
    ![Endlessly Terminating](../images/debug/endless05.png "Endlessly Terminating")
    åŒæ¨£ç§»é™¤ä¹‹å¾Œï¼Œå¯æˆåŠŸåˆªé™¤è©²ns:
    ![Endlessly Terminating](../images/debug/endless06.png "Endlessly Terminating")




9. Field Immutability

    deploymentçš„spec.labelsåœ¨deploymentå»ºç«‹ä¹‹å¾Œä¾¿ä¸å¯ä¿®æ”¹ï¼Œè‹¥å˜—è©¦ä¿®æ”¹æˆ–æ–°å¢è©²æ¬„ä½å°‡æœƒå‡ºç¾å¦‚ä¸‹æç¤º:
    ![Immutable Field](../images/debug/immutable.png "Immutable Field")
    åªèƒ½åˆªé™¤è©²deploymenté‡æ–°å»ºç«‹:
    ![Immutable Field](../images/debug/immutable02.png "Immutable Field")


10. Enable Service Links (é€™CKADä¸æœƒè€ƒ)

    ![Enable Service Link](../images/debug/enableservicelink.png "Enable Service Link")
    ![Enable Service Link](../images/debug/enableservicelink02.png "Enable Service Link")
    
    å¦‚åœ–ï¼Œç‚ºä»€éº¼ app-frontend è®Šæˆ CrashLoopBackOffï¼Ÿ
    a. **ç’°å¢ƒè®Šæ•¸éå¤šå°è‡´ `Argument List Too Long`**
       - **è§£æ³•**ï¼šåœ¨ `Deployment` è¨­å®š `enableServiceLinks: false`ï¼Œé¿å… Kubernetes è‡ªå‹•åŠ å…¥ `ServiceLinks` ç’°å¢ƒè®Šæ•¸ã€‚

    b. **ConfigMap æˆ– Secret ç¼ºå¤±**
       - **è§£æ³•**ï¼šç¢ºèªå®ƒå€‘æ˜¯å¦å­˜åœ¨æ–¼æ–° namespaceï¼Œä¸¦æ‰‹å‹•è¤‡è£½ã€‚

    c. **ç„¡æ³•é€£æ¥å¾Œç«¯æœå‹™**
       - **è§£æ³•**ï¼šä½¿ç”¨ `nslookup` æ¸¬è©¦ DNSï¼Œç¢ºä¿æ­£ç¢ºé€£æ¥ `backend`ï¼Œä¸¦ä¿®æ”¹ `BACKEND_URL` è¨­å®šå®Œæ•´ FQDNã€‚

    è§£æ±ºå®Œé€™äº›å•é¡Œå¾Œï¼ŒåŸ·è¡Œï¼š
    ```bash
    kubectl delete pod app-frontend-5d55d67ccc-2dwdd -n <new-namespace>
    ```
    è®“ Kubernetes é‡æ–°å•Ÿå‹• Podï¼Œæª¢æŸ¥æ˜¯å¦æ¢å¾©æ­£å¸¸é‹è¡Œ


    ä»¥ä¸‹å±•ç¤ºa. ç’°å¢ƒè®Šæ•¸éå¤šå°è‡´ Argument List Too Long:

    | æ–¹æ¡ˆ | å„ªé» | ç¼ºé» |
    ||||
    | **Primary Approach - DNS Plugin** | ä¸æœƒç”¢ç”Ÿéå¤šç’°å¢ƒè®Šæ•¸ï¼Œé©åˆå¤§è¦æ¨¡ç³»çµ± | éœ€è¦æ‡‰ç”¨ç¨‹å¼æ”¯æ´ DNS è§£æ |
    | **Secondary Approach - Environment Variables** | ç°¡å–®æ˜“ç”¨ | Service éå¤šæ™‚ï¼Œå¯èƒ½å°è‡´ `Argument List Too Long` å•é¡Œ |

    è§£æ±ºæ–¹æ¡ˆï¼šè¨­ç½® `enableServiceLinks: false`
    - åœ¨ Deployment ä¸­ **é¡¯å¼é—œé–‰** `enableServiceLinks`ï¼Œé¿å… Kubernetes è‡ªå‹•æ³¨å…¥ Service ç’°å¢ƒè®Šæ•¸ï¼š
      ```yaml
      spec:
        enableServiceLinks: false
      ```
    - é€™æ¨£ Kubernetes å°±ä¸æœƒå°‡ `SERVICE_<NAME>_HOST` å’Œ `SERVICE_<NAME>_PORT` é€™äº›è®Šæ•¸æ³¨å…¥åˆ° Podï¼Œå¾è€Œ**æ¸›å°‘ç’°å¢ƒè®Šæ•¸æ•¸é‡ï¼Œé¿å… `Argument List Too Long` å•é¡Œ**ã€‚

    é è¨­ç‚ºenable,é™¤é**æ‰‹å‹•è¨­ç½®ç‚ºfalse**: enableSerivceLink: false
    ![Enable Service Link](../images/debug/enableservicelink03.png "Enable Service Link") 

    ![Enable Service Link](../images/debug/enableservicelink04.png "Enable Service Link") 


11. Troubleshooting  Combo Pratcice:
    
    A node no longer has capacity for a pod to be scheduled, what will be the state of the pod?
    Answer: Pending

    
    In which of the following scenarios would you have to delete the object and apply it again for changes to take effect?
    option1: Adding a new value in a configmap
    option2: Decreasing container resource requests of deployment
    option3: Modifying the value of an existing label selector
    option4: Adding a new value in a secret

    Answer: option3 (å› ç‚ºæœ‰äº›spec.propertiesæ˜¯immutable)


    Which of the following will cause a CreateContainerError?
    option1: Incorrect start command -> RunContainerError
    option2: Missing Configmap -> CreateContainerConfigError
    option3: Insufficient resources -> å¦‚æœ Pod è«‹æ±‚çš„ CPU æˆ–è¨˜æ†¶é«”è¶…éç¯€é»ä¸Šå¯ç”¨çš„æ•¸é‡ï¼Œå‰‡ Kubernetes å°‡ç„¡æ³•å»ºç«‹å®¹å™¨ã€‚

    Answer: option3


    How does setting enableServiceLinks: false impact pod networking in Kubernetes?
    Answer: It disables the automatic injection of environment variables related to services in the pod.


    Youâ€™re trying to deploy a pod v2-release-testing in the devnamespace, but because the cluster admins know developers tend to abuse the development cluster, they created a resource quota restricting the number of pods on this namespace.
    Without modifying the resource quota, deploy the pod alpha-release
    Manifest file for v2-release-testing is present at /root/v2-release-testing.yml

    Hint: You can delete pods of old release testing v1-release-testing
    The pod may take a long time to terminate, use what you learned in the previous lessons to force delete the pod

    controlplane ~ âœ  k get pod -n dev
    NAME                 READY   STATUS    RESTARTS        AGE
    analytics            1/1     Running   0               9m50s
    api                  1/1     Running   0               9m51s
    cart-api             1/1     Running   0               9m50s
    data-processor       1/1     Running   0               9m51s
    database-pod         1/1     Running   0               9m51s
    feedback-api         1/1     Running   0               9m50s
    ml-api               1/1     Running   1 (9m36s ago)   9m51s
    search               1/1     Running   0               9m50s
    user-api             1/1     Running   0               9m50s
    v1-release-testing   1/1     Running   0               9m51s

    controlplane ~ âœ  k get resourcequotas -n dev
    NAME        AGE   REQUEST       LIMIT
    pod-quota   10m   pods: 10/10   

    controlplane ~ âœ  k describe resourcequotas -n dev pod-quota 
    Name:       pod-quota
    Namespace:  dev
    Resource    Used  Hard
    --------    ----  ----
    pods        10    10

    controlplane ~ âœ  cat v2-release-testing.yml 
    apiVersion: v1
    kind: Pod
    metadata:
        name: v2-release-testing
        namespace: dev
    spec:
        containers:
        - name: nginx-container
            image: nginx:latest
            ports:
            - containerPort: 80

    åœ¨ Kubernetes å¢é›†ä¸­ï¼Œç”±æ–¼ namespace dev è¨­æœ‰ è³‡æºé…é¡ (ResourceQuota)ï¼Œé™åˆ¶è©²å‘½åç©ºé–“å…§æœ€å¤šåªèƒ½æœ‰ 10 å€‹ podsã€‚åœ¨é€™å€‹æƒ…å¢ƒä¸‹ï¼Œç•¶æˆ‘å€‘æƒ³è¦éƒ¨ç½² v2-release-testing pod æ™‚ï¼Œç™¼ç¾ dev å‘½åç©ºé–“å·²ç¶“é”åˆ° 10/10 çš„ pod é™åˆ¶ï¼Œå°è‡´ç„¡æ³•å†æ–°å¢æ–°çš„ podã€‚

    ç‚ºä½•è¦åˆªé™¤ v1-release-testing podï¼Ÿ
    å› ç‚º è³‡æºé…é¡å·²æ»¿ (10/10)ï¼Œæ‰€ä»¥è‹¥ä¸åˆªé™¤èˆŠçš„ v1-release-testing podï¼Œå°±ç„¡æ³•å†æ–°å¢ v2-release-testing podã€‚é€™æ˜¯ä¸€ç¨® è³‡æºå›æ”¶ çš„ç­–ç•¥ï¼Œåœ¨ä¸è®Šæ›´è³‡æºé…é¡çš„å‰æä¸‹ï¼Œé€éåˆªé™¤èˆŠçš„ podï¼Œé‡‹æ”¾å‡ºä¸€å€‹å¯ç”¨çš„ pod é…é¡ã€‚

    controlplane ~ âœ  k delete pod -n dev v1-release-testing --force
    Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.
    pod "v1-release-testing" force deleted

    controlplane ~ âœ  k get pod -n dev
    NAME             READY   STATUS    RESTARTS      AGE
    analytics        1/1     Running   0             15m
    api              1/1     Running   0             15m
    cart-api         1/1     Running   0             15m
    data-processor   1/1     Running   0             15m
    database-pod     1/1     Running   0             15m
    feedback-api     1/1     Running   0             15m
    ml-api           1/1     Running   1 (15m ago)   15m
    search           1/1     Running   0             15m
    user-api         1/1     Running   0             15m

    controlplane ~ âœ  k apply -f v2-release-testing.yml 
    pod/v2-release-testing created

    controlplane ~ âœ  k get pod -n dev
    NAME                 READY   STATUS    RESTARTS      AGE
    analytics            1/1     Running   0             15m
    api                  1/1     Running   0             15m
    cart-api             1/1     Running   0             15m
    data-processor       1/1     Running   0             15m
    database-pod         1/1     Running   0             15m
    feedback-api         1/1     Running   0             15m
    ml-api               1/1     Running   1 (15m ago)   15m
    search               1/1     Running   0             15m
    user-api             1/1     Running   0             15m
    v2-release-testing   1/1     Running   0             2s
    
    
    Your team has to deploy web-a and web-b, which are 2 web services. Youâ€™re reviewing the manifests and notice that the apps contain the same label values and that label is used as a selector in the service. Why is this a problem?

    option1: The shared label selectors will result in indiscriminate traffic routing, leading to potential misrouting of requests between the two services.
    option2: This is not a problem at all, the apps will be deployed normally.

    Answer: option1


    Youâ€™ll find webapp-color-v1 and webapp-color-v2 deployed in the default namespace. Fix the issue mentioned in the previous task.
    Adjust the label on webapp-color-v1 to be app: webapp-color-v1 and on webapp-color-v2 to be app: webapp-color-v2

    controlplane ~ âœ  k get deploy
    NAME              READY   UP-TO-DATE   AVAILABLE   AGE
    feedback          0/1     1            0           20m
    webapp            0/1     1            0           20m
    webapp-color-v1   1/1     1            1           20m
    webapp-color-v2   1/1     1            1           20m


    controlplane ~ âœ  k get svc
    NAME                      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
    kubernetes                ClusterIP   10.96.0.1        <none>        443/TCP        71m
    webapp-color-v1-service   NodePort    10.102.121.149   <none>        80:30080/TCP   24m
    webapp-color-v2-service   NodePort    10.107.20.61     <none>        80:30081/TCP   24m

    controlplane ~ âœ  k describe svc webapp-color-v1-service 
    Name:                     webapp-color-v1-service
    Namespace:                default
    Labels:                   <none>
    Annotations:              <none>
    Selector:                 app=web
    Type:                     NodePort
    IP Family Policy:         SingleStack
    IP Families:              IPv4
    IP:                       10.102.121.149
    IPs:                      10.102.121.149
    Port:                     <unset>  80/TCP
    TargetPort:               8080/TCP
    NodePort:                 <unset>  30080/TCP
    Endpoints:                10.244.192.10:8080,10.244.192.12:8080
    Session Affinity:         None
    External Traffic Policy:  Cluster
    Internal Traffic Policy:  Cluster
    Events:                   <none>

    controlplane ~ âœ  k describe svc webapp-color-v2-service 
    Name:                     webapp-color-v2-service
    Namespace:                default
    Labels:                   <none>
    Annotations:              <none>
    Selector:                 app=web
    Type:                     NodePort
    IP Family Policy:         SingleStack
    IP Families:              IPv4
    IP:                       10.107.20.61
    IPs:                      10.107.20.61
    Port:                     <unset>  80/TCP
    TargetPort:               8080/TCP
    NodePort:                 <unset>  30081/TCP
    Endpoints:                10.244.192.10:8080,10.244.192.12:8080
    Session Affinity:         None
    External Traffic Policy:  Cluster
    Internal Traffic Policy:  Cluster
    Events:                   <none>

    
    controlplane ~ âœ  k get deploy webapp-color-v1 -o yaml > webapp-color-v1.yaml
    controlplane ~ âœ  k get deploy webapp-color-v2 -o yaml > webapp-color-v2.yaml

    controlplane ~ âœ  vim webapp-color-v1.yaml 
    controlplane ~ âœ  vim webapp-color-v2.yaml 

    controlplane ~ âœ  k replace -f webapp-color-v2.yaml --force
    deployment.apps "webapp-color-v2" deleted
    deployment.apps/webapp-color-v2 replaced

    controlplane ~ âœ  k get deploy
    NAME              READY   UP-TO-DATE   AVAILABLE   AGE
    feedback          0/1     1            0           26m
    webapp            0/1     1            0           26m
    webapp-color-v1   1/1     1            1           13s
    webapp-color-v2   1/1     1            1           6s


    controlplane ~ âœ  k get  svc webapp-color-v1-service -o yaml > webapp-color-v1-service.yaml

    controlplane ~ âœ  k get  svc webapp-color-v2-service -o yaml > webapp-color-v2-service.yaml

    controlplane ~ âœ  vim webapp-color-v1-service.yaml 

    controlplane ~ âœ  vim webapp-color-v2-service.yaml 

    controlplane ~ âœ  k replace -f  webapp-color-v1-service.yaml --force
    service "webapp-color-v1-service" deleted
    service/webapp-color-v1-service replaced

    controlplane ~ âœ  k replace -f  webapp-color-v2-service.yaml --force
    service "webapp-color-v2-service" deleted
    service/webapp-color-v2-service replaced

    controlplane ~ âœ  k get svc
    NAME                      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
    kubernetes                ClusterIP   10.96.0.1        <none>        443/TCP        76m
    webapp-color-v1-service   NodePort    10.102.121.149   <none>        80:30080/TCP   9s
    webapp-color-v2-service   NodePort    10.107.20.61     <none>        80:30081/TCP   4s

    controlplane ~ âœ  k describe svc webapp-color-v1-service 
    Name:                     webapp-color-v1-service
    Namespace:                default
    Labels:                   <none>
    Annotations:              <none>
    Selector:                 app=webapp-color-v1
    Type:                     NodePort
    IP Family Policy:         SingleStack
    IP Families:              IPv4
    IP:                       10.102.121.149
    IPs:                      10.102.121.149
    Port:                     <unset>  80/TCP
    TargetPort:               8080/TCP
    NodePort:                 <unset>  30080/TCP
    Endpoints:                10.244.192.20:8080
    Session Affinity:         None
    External Traffic Policy:  Cluster
    Internal Traffic Policy:  Cluster
    Events:                   <none>

    controlplane ~ âœ  k describe svc webapp-color-v2-service 
    Name:                     webapp-color-v2-service
    Namespace:                default
    Labels:                   <none>
    Annotations:              <none>
    Selector:                 app=webapp-color-v2
    Type:                     NodePort
    IP Family Policy:         SingleStack
    IP Families:              IPv4
    IP:                       10.107.20.61
    IPs:                      10.107.20.61
    Port:                     <unset>  80/TCP
    TargetPort:               8080/TCP
    NodePort:                 <unset>  30081/TCP
    Endpoints:                10.244.192.21:8080
    Session Affinity:         None
    External Traffic Policy:  Cluster
    Internal Traffic Policy:  Cluster
    Events:                   <none>


    What cause the webapp pod not running?

    controlplane ~ âœ  k get events|grep webapp
    31m         Normal    Scheduled           pod/webapp-76455b47bf-2x6gf             Successfully assigned default/webapp-76455b47bf-2x6gf to node01
    100s        Normal    Pulling             pod/webapp-76455b47bf-2x6gf             Pulling image "redis"
    31m         Normal    Pulled              pod/webapp-76455b47bf-2x6gf             Successfully pulled image "redis" in 171ms (32.922s including waiting). Image size: 45013665 bytes.
    73s         Warning   Failed              pod/webapp-76455b47bf-2x6gf             Error: configmap "myconfigmap" not found
    30m         Normal    Pulled              pod/webapp-76455b47bf-2x6gf             Successfully pulled image "redis" in 286ms (17.36s including waiting). Image size: 45013665 bytes.
    30m         Normal    Pulled              pod/webapp-76455b47bf-2x6gf             Successfully pulled image "redis" in 165ms (165ms including waiting). Image size: 45013665 bytes.


      template:
        metadata:
        creationTimestamp: null
        labels:
            app: webapp
        spec:
        containers:
            #- envFrom:
            #- configMapRef:
            #    name: myconfigmap
        - image: redis
            imagePullPolicy: Always
            name: webapp
            resources: {}
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File

    controlplane ~ âœ– vim webapp.yaml 

    controlplane ~ âœ  k replace -f webapp.yaml --force 
    deployment.apps/webapp replaced

    controlplane ~ âœ  k get deploy
    NAME              READY   UP-TO-DATE   AVAILABLE   AGE
    feedback          0/1     1            0           40m
    webapp            1/1     1            1           3s
    webapp-color-v1   1/1     1            1           14m
    webapp-color-v2   1/1     1            1           14m

    controlplane ~ âœ  k get pod
    NAME                               READY   STATUS                 RESTARTS   AGE
    feedback-7b4c575c64-xtxgj          0/1     CreateContainerError   0          40m
    webapp-667c99746b-gskmb            1/1     Running                0          6s
    webapp-color-v1-667df85f7f-9j9f9   1/1     Running                0          14m
    webapp-color-v2-6c7ddbd6b9-hsmfl   1/1     Running                0          14m

    Answer: Missing ConfigMap


    Deployment feedback is facing errors in creation. Investigate the cause. What seems to be the problem this time?

    controlplane ~ âœ  k get pod
    NAME                               READY   STATUS                 RESTARTS   AGE
    feedback-7b4c575c64-xtxgj          0/1     CreateContainerError   0          40m
    webapp-667c99746b-gskmb            1/1     Running                0          6s
    webapp-color-v1-667df85f7f-9j9f9   1/1     Running                0          14m
    webapp-color-v2-6c7ddbd6b9-hsmfl   1/1     Running                0          14m

    controlplane ~ âœ  k get events |grep feedback
    43m         Normal    Scheduled           pod/feedback-7b4c575c64-xtxgj           Successfully assigned default/feedback-7b4c575c64-xtxgj to node01
    2m57s       Normal    Pulling             pod/feedback-7b4c575c64-xtxgj           Pulling image "rakshithraka/entrypoint:latest"
    42m         Normal    Pulled              pod/feedback-7b4c575c64-xtxgj           Successfully pulled image "rakshithraka/entrypoint:latest" in 17.985s (32.845s including waiting). Image size: 349284039 bytes.
    42m         Warning   Failed              pod/feedback-7b4c575c64-xtxgj           Error: failed to generate container "63d1dddf8813defeb26ea79e4458c5b42ab5e328fa2c6d182684b8de40b91c8f" spec: failed to generate spec: no command specified

    Answer: Start command not specified


    One of the application requirements is high availability, so you increase the number of replicas of the api deployment to 10. You first try this out on the testing namespace, but notice the number of replicas is not 10. Identify the cause.
    optiob1: Insufficient resources
    option2: Replicas exceed number of pods set for namespace

    controlplane ~ âœ  k get deploy -n testing 
    NAME   READY   UP-TO-DATE   AVAILABLE   AGE
    api    5/10    5            5           43m

    controlplane ~ âœ  k get resourcequotas -n testing 
    NAME        AGE   REQUEST     LIMIT
    pod-quota   44m   pods: 5/5   

    controlplane ~ âœ  k describe resourcequotas -n testing 
    Name:       pod-quota
    Namespace:  testing
    Resource    Used  Hard
    --------    ----  ----
    pods        5     5

    Answer: option2

12. RBAC Troubleshooting


13. Netowrk Troubleshooting


14. Ingress Troubleshooting


15. Storage Troubleshoting
