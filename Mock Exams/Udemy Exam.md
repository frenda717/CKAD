### 01. Application Deployment (20%)

  1. CronJobã€Job Limitã€BackoffLimit
  
      Create a cronjob called dice that runs every one minute. Use the Pod template located at /root/throw-a-dice. The image throw-dice randomly returns a value between 1 and 6. The result of 6 is considered success and all others are failure.
      The job should be **non-parallel** and **complete the task once**. Use a backoffLimit of 25.
      
      If the task is not completed within 20 seconds the job should fail and pods should be terminated.
      You don't have to wait for the job completion. As long as the cronjob has been created as per the requirements.

      æˆ‘çš„é…ç½®:

      controlplane ~ âœ  ls
      dice-conjob.yaml  dice-job.yaml  throw-a-dice

      controlplane ~ âœ  cat throw-a-dice/throw-a-dice.yaml (æŸ¥çœ‹ç•¶å‰pod è¨­ç½®)
        
        apiVersion: v1
        kind: Pod
        metadata:
          name: throw-dice-pod
        spec:
          containers: # æ ¹æ“šdoc: "For a non-parallel Job, you can leave both .spec.completions and .spec.parallelism unset. When both are unset, both are defaulted to 1" ï¼Œæ•…æ­¤è™•ä¸è¨­ç½®parallelism è·Ÿcompletionsï¼Œé»˜èªç‚º1ã€‚
          -  image: kodekloud/throw-dice (æª¢æŸ¥Imgage)
            name: throw-dice
          restartPolicy: Never

      controlplane ~ âœ  cat dice-job.yaml
        
        apiVersion: batch/v1
        kind: Job
        metadata:
          name: dice-job
        spec:
          template:
            spec:
              restartPolicy: Never # required for the feature
              containers:
              - name: throw-dice
                image: kodekloud/throw-dice
          backoffLimit: 25
          activeDeadlineSeconds: 20


      controlplane ~ âœ  k get job
      NAME       STATUS     COMPLETIONS   DURATION   AGE
      dice-job   Complete   1/1           4s         7m38s

      ä½†æ˜¯!!!ç›®å‰å»ºç«‹çš„ dice-job.yaml æ˜¯ä¸€å€‹ Jobï¼Œä½†é¡Œç›®è¦æ±‚ CronJobï¼Œæ‰€ä»¥æˆ‘å€‘éœ€è¦ä¿®æ”¹ apiVersion å’Œ kindï¼š

      Job åªæœƒåŸ·è¡Œä¸€æ¬¡
      CronJob å‰‡å¯ä»¥æŒ‰ç…§æ’ç¨‹åŸ·è¡Œï¼ˆå¦‚é¡Œç›®è¦æ±‚çš„ã€Œæ¯åˆ†é˜ä¸€æ¬¡ã€ï¼‰


      æ ¹æ“šjobçš„é…ç½®ï¼Œå°‡template specè£¡é¢çš„å…§å®¹è²¼åˆ°cronjobé…ç½®æ–‡ä»¶ä¸­:

        apiVersion: batch/v1
        kind: CronJob
        metadata:
          name: dice
        spec:
          schedule: "* * * * *"
          jobTemplate:
            spec:
              template:
                spec:
                  restartPolicy: Never # required for the feature
                  containers:
                  - name: throw-dice
                    image: kodekloud/throw-dice
              backoffLimit: 25
              activeDeadlineSeconds: 20 (ä½¿ç”¨k explain cronjob.spec.jobTemplate.spec.template --recursiveæŸ¥çœ‹æ˜¯å¦æœ‰æ­¤é…ç½®é …)

      controlplane ~ âœ  vim dice-conjob.yaml 
      
      OR
      
      controlplane ~ âœ  kubectl create cronjob dice --image=kodekloud/throw-dice --schedule="*/1 * * * *"
      cronjob.batch/dice created

      controlplane ~ âœ  k edit cronjobs.batch 
        spec:
          concurrencyPolicy: Allow
          failedJobsHistoryLimit: 1
          jobTemplate:
            metadata:
              creationTimestamp: null
              name: dice
            spec:
              activeDeadlineSeconds: 20
              backoffLimit: 25
              template:
                metadata:
                  creationTimestamp: null
                spec:
                  containers:
                  - image: kodekloud/throw-dice
                    imagePullPolicy: Always
                    name: dice
                    resources: {}
                    terminationMessagePath: /dev/termination-log
                    terminationMessagePolicy: File
                  dnsPolicy: ClusterFirst
                  restartPolicy: OnFailure
                  schedulerName: default-scheduler
                  securityContext: {}
                  terminationGracePeriodSeconds: 30
          schedule: '*/1 * * * *'

      cronjob.batch/dice edited

      controlplane ~ âœ  k get job --watch
      NAME            STATUS     COMPLETIONS   DURATION   AGE
      dice-28981055   Complete   1/1           5s         97s
      dice-28981056   Complete   1/1           18s        37s
      dice-28981057   Running    0/1                      0s
      dice-28981057   Running    0/1           0s         0s
      dice-28981057   Running    0/1           3s         3s
      dice-28981057   Complete   1/1           3s         3s

      controlplane ~ âœ  k create -f  dice-conjob.yaml 
      cronjob.batch/dice created

      controlplane ~ âœ  k get cronjobs 
      NAME   SCHEDULE    TIMEZONE   SUSPEND   ACTIVE   LAST SCHEDULE   AGE
      dice   * * * * *   <none>     False     0        <none>          4s


      controlplane ~ âœ– kubectl get cronjobs --watch
      NAME   SCHEDULE    TIMEZONE   SUSPEND   ACTIVE   LAST SCHEDULE   AGE
      dice   * * * * *   <none>     False     0        48s             90s
      dice   * * * * *   <none>     False     1        0s              102s
      dice   * * * * *   <none>     False     0        4s              106s

      é–‹å¦ä¸€å€‹åˆ†é ï¼Œè§€å¯Ÿpod

      controlplane ~ âœ  **kubectl get pods --watch**
      NAME                  READY   STATUS      RESTARTS   AGE
      dev-pod-dind-878516   3/3     Running     0          25m
      dice-28980667-9k4l2   0/1     Error       0          67s
      dice-28980667-jj27l   0/1     Completed   0          56s
      dice-28980668-tb9gb   0/1     Completed   0          7s
      dice-job-s88jn        0/1     Completed   0          12m
      pod-xyz1123           1/1     Running     0          25m
      webapp-color          1/1     Running     0          25m

      CronJob æœƒæ ¹æ“š schedule è‡ªå‹•ç”¢ç”Ÿ Jobï¼Œç„¶å¾Œ Job è² è²¬åŸ·è¡Œå…·é«”çš„ Podã€‚æ•…æª¢æŸ¥jobæ˜¯å¦è¢«å»ºç«‹:

      controlplane ~ âœ  **kubectl get jobs --watch**
      NAME            STATUS     COMPLETIONS   DURATION   AGE
      dice-28980667   Complete   1/1           15s        3m42s
      dice-28980668   Complete   1/1           4s         2m42s
      dice-28980669   Complete   1/1           15s        102s
      dice-28980670   Failed     0/1           42s        42s
      dice-job        Complete   1/1           4s         14m


      controlplane ~ âœ– k get jobs
      NAME            STATUS     COMPLETIONS   DURATION   AGE
      dice-28980667   Complete   1/1           15s        2m29s
      dice-28980668   Complete   1/1           4s         89s
      dice-28980669   Complete   1/1           15s        29s
      dice-job        Complete   1/1           4s         13m



### 02. Application Environment, Configuration and Security (25%)  

1. Pod + ConfigMap + Volume
    Create a pod called time-check in the dvl1987 namespace. 
    This pod should run a container called time-check that uses the busybox image.

    Create a config map called time-config with the data TIME_FREQ=10 in the same namespace.
    The time-check container should run the command: while true; do date; sleep $TIME_FREQ;done and write the result to the location /opt/time/time-check.log.
    The path /opt/time on the pod should mount a volume that lasts the lifetime of this pod.

    æˆ‘çš„é…ç½®:

    controlplane ~ âœ  k get pod time-check -n dvl1987 -o yaml > time-check-pod.yaml

    controlplane ~ âœ  k delete pod -n dvl1987 time-check 
    pod "time-check" deleted


    controlplane ~ âœ  vim time-check-pod.yaml 
    
    ---
    apiVersion: v1
    kind: Pod
    metadata:
      name: dapi-test-pod
      namespace: dvl1987
    spec:
      containers:
        - name: test-container
          image: registry.k8s.io/busybox
          command: [ "/bin/sh", "-c", "while true; do date; sleep $TIME_FREQ;done >  /opt/time/time-check.log" ]
          env:
            # Define the environment variable
            - name: TIME_FREQ
              valueFrom:
                configMapKeyRef:
                  # The ConfigMap containing the value you want to assign to SPECIAL_LEVEL_KEY
                  name: time-config
                  # Specify the key associated with the value
                  key: TIME_FREQ
        volumeMounts:
          - name: log-volume
          mountPath: /opt/time

    volumes:
    - name: log-volume
      ~~configMap:~~
        ~~name: time-config~~

    ---
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: time-config
      namespace: dvl1987
    data:
      TIME_FREQ: "10"
    
    ---

    controlplane ~ âœ  k apply -f time-check-pod.yaml 
    pod/time-check created

    controlplane ~ âœ  k get pod
    NAME           READY   STATUS    RESTARTS   AGE
    logger         1/1     Running   0          28m
    secure-pod     1/1     Running   0          25m
    webapp-color   1/1     Running   0          45m



      Solution:
        ---
        apiVersion: v1
        kind: Pod
        metadata:
        labels:
            run: time-check
        name: time-check
        namespace: dvl1987
        spec:
        **volumes**:
        - name: log-volume
            **emptyDir**: {} (è¦‹IMPORTANT -1)
        containers:
        - image: busybox
            name: time-check
            **env**:
            - name: TIME_FREQ
            **valueFrom**:
                    **configMapKeyRef**:
                    name: time-config
                    key: TIME_FREQ
            **volumeMounts**:
            - mountPath: /opt/time
            name: log-volume
            command:
            - "**/bin/sh**" (è¦‹IMPORTANT -2)
            - "**-c**"
            - "while true; do date; sleep $TIME_FREQ;done > /opt/time/time-check.log"
            (OR:
            command: ["**/bin/sh**","**-c**","cat /etc/config/keys" ])

      **IMPORTANT -1**:
      Solutionçµ¦çš„:
        volumes:
        - name: log-volume
            **emptyDir**: {} # {} è¡¨ç¤ºè©²å°è±¡æ²’æœ‰ä»»ä½•é¡å¤–çš„é…ç½®ï¼ˆä½¿ç”¨é è¨­å€¼ï¼‰

      controlplane ~ âœ  k explain pod.spec.volumes.emptyDir --recursive
      KIND:       Pod
      VERSION:    v1

      FIELD: emptyDir <EmptyDirVolumeSource>


      DESCRIPTION:
          emptyDir represents a temporary directory that shares a pod's lifetime. More
          info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir
          Represents an empty directory for a pod. Empty directory volumes support
          ownership management and SELinux relabeling.
          
      FIELDS:
        medium        <string>
        sizeLimit     <Quantity>

      é€™ç­‰åŒæ–¼ï¼š

      volumes:
        - name: config
          emptyDir:
            medium: ""    # é è¨­ç‚ºç©ºå­—ç¬¦ä¸²ï¼Œè¡¨ç¤ºä½¿ç”¨ç£ç¢Ÿå­˜å„²
            sizeLimit: "" # é è¨­ç‚ºç©ºï¼Œè¡¨ç¤ºä¸é™åˆ¶å¤§å°


      æˆ‘è¨­ç½®çš„:
        volumes:
        - name: log-volume
        ~~configMap:~~
        ~~name: time-config~~
      
      ç‚ºä½• emptyDir: {} æœƒè¢«ç”¨æ–¼ volumesï¼Ÿ
      åœ¨ Kubernetes ä¸­ï¼Œvolumes çš„è¨­ç½®æ±ºå®šäº† Pod å…§éƒ¨å¦‚ä½•å­˜å„²å’Œå…±äº«æ•¸æ“šã€‚ä½ çš„ç–‘å•æ˜¯ ç‚ºä½• emptyDir: {} è¢«ç”¨æ–¼ volumesï¼Œè€Œä¸æ˜¯ configMapï¼Ÿ

      a. emptyDir çš„ä½œç”¨
      **emptyDir æ˜¯ä¸€ç¨®è‡¨æ™‚æ€§ï¼ˆEphemeralï¼‰çš„ Volume**ã€‚
      é€™ç¨® Volume æœƒåœ¨ Pod å­˜åœ¨æœŸé–“ä¿æŒå­˜å„²æ•¸æ“šï¼Œä½† ä¸€æ—¦ Pod è¢«åˆªé™¤ï¼Œæ•¸æ“šå°±æœƒä¸Ÿå¤±ã€‚ (ç¬¦åˆé¡Œç›®: mount a volume that lasts the lifetime of this pod)
      emptyDir åœ¨ Pod å…§éƒ¨çš„æ‰€æœ‰å®¹å™¨ä¹‹é–“æ˜¯å…±äº«çš„ï¼Œä¹Ÿå°±æ˜¯èªªï¼Œå¦‚æœæœ‰å¤šå€‹å®¹å™¨ï¼Œå®ƒå€‘å¯ä»¥åŒæ™‚è®€å¯« emptyDir æ›è¼‰çš„ç›®éŒ„ã€‚
      é©ç”¨æƒ…å¢ƒ
        æ—¥èªŒæ–‡ä»¶å­˜å„²ï¼ˆlog filesï¼‰
        è‡¨æ™‚ç·©å­˜æ•¸æ“šï¼ˆtemporary cacheï¼‰
        å®¹å™¨ä¹‹é–“å…±äº«æ•¸æ“šï¼ˆdata sharing between containersï¼‰
      
      b. ç‚ºä½•ä¸ä½¿ç”¨ configMap ä½œç‚º volumesï¼Ÿ
      ä½ åœ¨ YAML æ–‡ä»¶ä¸­å¯«äº†ï¼š
      volumes:
        - name: log-volume
          configMap:
            name: time-config
      é€™æ˜¯ä¸æ­£ç¢ºçš„ï¼Œå› ç‚ºï¼š

      **configMap ä¸»è¦ç”¨ä¾†æä¾›éœæ…‹çš„ Key-Value é…ç½®ï¼Œè€Œ ä¸èƒ½ç”¨ä¾†å­˜å„²è®Šå‹•çš„æ—¥èªŒæ–‡ä»¶**ã€‚
      configMap åªèƒ½å­˜æ”¾éœæ…‹æ–‡ä»¶ï¼Œè€Œ while true; do date; sleep $TIME_FREQ; done > /opt/time/time-check.log æœƒä¸æ–·ç”¢ç”Ÿæ–°æ•¸æ“šï¼Œé€™ä¸¦ä¸ç¬¦åˆ configMap çš„ç”¨é€”ã€‚
      **configMap æ˜¯åªè®€çš„**ï¼Œå®ƒçš„å…§å®¹ä¸èƒ½åœ¨å®¹å™¨å…§éƒ¨è¢«å‹•æ…‹ä¿®æ”¹ã€‚
      ä»€éº¼æ™‚å€™ç”¨ configMap ä½œç‚º volumesï¼Ÿ
        åªç”¨ä¾†å­˜æ”¾ éœæ…‹é…ç½®æ–‡ä»¶ï¼ˆä¾‹å¦‚ config.yamlï¼‰ã€‚
        ä¸æœƒè®Šæ›´çš„æ•¸æ“šï¼ˆä¾‹å¦‚ SSH é‡‘é‘°ã€TLS æ†‘è­‰ï¼‰ã€‚

    emptyDir åœ¨æ­¤è™•çš„ä½œç”¨
    åœ¨ solution ä¸­ï¼Œé€™æ®µï¼š

    volumes:
      - name: log-volume
        emptyDir: {}
    
    è¡¨ç¤ºï¼š
    Kubernetes æœƒçµ¦é€™å€‹ Pod åˆ†é…ä¸€å¡Šè‡¨æ™‚å­˜å„²ç©ºé–“ï¼Œç”¨ä¾†å­˜æ”¾ /opt/time/time-check.logã€‚
    é€™å¡Šå­˜å„²åœ¨ Pod å­˜åœ¨æœŸé–“éƒ½å¯ä»¥ä½¿ç”¨ï¼Œä½† ä¸€æ—¦ Pod é‡æ–°å•Ÿå‹•æˆ–åˆªé™¤ï¼Œæ•¸æ“šå°±æœƒæ¶ˆå¤±ã€‚
    å®ƒå…è¨±å®¹å™¨å¯«å…¥ /opt/time/time-check.logï¼Œè€Œä¸æœƒé‡åˆ°åªè®€å•é¡Œã€‚
    æ‰€ä»¥ï¼š 
    âœ… ç”¨ emptyDir: {}ï¼Œå› ç‚ºå®ƒé©ç”¨æ–¼ Pod å…§çš„è‡¨æ™‚æ•¸æ“šå­˜å„²ï¼ˆå¦‚æ—¥èªŒï¼‰ã€‚
    âŒ ä¸èƒ½ç”¨ configMapï¼Œå› ç‚º configMap ç„¡æ³•å‹•æ…‹å­˜å„²è®Šæ›´çš„æ•¸æ“šã€‚


    Volume é¡å‹çš„å¿«é€Ÿæ±ºç­–:
    | éœ€æ±‚                               | é©åˆçš„ Volume é¡å‹              |
    |:-----------------------------------|:-------------------------------|
    | å­˜å„²å¯ä¿®æ”¹çš„è‡¨æ™‚æ•¸æ“š                 | emptyDir |
    | æŒä¹…åŒ–æ•¸æ“šï¼Œå³ä½¿ Pod é‡æ–°å•Ÿå‹•	       |  PersistentVolumeClaim (PVC)   |
    | åªè®€çš„éœæ…‹é…ç½®                      | configMap æˆ– secret |
    | ç›´æ¥å­˜å–å®¿ä¸»æ©Ÿçš„æª”æ¡ˆ                 | hostPath  |
      

      **IMPORTANT -2**:
        /bin/sh æ˜¯ä»€éº¼ï¼Ÿ
        å®¹å™¨çš„åŸºç¤æ˜ åƒæ˜¯ busyboxï¼Œå®ƒæ˜¯ä¸€å€‹è¼•é‡ç´šçš„ Linux ç³»çµ±ï¼Œä¸¦æ²’æœ‰ bashï¼Œä½†æœ‰ /bin/shï¼ˆé¡ä¼¼ ash æˆ– dashï¼‰ã€‚
        åœ¨ busybox æˆ–å¤§å¤šæ•¸ Linux ç’°å¢ƒä¸­ï¼Œç•¶ä½ è¦åŸ·è¡Œä¸€å€‹è¼ƒç‚ºè¤‡é›œçš„ Shell æŒ‡ä»¤ï¼ˆå¦‚ while è¿´åœˆï¼‰ï¼Œä½ éœ€è¦ä¸€å€‹ Shell è§£é‡‹å™¨ä¾†åŸ·è¡Œå®ƒã€‚
        ç›´æ¥åŸ·è¡Œ while true; do date; sleep $TIME_FREQ; done > /opt/time/time-check.log å¯èƒ½ç„¡æ³•æ­£å¸¸è§£æï¼Œå› ç‚º command éœ€è¦åŸ·è¡Œçš„æ˜¯ä¸€å€‹å…·é«”çš„äºŒé€²åˆ¶ç¨‹å¼ï¼Œè€Œ while true é€™é¡æŒ‡ä»¤æ˜¯ Shell æŒ‡ä»¤ã€‚
        
        -c çš„ä½œç”¨
        -c åƒæ•¸çš„ä½œç”¨æ˜¯è®“ Shell åŸ·è¡Œå¾Œé¢æ¥çš„å­—ä¸²ä½œç‚º Shell å‘½ä»¤ã€‚
        command åƒæ•¸åˆ—è¡¨çš„å®Œæ•´æ ¼å¼ï¼š
        command:
        - "/bin/sh"
        - "-c"
        - "while true; do date; sleep $TIME_FREQ; done > /opt/time/time-check.log"
        é€™ç›¸ç•¶æ–¼æ‰‹å‹•åŸ·è¡Œï¼š
        /bin/sh -c "while true; do date; sleep $TIME_FREQ; done > /opt/time/time-check.log"
        
        å¦‚æœä¸åŠ  -cï¼ŒShell æœƒå°‡ "while true; do date; sleep $TIME_FREQ; done > /opt/time/time-check.log" ç•¶ä½œä¸€å€‹äºŒé€²åˆ¶å¯åŸ·è¡Œæ–‡ä»¶ä¾†åŸ·è¡Œï¼Œä½†é€™é¡¯ç„¶ä¸æ˜¯å¯åŸ·è¡Œçš„ç¨‹åºï¼Œå› æ­¤æœƒå ±éŒ¯ã€‚  

      **IMPORTANT -3**:
      controlplane ~ âœ  kubectl exec -it time-check -n dvl1987 -- env | grep TIME_FREQ
      error: Internal error occurred: unable to upgrade connection: container not found ("time-check")


      å€˜è‹¥podå› ç‚ºcontainerå…§éƒ¨é…ç½®æœ‰å•é¡Œå°è‡´ç„¡é™é‡å•Ÿï¼Œå¯å»ºç«‹debug pod ä¾†æ’æŸ¥åŸå› :
        
          kubectl run debug-shell --rm -i -t --image=busybox --restart=Never --namespace=dvl1987 -- sh


      controlplane ~ âœ– kubectl run debug-shell --rm -i -t --image=busybox --restart=Never --namespace=dvl1987 -- sh
      If you don't see a command prompt, try pressing enter.
      / # env | grep TIME_FREQ (æª¢æŸ¥podæ˜¯å¦æ­£ç¢ºå‚³éç’°å¢ƒè®Šé‡)
      / # 
      / # 

      å›åˆ°é…ç½®æ–‡ä»¶:
      spec:
        containers:
        - image: busybox
          imagePullPolicy: Always
          name: time-check
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          command:
            - "/bin/sh"
            - "-c"
            - "while true; do date; sleep ${TIME_FREQ:-5};done > /opt/time/time-check.log"
          env:
          - name: TIME_FREQ
            valueFrom:
              configMapKeyRef:
                name: time-config
                key: TIME_FREQ
          volumeMounts:
            - name: config
              mountPath: /opt/time
              **readOnly: true** # **è¡¨ç¤ºé–‹èµ·å”¯ç¨æ¨¡å¼ï¼Œå°è‡´å› ç‚ºç„¡æ³•å¯«å…¥/opt/timeè€Œå ±éŒ¯!!!**
        volumes:
        - name: config
          emptyDir: {}

      controlplane ~ âœ  vim time-check-pod.yaml 
      åˆªé™¤readOnlyè¨­ç½®ã€‚

      controlplane ~ âœ  k delete pod time-check -n dvl1987 --force
      Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.
      pod "time-check" force deleted

      controlplane ~ âœ  kc time-check-pod.yaml 
      pod/time-check created

      controlplane ~ âœ  k get pod -n dvl1987
      NAME         READY   STATUS    RESTARTS   AGE
      time-check   1/1     Running   0          8s



   
2. Persistent Volume + PVC + Pod Mounting
  
    Create a Persistent Volume called log-volume. It should make use of a storage class name manual. It should use RWX as the access mode and have a size of 1Gi. The volume should use the hostPath /opt/volume/nginx

    Next, create a PVC called log-claim requesting a minimum of 200Mi of storage. This PVC should bind to log-volume.

    Mount this in a pod called logger at the location /var/www/nginx. This pod should use the image nginx:alpine.

    æˆ‘çš„é…ç½®:

    controlplane ~ âœ  vim log-volume-pv.yaml
    apiVersion: v1
    kind: PersistentVolume
    metadata:
      name: log-volume
    spec:
      capacity:
        storage: 1Gi
      accessModes:
        - ReadWriteMany
      volumeMode: Filesystem
      persistentVolumeReclaimPolicy: Retain
      storageClassName: manual

      hostPath:
        path: /opt/volume/nginx


    controlplane ~ âœ  k create -f log-volume-pv.yaml 
    persistentvolume/log-volume created

    controlplane ~ âœ  k get pv
    NAME         CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE
    log-volume   1Gi        RWX            Retain           Available           manual         <unset>                          2s


    controlplane ~ âœ– vim log-claim-pvc.yaml
    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: log-claim
    spec:
      accessModes:
        - ReadWriteMany
      volumeMode: Filesystem
      resources:
        requests:
          storage: 200Mi
      storageClassName: manual


    controlplane ~ âœ  k create -f  log-claim-pvc.yaml
    persistentvolumeclaim/log-claim created

    controlplane ~ âœ  k get pvc
    NAME        STATUS   VOLUME       CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
    log-claim   Bound    log-volume   1Gi        RWX            manual         <unset>                 2s

    controlplane ~ âœ  k get pod
    NAME           READY   STATUS    RESTARTS   AGE
    webapp-color   1/1     Running   0          13m

    controlplane ~ âœ  vim logger-pod.yaml 
    apiVersion: v1
    kind: Pod
    metadata:
      name: logger
      namespace: default
    spec:
      restartPolicy: Never
      containers:
      - name: logger
        image: nginx:alpine
        volumeMounts:
        - mountPath: /var/www/nginx
          name: volume
          readOnly: true

      volumes:
      - name: volume
        hostPath:
          path: /opt/volume/nginx


    controlplane ~ âœ  k create -f logger-pod.yaml 
    Error from server (BadRequest): error when creating "logger-pod.yaml": Pod in version "v1" cannot be handled as a Pod: strict decoding error: unknown field "spec.containers[0].volumes"

    controlplane ~ âœ– vim logger-pod.yaml

    controlplane ~ âœ  k create -f logger-pod.yaml 
    pod/logger created

    controlplane ~ âœ  k get pod
    NAME           READY   STATUS    RESTARTS   AGE
    logger         1/1     Running   0          3s
    webapp-color   1/1     Running   0          17m




3. Secret Volume + Node Scheduling
    
    Create a pod called my-busybox in the dev2406 namespace using the busybox image. The container should be called secret and should sleep for 3600 seconds.
    The container should mount a read-only secret volume called secret-volume at the path /etc/secret-volume. The secret being mounted has already been created for you and is called dotfile-secret.
    Make sure that the pod is scheduled on controlplane and no other node in the cluster.

    æˆ‘çš„é…ç½®:
    
    k run my-busybox -n dev2406 --iamge=busybox -o yaml > my-busybox-pod.yaml

    vim my-busybox-pod.yaml
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: **beta.kubernetes.io/arch** # æ­¤è™•é…ç½®éŒ¯èª¤!!! 
                operator: **In** # éŒ¯èª¤
                values: 
                - **amd64** # éŒ¯èª¤
        containers:
          - image: busybox
          imagePullPolicy: Always
          name: secret
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          command: ["/bin/sh","-c","sleep 3600"]
          volumeMounts:
          - mountPath: /etc/secret-volume
            name: secret-vol
            readOnly: true
        volumes:
          - name: secret-vol
            secret:
              secretName: dotfile-secret
        tolerations:
          - key: **"beta.kubernetes.io/arch"** # é…ç½®éŒ¯èª¤!
            operator: "Exists"
            effect: "NoSchedule"


    controlplane ~ âœ  k describe node controlplane 
    Name:               controlplane
    Roles:              control-plane
    **Labels**:         beta.kubernetes.io/arch=amd64
                        beta.kubernetes.io/os=linux
                        kubernetes.io/arch=amd64
                        kubernetes.io/hostname=controlplane
                        kubernetes.io/os=linux
                        node-role.kubernetes.io/control-plane=
                        node.kubernetes.io/exclude-from-external-load-balancers=
    
    controlplane ~ âœ  vim my-buxybox.yaml 

    controlplane ~ âœ  k delete pod my-busybox -n dev2406 --force
    Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.
    pod "my-busybox" force deleted

    controlplane ~ âœ  k create -f  my-buxybox.yaml 
    pod/my-busybox created

    controlplane ~ âœ  k get pod -n dev2406
    NAME          READY   STATUS    RESTARTS   AGE
    my-busybox    1/1     Running   0          3s
    nginx2406     1/1     Running   0          35m
    pod-var2016   1/1     Running   0          35m

    controlplane ~ âœ– k describe pod -n dev2406 my-busybox |grep -i "toleration"
    Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s

    controlplane ~ âœ  k describe nodes controlplane |grep -i "taints"
    Taints:             <none>

    controlplane ~ âœ  k taint node controlplane beta.kubernetes.io/arch=amd64:NoSchedule

    controlplane ~ âœ– k describe node controlplane |grep -i "taint"
    Taints:             beta.kubernetes.io/arch=amd64:NoSchedule

    
    é©—è­‰my-busyboxæ˜¯å¦é‹è¡Œåœ¨controlplaneç¯€é»:
    controlplane ~ âœ  k get pod my-busybox -n dev2406 -o wide
    NAME         READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
    my-busybox   1/1     Running   0          70s   172.17.1.15   **node01**   <none>           <none>
    (å¤±æ•—)


    ä¿®æ­£å¾Œçš„é…ç½®:

    (é‡æ–°æª¢æŸ¥label)
    controlplane ~ âœ  kubectl get nodes --show-labels
    NAME           STATUS   ROLES           AGE   VERSION   LABELS
    controlplane   Ready    control-plane   32m   v1.31.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,**kubernetes.io/hostname=controlplane**,kubernetes.io/os=linux,**node-role.kubernetes.io/control-plane=**,node.kubernetes.io/exclude-from-external-load-balancers=
    node01         Ready    <none>          31m   v1.31.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node01,kubernetes.io/os=linux


    controlplane ~ âœ  vim my-buxybox.yaml 

    controlplane ~ âœ  k create -f  my-buxybox.yaml 
    pod/my-busybox created

    controlplane ~ âœ  k get pod my-busybox -n dev2406 -o wide
    NAME         READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
    my-busybox   0/1     Pending   0          12s   <none>   <none>   <none>           <none>


    controlplane ~ âœ  cat my-buxybox.yaml 
    apiVersion: v1
    kind: Pod
    metadata:
      name: my-busybox
      namespace: dev2406
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: **"kubernetes.io/hostname"**
                operator: In
                values:
                - **controlplane**
      containers:
      - image: busybox
        imagePullPolicy: Always
        name: secret
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        command: ["/bin/sh","-c","sleep 3600"]
        volumeMounts:
        - mountPath: /etc/secret-volume
          name: secret-vol
          readOnly: true
      volumes:
        - name: secret-vol
          secret:
            secretName: dotfile-secret
      tolerations:
        - key: **"kubernetes.io/hostname"**
          operator: Exists
          effect: NoSchedule

    controlplane ~ âœ– kubectl taint node controlplane beta.kubernetes.io/arch=amd64:NoSchedule-
    node/controlplane untainted åˆªé™¤taint

    controlplane ~ âœ– kubectl taint node controlplane kubernetes.io/hostname=controlplane:NoSchedule
    node/controlplane tainted

    controlplane ~ âœ  k describe node controlplane |grep -i "taint"
    Taints:             kubernetes.io/hostname=controlplane:NoSchedule


    é‡æ–°é©—è­‰:
    controlplane ~ âœ  k get pod my-busybox -n dev2406 -o wide
    NAME         READY   STATUS    RESTARTS   AGE     IP           NODE           NOMINATED NODE   READINESS GATES
    my-busybox   1/1     Running   0          6m46s   172.17.0.6   controlplane   <none>           <none>


    å»ç™¼ç¾ä»æœ‰å¤§é‡çš„pod é‹è¡Œåœ¨controlplaneä¸Š!!!!
    controlplane ~ âœ  kubectl get pods -A -o wide | grep controlplane
    default         nginx                                       0/1     ContainerCreating   0          34m   <none>            controlplane   <none>           <none>
    dev2406         my-busybox                                  1/1     Running             0          11m   172.17.0.6        controlplane   <none>           <none>
    ingress-nginx   ingress-nginx-admission-patch-l7nfk         0/1     Completed           1          38m   172.17.0.5        controlplane   <none>           <none>
    kube-system     calico-kube-controllers-5d7d9cdfd8-sbfcz    1/1     Running             0          43m   172.17.0.2        controlplane   <none>           <none>
    kube-system     canal-5nkrt                                 2/2     Running             0          43m   192.168.231.151   controlplane   <none>           <none>
    kube-system     coredns-77d6fd4654-fjvc7                    1/1     Running             0          43m   172.17.0.3        controlplane   <none>           <none>
    kube-system     coredns-77d6fd4654-sj7w5                    1/1     Running             0          43m   172.17.0.4        controlplane   <none>           <none>
    kube-system     etcd-controlplane                           1/1     Running             0          43m   192.168.231.151   controlplane   <none>           <none>
    kube-system     kube-apiserver-controlplane                 1/1     Running             0          43m   192.168.231.151   controlplane   <none>           <none>
    kube-system     kube-controller-manager-controlplane        1/1     Running             0          43m   192.168.231.151   controlplane   <none>           <none>
    kube-system     kube-proxy-dk842                            1/1     Running             0          43m   192.168.231.151   controlplane   <none>           <none>
    kube-system     kube-scheduler-controlplane                 1/1     Running             0          43m   192.168.231.151   controlplane   <none>           <none>

    âŒ ä½ çš„ Toleration è¨­ç½®ç‚ºä»€éº¼ç„¡æ³•è®“å…¶ä»– Pod è½‰ç§»ï¼Ÿ
    ğŸ”¹ Toleration çš„ä½œç”¨
    Toleration åªæœƒå½±éŸ¿ Pod çš„èª¿åº¦éç¨‹ï¼Œå®ƒä¸æœƒå½±éŸ¿å·²ç¶“é‹è¡Œçš„ Podï¼Œä¹Ÿä¸æœƒå¼·åˆ¶ Pod è½‰ç§»ã€‚

    ğŸ”¹ Taint çš„ä½œç”¨
    å¦‚æœä½ æƒ³è®“ controlplane åªå…è¨± my-busybox é‹è¡Œï¼Œè€Œå…¶ä»– Pod è½‰ç§»åˆ°å…¶ä»–ç¯€é»ï¼Œä½ éœ€è¦ï¼š

    è¨­å®šä¸€å€‹æ›´åš´æ ¼çš„ Taintï¼Œè®“ controlplane åªå…è¨± my-busybox èª¿åº¦ã€‚
    è®“å…¶ä»– Pod ä¸èƒ½å®¹å¿é€™å€‹ Taintï¼Œé€™æ¨£ Kubernetes æœƒã€Œé©…é€ã€å®ƒå€‘åˆ°å…¶ä»–ç¯€é»ã€‚    


    controlplane ~ âœ  kubectl taint nodes controlplane kubernetes.io/hostname:NoSchedule-
    node/controlplane untainted

    controlplane ~ âœ  kubectl taint nodes controlplane only-my-busybox=true:NoSchedule
    node/controlplane tainted

    controlplane ~ âœ  vim my-buxybox.yaml 

    controlplane ~ âœ  k delete pod my-busybox --force
    Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.
    Error from server (NotFound): pods "my-busybox" not found

    controlplane ~ âœ– k delete pod my-busybox -n dev2406 --force
    Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.
    pod "my-busybox" force deleted

    controlplane ~ âœ  k create -f  my-buxybox.yaml 
    pod/my-busybox created

    controlplane ~ âœ  k get pod  -n dev2406 -o wide
    NAME          READY   STATUS    RESTARTS   AGE   IP           NODE           NOMINATED NODE   READINESS GATES
    my-busybox    1/1     Running   0          5s    172.17.0.7   controlplane   <none>           <none>
    pod-var2016   1/1     Running   0          44m   172.17.1.9   node01         <none>           <none>

    controlplane ~ âœ  kubectl get pods -A -o wide | grep controlplane
    default         nginx                                       0/1     ContainerCreating   0          39m   <none>            controlplane   <none>           <none>
    dev2406         my-busybox                                  1/1     Running             0          14s   172.17.0.7        controlplane   <none>           <none>
    ingress-nginx   ingress-nginx-admission-patch-l7nfk         0/1     Completed           1          44m   172.17.0.5        controlplane   <none>           <none>
    kube-system     calico-kube-controllers-5d7d9cdfd8-sbfcz    1/1     Running             0          48m   172.17.0.2        controlplane   <none>           <none>
    kube-system     canal-5nkrt                                 2/2     Running             0          48m   192.168.231.151   controlplane   <none>           <none>
    kube-system     coredns-77d6fd4654-fjvc7                    1/1     Running             0          48m   172.17.0.3        controlplane   <none>           <none>
    kube-system     coredns-77d6fd4654-sj7w5                    1/1     Running             0          48m   172.17.0.4        controlplane   <none>           <none>
    kube-system     etcd-controlplane                           1/1     Running             0          48m   192.168.231.151   controlplane   <none>           <none>
    kube-system     kube-apiserver-controlplane                 1/1     Running             0          48m   192.168.231.151   controlplane   <none>           <none>
    kube-system     kube-controller-manager-controlplane        1/1     Running             0          48m   192.168.231.151   controlplane   <none>           <none>
    kube-system     kube-proxy-dk842                            1/1     Running             0          48m   192.168.231.151   controlplane   <none>           <none>
    kube-system     kube-scheduler-controlplane                 1/1     Running             0          48m   192.168.231.151   controlplane   <none>           <none>

    (åŸæœ¬çš„podå»æ²’æœ‰è½‰ç§»)

    å¦‚ä½•å¼·åˆ¶å°‡ç¾æœ‰çš„ Pod è½‰ç§»ï¼Ÿ
    å¦‚æœä½ å¸Œæœ› controlplane ä¸Šçš„ kube-system å…§å»º Pod å’Œå…¶ä»– Pod è½‰ç§»åˆ°å…¶ä»–ç¯€é»ï¼Œä½ éœ€è¦ ä½¿ç”¨ NoExecute Taintï¼Œé€™æ¨£ï¼š

    Kubernetes æœƒç«‹åˆ»é©…é€ (evict) ä¸ç¬¦åˆæ¢ä»¶çš„ Podã€‚
    åªæœ‰å…·å‚™ Toleration çš„ Podï¼ˆå¦‚ my-busyboxï¼‰èƒ½å¤ å­˜æ´»åœ¨ controlplaneã€‚
    ğŸ”¹ 1ï¸âƒ£ ç§»é™¤èˆŠçš„ NoSchedule Taint
    å…ˆåˆªé™¤åŸæœ¬çš„ NoSchedule Taintï¼š

    kubectl taint nodes controlplane only-my-busybox=true:NoSchedule-

    ğŸ”¹ 2ï¸âƒ£ è¨­ç½® NoExecute Taintï¼ˆæœƒå¼·åˆ¶é©…é€ä¸ç¬¦åˆçš„ Podï¼‰
    kubectl taint nodes controlplane only-my-busybox=true:NoExecute

    NoExecute æœƒå¼·åˆ¶é©…é€æ‰€æœ‰æ²’æœ‰ Toleration çš„ Podã€‚
    ğŸ”¹ 3ï¸âƒ£ ç¢ºä¿ my-busybox å…è¨± NoExecute
    ç¢ºä¿ my-busybox.yaml å…è¨± NoExecuteï¼Œå¦å‰‡å®ƒä¹Ÿæœƒè¢«é©…é€ï¼š


    tolerations:
      - key: "only-my-busybox"
        operator: "Equal"
        value: "true"
        effect: "NoExecute"


    ğŸ”¹ 4ï¸âƒ£ åˆªé™¤ä¸¦é‡æ–°å»ºç«‹ my-busybox
    åˆªé™¤èˆŠçš„ Podï¼Œä¸¦ç”¨æ–°çš„ Toleration é‡æ–°å‰µå»ºï¼š

    kubectl delete pod my-busybox -n dev2406 --force
    kubectl apply -f my-busybox.yaml

    é©—è­‰æ˜¯å¦æˆåŠŸ
    1ï¸âƒ£ æª¢æŸ¥ controlplane æ˜¯å¦é‚„æœ‰å…¶ä»– Pod

    kubectl get pods -A -o wide | grep controlplane

    2ï¸âƒ£ æª¢æŸ¥ Taint æ˜¯å¦ç”Ÿæ•ˆ

    kubectl describe node controlplane | grep -i "Taints"



    Solution:

    "Make sure that the pod is scheduled on controlplane and no other node in the cluster": 
    é€™å€‹ Pod å¿…é ˆé‹è¡Œåœ¨ controlplane ç¯€é»ä¸Šï¼Œä¸èƒ½è¢« Kubernetes æ’ç¨‹åˆ°å…¶ä»– Worker ç¯€é»ã€‚
    é€šå¸¸å¯ä»¥é€é Node Selectorã€Node Affinity æˆ– Taints & Tolerations ä¾†å¼·åˆ¶ Pod åªèƒ½é‹è¡Œåœ¨ç‰¹å®šç¯€é»ä¸Š
    (è§£ç­”ä½¿ç”¨node selector)

      apiVersion: v1
      kind: Pod
      metadata:
        creationTimestamp: null
        labels:
          run: my-busybox
        name: my-busybox
        namespace: dev2406
      spec:
        volumes:
        - name: secret-volume
          secret:
            secretName: dotfile-secret
        nodeSelector:
          kubernetes.io/hostname: controlplane
        containers:
        - command:
          - sleep
          args:
          - "3600"
          image: busybox
          name: secret
          volumeMounts:
          - name: secret-volume
            readOnly: true
            mountPath: "/etc/secret-volume"


### 03. Application Design and Build (20%)

1. Deployment + Rolling Update + Rollback 
  Create a new deployment called nginx-deploy, with one single container called nginx, image nginx:1.16 and 4 replicas.
  The deployment should use RollingUpdate strategy with maxSurge=1, and maxUnavailable=2.

    Next upgrade the deployment to version 1.17.

    Finally, once all pods are updated, undo the update and go back to the previous version.

    Step1: upgrade the deployment to version 1.17
      
      æˆ‘çš„é…ç½®:
      
      controlplane ~ âœ  k create deploy nginx-deploy --image=nginx:1.16 --replicas=4
      deployment.apps/nginx-deploy created

      controlplane ~ âœ  k get deploy
      NAME           READY   UP-TO-DATE   AVAILABLE   AGE
      nginx-deploy   0/4     4            0           5s

      controlplane ~ âœ  k get pod
      NAME                            READY   STATUS    RESTARTS   AGE
      logger                          1/1     Running   0          30m
      nginx-deploy-58df6b7867-jcj7d   1/1     Running   0          8s
      nginx-deploy-58df6b7867-qhrgj   1/1     Running   0          8s
      nginx-deploy-58df6b7867-qsqn8   1/1     Running   0          8s
      nginx-deploy-58df6b7867-xkvsk   1/1     Running   0          8s
      secure-pod                      1/1     Running   0          28m
      webapp-color                    1/1     Running   0          48m

      controlplane ~ âœ  k edit deploy nginx-deploy  (ä¿®æ”¹maxSurge=1, and maxUnavailable=2)
      deployment.apps/nginx-deploy edited


      controlplane ~ âœ  kubectl set image deployment/nginx-deploy nginx=nginx:1.17
      deployment.apps/nginx-deploy image updated

      controlplane ~ âœ– k rollout history deployment nginx-deploy 
      deployment.apps/nginx-deploy 
      REVISION  CHANGE-CAUSE
      1         <none>
      2         <none>

    
     ---


      Solution:
      controlplane ~ âœ  k set 
      env             (Update environment variables on a pod template)
      image           (Update the image of a pod template)
      resources       (Update resource requests/limits on objects with pod templates)
      selector        (Set the selector on a resource)
      serviceaccount  (Update the service account of a resource)
      subject         (Update the user, group, or service account in a role binding or cluster role binding)

      controlplane ~ âœ  k set image deployments nginx-deploy nginx=nginx:1.16
      deployment.apps/nginx-deploy image updated

      controlplane ~ âœ  k rollout status deployment nginx-deploy 
      deployment "nginx-deploy" successfully rolled out


      controlplane ~ âœ  k rollout history deployment nginx-deploy 
      deployment.apps/nginx-deploy 
      REVISION  CHANGE-CAUSE
      2         <none>
      3         <none>
          (eployment ç¢ºå¯¦é€²è¡Œäº†ç‰ˆæœ¬è®Šæ›´ï¼ˆå¾ 2 -> 3ï¼‰ï¼Œä½†æ²’æœ‰ CHANGE-CAUSEï¼ŒåŸå› å¯èƒ½æ˜¯ï¼š
          kubectl edit åŠ kubectl set image æ²’æœ‰ä½¿ç”¨ --recordï¼Œæ‰€ä»¥æ²’æœ‰è¨˜éŒ„ Change-Causeã€‚
          ä½ å¯ä»¥æ‰‹å‹•åŠ ä¸Š **--record** ä¾†è¨˜éŒ„ï¼š
          kubectl set image deployment nginx-deploy nginx=nginx:1.17 --record
          é€™æ¨£ kubectl rollout history å°±æœƒé¡¯ç¤ºè®Šæ›´åŸå› ã€‚)


      (ç‚ºä½• kubectl edit deploy ä¹Ÿå¯ä»¥è¦–ç‚º Upgradeï¼Ÿ)
      ç•¶ä½ ä½¿ç”¨ï¼š
      kubectl edit deploy nginx-deploy
      ç„¶å¾Œæ‰‹å‹•ä¿®æ”¹ image: nginx:1.16 -> nginx:1.17ï¼Œç³»çµ±æœƒè¦–ç‚ºä¸€æ¬¡ Deployment æ›´æ–°ï¼ˆUpgradeï¼‰ï¼Œå› ç‚º Kubernetes æœƒæª¢æ¸¬åˆ° Deployment è¦ç¯„çš„è®Šæ›´ï¼Œç„¶å¾ŒåŸ·è¡Œ Rolling Updateã€‚

      ç™¼ç”Ÿäº†ä»€éº¼ï¼Ÿ
      kubectl edit æœƒé–‹å•Ÿ Deployment çš„ YAMLï¼Œè®“ä½ æ‰‹å‹•ç·¨è¼¯ã€‚
      ç•¶ä½ è®Šæ›´ image: nginx:1.16 -> nginx:1.17 ä¸¦å„²å­˜å¾Œï¼ŒKubernetes æœƒè‡ªå‹•åµæ¸¬é€™æ¬¡è®Šæ›´ï¼Œä¸¦è§¸ç™¼æ»¾å‹•æ›´æ–°ï¼ˆRolling Updateï¼‰ã€‚
      kubectl edit æœ¬è³ªä¸Šä¿®æ”¹äº† Deployment çš„ .spec.template.spec.containers.imageï¼ŒKubernetes æœƒæ ¹æ“š Deployment çš„ RollingUpdate ç­–ç•¥ï¼ˆmaxSurge=1, maxUnavailable=2ï¼‰ä¾†æ›´æ–° Podã€‚
      ä½ å¯ä»¥é€é kubectl rollout status deployment nginx-deploy ä¾†è§€å¯Ÿæ»¾å‹•æ›´æ–°çš„é€²åº¦ã€‚
      æ‰€ä»¥ï¼Œå³ä½¿ kubectl edit ä¸æ˜¯å°ˆé–€è¨­è¨ˆä¾†ã€ŒåŸ·è¡Œæ›´æ–°ã€çš„æŒ‡ä»¤ï¼Œå®ƒä»ç„¶æœƒå°è‡´ Deployment è®Šæ›´ï¼Œå¾è€Œè§¸ç™¼å‡ç´šã€‚

      kubectl edit vs kubectl set image
      é€™å…©ç¨®æ–¹å¼æœ¬è³ªä¸Šéƒ½æ˜¯ä¿®æ”¹ Deployment è¦ç¯„ï¼Œä½†æ–¹å¼ä¸åŒï¼š

      |                 æŒ‡ä»¤               |                 ä¿®æ”¹æ–¹å¼         	 |               å½±éŸ¿             |
      |:-----------------------------------|:-----------------------------------|:-------------------------------|
      |kubectl edit deploy nginx-deploy    | æ‰‹å‹•ç·¨è¼¯ YAML ä¾†ä¿®æ”¹ image	         |è®Šæ›´å¾Œ Kubernetes æœƒè§¸ç™¼ Rolling Update|
      |kubectl set image deploy/nginx-deploy nginx=nginx:1.17|	ç›´æ¥ä¿®æ”¹ image çš„å€¼	|ç«‹å³ç”Ÿæ•ˆï¼Œä¸¦è§¸ç™¼ Rolling Update|

    Step2: undo the update and go back to the previous version

      æˆ‘çš„é…ç½®:

      controlplane ~ âœ  k rollout undo deployment nginx-deploy 
      deployment.apps/nginx-deploy rolled back

      controlplane ~ âœ  k rollout history deployment nginx-deploy 
      deployment.apps/nginx-deploy 
      REVISION  CHANGE-CAUSE
      2         <none>
      3         <none>
      
      Solution:

      kubectl rollout undo deployment nginx-deploy (å›æ»¾)
      é€™æœƒè®“ Deployment å›åˆ° ä¸Šä¸€å€‹ç‰ˆæœ¬ï¼ˆå¾ 3 å›åˆ° 2ï¼‰ã€‚

      å¦‚æœä½ **æƒ³å›æ»¾åˆ°ç‰¹å®šç‰ˆæœ¬**ï¼š
      kubectl rollout undo deployment nginx-deploy **--to-revision**=2


      controlplane ~ âœ  k get pod
      NAME                            READY   STATUS    RESTARTS   AGE
      logger                          1/1     Running   0          24m
      nginx-deploy-58df6b7867-25qr9   1/1     Running   0          32s
      nginx-deploy-58df6b7867-f756s   1/1     Running   0          29s
      nginx-deploy-58df6b7867-nwmql   1/1     Running   0          32s
      nginx-deploy-58df6b7867-tz8ck   1/1     Running   0          32s
      secure-pod                      1/1     Running   0          24m
      webapp-color                    1/1     Running   0          44m

      controlplane ~ âœ  k describe deployments.apps nginx-deploy 
      (æª¢æŸ¥versionè®Šæ›´å›nginx: 1.16çœç•¥)

 



2. Deployment + Resource + Volume (State Persistence)

    Create a redis deployment with the following parameters:
    Name of the deployment should be redis using the redis:alpine image. It should have exactly 1 replica.
    The container should request for .2 CPU. It should use the label app=redis.
    It should mount exactly 2 volumes.

    a. An Empty directory volume called data at path /redis-master-data.
    b. A configmap volume called redis-config at path /redis-master.
    c. The container should expose the port 6379.
    The configmap has already been created.

    æˆ‘çš„é…ç½®:

    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: redis
    spec:
      selector:
        matchLabels:
          app: redis
      replicas: 1
      template:
        metadata:
          labels:
            app: redis
        spec:
          volumes:
            - name: configmap-volume
              configMap:
                name: redis-config
            (**å°‘ä¸€å€‹/redis-master-dataå°æ‡‰çš„volumesé…ç½®: emptyDir:{}**)
          containers:
          - name: redis
            image: redis:alpine
            ports:
            - containerPort: 6379
            (**æœªè¨­ç½®containerçš„resource requestå±¬æ€§**)
            volumeMounts:
            - mountPath: /redis-master
              name: configmap-volume
            (**å°‘ä¸€å€‹/redis-master-data**)

        (volume mountsè·Ÿvolumeséœ€è¦æˆå°ï¼Œæœ‰å¤šå°‘å€‹volumeMountsï¼Œå°±æœ‰å¤šå°‘å€‹volumes)
        ä¸ä¸€å®š "å¿…é ˆ" å®Œå…¨æˆå°ï¼Œä½†é€šå¸¸æ‡‰è©²ä¸€ä¸€å°æ‡‰ï¼Œå¦å‰‡ volumeMounts å¯èƒ½æœƒå› ç‚ºæ‰¾ä¸åˆ°å°æ‡‰çš„ volumes è€Œå°è‡´ Pod å‰µå»ºå¤±æ•—ã€‚
        volumeMounts å’Œ volumes çš„é—œä¿‚
      
      åœ¨ Kubernetes ä¸­ï¼š

      volumes æ˜¯ Pod å…§å®šç¾©çš„å„²å­˜è³‡æºã€‚
      volumeMounts æ˜¯å®¹å™¨å…§éƒ¨æ›è¼‰ volumes çš„è·¯å¾‘ã€‚
      åŸºæœ¬è¦å‰‡ï¼š âœ… æ¯å€‹ volumeMounts.name å¿…é ˆå°æ‡‰ä¸€å€‹ volumes.nameï¼Œå¦å‰‡æœƒå ±éŒ¯ã€‚
      âœ… å¯ä»¥æœ‰ volumes ä½†æ²’æœ‰ volumeMountsï¼ˆä¾‹å¦‚å¦ä¸€å€‹å®¹å™¨ä½¿ç”¨è©² volumeï¼‰ã€‚
      âŒ ä¸èƒ½æœ‰ volumeMounts ä½†æ²’æœ‰å°æ‡‰çš„ volumesï¼Œå¦å‰‡ Pod æœƒå ±éŒ¯ã€‚    

      volumes æ¯” volumeMounts å¤šï¼Œæ˜¯å¦å¯ä»¥ï¼Ÿ
      å¯ä»¥ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä¸€å€‹ Pod æœ‰ å¤šå€‹å®¹å™¨ï¼Œå‰‡ volumes å¯èƒ½æ¯” volumeMounts å¤šï¼Œå› ç‚ºä¸æ˜¯æ‰€æœ‰å®¹å™¨éƒ½æœƒæ›è¼‰æ‰€æœ‰ volumesï¼š

      spec:
        volumes:
          - name: shared-volume
            emptyDir: {}
        containers:
          - name: container1
            image: nginx
            volumeMounts:
              - mountPath: /data
                name: shared-volume  # é€™å€‹å®¹å™¨æ›è¼‰ shared-volume
          - name: container2
            image: busybox
            command: ["sleep", "3600"]
            # âŒ é€™å€‹å®¹å™¨æ²’æœ‰ volumeMountsï¼Œä½† `volumes` ä»ç„¶å­˜åœ¨
      é€™æ˜¯åˆæ³•çš„ï¼Œå› ç‚º volumes å¯ä»¥æä¾›çµ¦ä¸åŒçš„å®¹å™¨é¸æ“‡æ€§æ›è¼‰ã€‚

      volumeMounts æ¯” volumes å¤šï¼Œæ˜¯å¦å¯ä»¥ï¼Ÿ
      ä¸å¯ä»¥ï¼
      å¦‚æœ volumeMounts æŒ‡å‘ä¸€å€‹ä¸å­˜åœ¨çš„ volumesï¼ŒPod æœƒç„¡æ³•å•Ÿå‹•ï¼Œä¸¦å ±éŒ¯ï¼š
      MountVolume.SetUp failed for volume "missing-volume" : volume not found


    Solution:

    apiVersion: apps/v1
    kind: Deployment
    metadata:
      labels:
        app: redis
      name: redis
    spec:
      selector:
        matchLabels:
          app: redis
      template:
        metadata:
          labels:
            app: redis
        spec:
          volumes:
          - name: data
            emptyDir: {}
          - name: redis-config
            configMap:
              name: redis-config
          containers:
          - image: redis:alpine
            name: redis
            volumeMounts:
            - mountPath: /redis-master-data
              name: data
            - mountPath: /redis-master
              name: redis-config
            ports:
            - containerPort: 6379
            **resources**: éœ€è¨­ç½®resource request (æˆ‘çš„é…ç½®ä¸­å¿½ç•¥äº†æ­¤è¨­ç½®)
              requests:
                cpu: "0.2"


### 04. Application Observability and Maintenance (15%)
1. A pod called dev-pod-dind-878516 has been deployed in the default namespace. Inspect the logs for the container called log-x and redirect the warnings to /opt/dind-878516_logs.txt on the controlplane node

    æˆ‘çš„é…ç½®:

    step1 æª¢æŸ¥podæ—¥èªŒ:
    controlplane ~ âœ– k logs dev-pod-dind-878516
    Defaulted container "engine-x" out of: engine-x, agent-x, log-x
    /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
    /docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
    /docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
    10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf
    10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf
    /docker-entrypoint.sh: Sourcing /docker-entrypoint.d/15-local-resolvers.envsh
    /docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
    /docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh
    /docker-entrypoint.sh: Configuration complete; ready for start up
    2025/02/07 14:55:51 [notice] 1#1: using the "epoll" event method
    2025/02/07 14:55:51 [notice] 1#1: nginx/1.27.4
    2025/02/07 14:55:51 [notice] 1#1: built by gcc 12.2.0 (Debian 12.2.0-14) 
    2025/02/07 14:55:51 [notice] 1#1: OS: Linux 5.15.0-1075-gcp
    2025/02/07 14:55:51 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576
    2025/02/07 14:55:51 [notice] 1#1: start worker processes
    2025/02/07 14:55:51 [notice] 1#1: start worker process 80
    ...

    
    å€˜è‹¥podæ—¥èªŒç„¡æ³•æŸ¥çœ‹ï¼Œå‰‡éœ€è¦é€²å…¥åˆ°å®¹å™¨å…§:
    k exec -it pod dev-pod-dind-878516 -c log-x   
    k exec -it dev-pod-dind-878516 -c log-x -- /bin/sh(é€²å…¥log-xå®¹å™¨å…§)
    cat /path/to/logs |grep -i "warn"

    step2 è­¦å‘Šè¨Šæ¯çš„æ ¼å¼åŒ…å«é—œéµå­— "WARNING" æˆ– "WARN":
    kubectl logs dev-pod-dind-878516 -c log-x -n default | grep -i "warn"

    controlplane ~ âœ  kubectl logs dev-pod-dind-878516 -c log-x -n default | grep -i "warn"
    [2025-02-07 14:56:28,693] WARNING in event-simulator: USER5 Failed to Login as the account is locked due to MANY FAILED ATTEMPTS.
    [2025-02-07 14:56:31,697] WARNING in event-simulator: USER7 Order failed as the item is OUT OF STOCK.
    [2025-02-07 14:56:33,699] WARNING in event-simulator: USER5 Failed to Login as the account is locked due to MANY FAILED ATTEMPTS.
    [2025-02-07 14:56:38,707] WARNING in event-simulator: USER5 Failed to Login as the account is locked due to MANY FAILED ATTEMPTS.
    [2025-02-07 14:56:39,708] WARNING in event-simulator: USER7 Order failed as the item is OUT OF STOCK.
    [2025-02-07 14:56:43,713] WARNING in event-simulator: USER5 Failed to Login as the account is locked due to MANY FAILED ATTEMPTS.
    [2025-02-07 14:56:47,719] WARNING in event-simulator: USER7 Order failed as the item is OUT OF STOCK.
    [2025-02-07 14:56:48,720] WARNING in event-simulator: USER5 Failed to Login as the account is locked due to MANY FAILED ATTEMPTS.
    [2025-02-07 14:56:53,726] WARNING in event-simulator: USER5 Failed to Login as the account is locked due to MANY FAILED ATTEMPTS.
    [2025-02-07 14:56:55,728] WARNING in event-simulator: USER7 Order failed as the item is OUT OF STOCK.
    [2025-02-07 14:56:58,731] WARNING in event-simulator: USER5 Failed to Login as the account is locked due to MANY FAILED ATTEMPTS.
    [2025-02-07 14:57:03,736] WARNING in event-simulator: USER5 Failed to Login as the account is locked due to MANY FAILED ATTEMPTS.
    [2025-02-07 14:57:03,736] WARNING in event-simulator: USER7 Order failed as the item is OUT OF STOCK.
    [2025-02-07 14:57:08,740] WARNING in event-simulator: USER5 Failed to Login as the account is locked due to MANY FAILED ATTEMPTS.
    [2025-02-07 14:57:11,744] WARNING in event-simulator: USER7 Order failed as the item is OUT OF STOCK.
    [2025-02-07 14:57:13,747] WARNING in event-simulator: USER5 Failed to Login as the account is locked due to MANY FAILED 


    step3: å°‡è­¦å‘Šè¨Šæ¯å­˜å…¥ /opt/dind-878516_logs.txt
    ~~kubectl logs dev-pod-dind-878516 -c log-x -n default | grep -i "warn" | kubectl exec -it controlplane -- bash -c "cat > /opt/dind-878516_logs.txt"~~

    (ä»¥ä¸Šé¡¯ç¤ºéŒ¯èª¤ï¼Œ)

    æ”¹å…ˆæŠŠlogæ‹·è²åˆ°æœ¬åœ°æ–‡ä»¶ï¼Œå†å°‡logæ–‡ä»¶å…§å®¹å¯«å…¥container:
    controlplane ~ âœ  kubectl exec dev-pod-dind-878516 -c log-x -- mkdir /opt;touch /opt/dind-878516_logs.txt  

    controlplane ~ âœ– k cp log-x.txt dev-pod-dind-878516:/opt/dind-878516_logs.txt -c log-x

    controlplane ~ âœ– k exec -it dev-pod-dind-878516 -c log-x -- sh
    / # ls
    bin                 lib                 proc                sys
    dev                 log                 root                tmp
    etc                 media               run                 usr
    event-simulator.py  mnt                 sbin                var
    home                opt                 srv
    / # cat opt/dind-878516_logs.txt 
    [2025-02-07 14:56:28,693] WARNING in event-simulator: USER5 Failed to Login as the account is locked due to MANY FAILED ATTEMPTS.
    [2025-02-07 14:56:31,697] WARNING in event-simulator: USER7 Order failed as the item is OUT OF STOCK.
    [2025-02-07 14:56:33,699] WARNING in event-simulator: USER5 Failed to Login as the account is locked due to MANY FAILED ATTEMPTS.
    ...



    Solution:
    kubectl logs dev-pod-dind-878516 -c log-x | grep WARNING > /opt/dind-878516_logs.txt

    controlplane ~ âœ  kubectl logs dev-pod-dind-878516 -c log-x | grep WARNING > /opt/dind-878516_logs.txt

    controlplane ~ âœ  cat /opt/dind-878516_logs.txt 
    [2025-02-07 14:56:28,693] WARNING in event-simulator: USER5 Failed to Login as the account is locked due to MANY FAILED ATTEMPTS.
    [2025-02-07 14:56:31,697] WARNING in event-simulator: USER7 Order failed as the item is OUT OF STOCK.
    [2025-02-07 14:56:33,699] WARNING in event-simulator: USER5 Failed to Login as the account is locked due to MANY FAILED ATTEMPTS.
    [2025-02-07 14:56:38,707] WARNING in event-simulator: USER5 Failed to Login as the account is locked due to MANY FAILED ATTEMPTS.
    [2025-02-07 14:56:39,708] WARNING in event-simulator: USER7 Order failed as the item is OUT OF STOCK.

2. Troubleshooting + Liveness Probe
  
    We have deployed a few pods in this cluster in various namespaces. Inspect them and identify the pod which is not in a Ready state. Troubleshoot and fix the issue.
    Next, add a check to restart the container on the same pod if the command ls /var/www/html/file_check fails. This check should **start after a delay of 10 seconds** and **run every 60 seconds**.
    You may delete and recreate the object. Ignore the warnings from the pro

    Hint: æ‰¾å‡º
    **READY æ¬„ä½ä¸æ˜¯ 1/1**
    **STATUS æ¬„ä½æ˜¯ CrashLoopBackOffã€Pendingã€Error**ã€~~Completed~~ ç­‰

    controlplane ~ âœ– k get pod -A
    NAMESPACE       NAME                                        READY   STATUS      RESTARTS   AGE
    default         pod-xyz1123                                 1/1     Running     0          29m
    default         webapp-color                                1/1     Running     0          29m
    dev0403         nginx0403                                   1/1     Running     0          29m
    dev0403         pod-dar85                                   1/1     Running     0          29m
    **dev1401       nginx1401                                   0/1     Running     0          29m**
    dev1401         pod-kab87                                   1/1     Running     0          29m
    dev2406         nginx2406                                   1/1     Running     0          29m
    dev2406         pod-var2016                                 1/1     Running     0          29m
    e-commerce      e-com-1123                                  1/1     Running     0          29m
    **ingress-nginx ingress-nginx-admission-create-9cdqh        0/1     Completed   0          28m**
    **ingress-ngin  ingress-nginx-admission-patch-nq7xg         0/1     Completed   1          28m**
    ingress-nginx   ingress-nginx-controller-7fb4f97b75-gcqj5   1/1     Running     0          28m
    kube-system     calico-kube-controllers-5d7d9cdfd8-n6cns    1/1     Running     0          34m
    kube-system     canal-6srw7                                 2/2     Running     0          34m
    kube-system     canal-dwc2k                                 2/2     Running     0          34m
    kube-system     coredns-77d6fd4654-7lvks                    1/1     Running     0          34m
    kube-system     coredns-77d6fd4654-qhgbf                    1/1     Running     0          34m
    kube-system     etcd-controlplane                           1/1     Running     0          34m
    kube-system     kube-apiserver-controlplane                 1/1     Running     0          34m
    kube-system     kube-controller-manager-controlplane        1/1     Running     0          34m
    kube-system     kube-proxy-dt25z                            1/1     Running     0          34m
    kube-system     kube-proxy-nqdps                            1/1     Running     0          34m
    kube-system     kube-scheduler-controlplane                 1/1     Running     0          34m
    marketing       redis-896d5c767-66547                       1/1     Running     0          29m


    åœ¨CKADè€ƒè©¦ä¸­ä¸å¯èƒ½ä¸€å€‹å€‹å»ä¿®æ”¹æ¯å€‹podçš„é…ç½®æ–‡ä»¶ï¼Œæ•…å…ˆæ‰¾åˆ°deployment, ç›´æ¥ä¿®æ”¹deploymentçš„é…ç½®ï¼Œå¦‚æœæ²’æœ‰deploymentå‰‡ä¿®æ”¹pod
    

    æˆ‘çš„é…ç½®: (è§£é¡Œæ€è·¯éŒ¯èª¤!!!)

    controlplane ~ âœ  k get deploy -n dev1401
    No resources found in dev1401 namespace.

    (æ­¤è™•åˆ¤æ–·éŒ¯èª¤ï¼Œç”±æ–¼nginx-admission-create è·Ÿ nginx-admission-patch å…©å€‹podçš„ç‹€æ…‹å‡ç‚ºcompletedï¼Œä¸ç¬¦åˆ"is not in a Ready state")
    ~~controlplane ~ âœ  k get deploy -n ingress-nginx~~
    ~~NAME                       READY   UP-TO-DATE   AVAILABLE   AGE~~
    ~~ingress-nginx-controller   1/1     1            1           33m~~

    ä»¥ä¸‹ä¿®æ”¹çš„æ˜¯ingress-nginx-controller deploy. å¯¦éš›è€ƒè©¦çš„æ™‚å€™ç”±æ–¼podç‹€æ…‹æ˜¯completed,æ•…æ­¤deploymentä¸éœ€è¦ä¿®æ”¹

        spec:
          progressDeadlineSeconds: 600
          replicas: 1
          revisionHistoryLimit: 10
          selector:
            matchLabels:
              app.kubernetes.io/component: controller
              app.kubernetes.io/instance: ingress-nginx
              app.kubernetes.io/name: ingress-nginx
          strategy:
            rollingUpdate:
              maxSurge: 25%
              maxUnavailable: 25%
            type: RollingUpdate
          template:
            metadata:
              creationTimestamp: null
              labels:
                app.kubernetes.io/component: controller
                app.kubernetes.io/instance: ingress-nginx
                app.kubernetes.io/name: ingress-nginx
            spec:
              containers:
              ... (ä¸­é–“å¿½ç•¥)
            livenessProbe:
              failureThreshold: 5
              httpGet:
                path: /healthz
                port: 10254
                scheme: HTTP
              initialDelaySeconds: 10
              periodSeconds: 10
              successThreshold: 1
              timeoutSeconds: 1


        ä¿®æ”¹ç‚º:

            livenessProbe:
              exec: (æ·»åŠ æ­¤block)
                command:
                  - ["ls", "/var/www/html/file_check"] 
              failureThreshold: 5
              httpGet:
                path: /healthz
                port: 10254
                scheme: HTTP
              initialDelaySeconds: 10 (Checked)
              periodSeconds: 60 (Modified)
              successThreshold: 1
              timeoutSeconds: 1


              livenessProbe:
              exec:
                command: ["ls", "/var/www/html/file_check"]
              failureThreshold: 5
              httpGet:
                path: /healthz
                port: 10254
                scheme: HTTP
              initialDelaySeconds: 10
              periodSeconds: 60
              successThreshold: 1
              timeoutSeconds: 1

    
          ERROR:
          # deployments.apps "ingress-nginx-controller" was not valid:
          # * spec.template.spec.containers[0].livenessProbe.httpGet: Forbidden: may not specify more than 1 handler type
          #

            exec è·ŸhttpGet é€™å…©å€‹handlerä¸å¯åŒæ™‚é…ç½®


            å†ä¿®æ”¹ç‚º:
            livenessProbe:
              exec:
                command: ["ls", "/var/www/html/file_check"]
              failureThreshold: 5
              initialDelaySeconds: 10
              periodSeconds: 60
              successThreshold: 1
              timeoutSeconds: 1

      ~~controlplane ~ âœ– k edit deploy -n ingress-nginx ingress-nginx-controller~~
      ~~deployment.apps/ingress-nginx-controller edited~~

      ingress-nginx-admission-create å’Œ ingress-nginx-admission-patch ä¸éœ€è¦ä¿®æ”¹ï¼Œå› ç‚ºå®ƒå€‘æ˜¯ä¸€æ¬¡æ€§é‹è¡Œçš„ Podï¼ŒCompleted ç‹€æ…‹æ˜¯é æœŸè¡Œç‚ºã€‚
      nginx1401 éœ€è¦ä¿®æ­£ï¼Œ**å› ç‚ºå®ƒæ˜¯ä¸€å€‹æŒçºŒé‹è¡Œçš„æœå‹™**ï¼Œä½† READY 0/1ï¼Œè¡¨ç¤º Readiness Probe å¤±æ•—ï¼Œå°è‡´ç„¡æ³•å°å¤–æä¾›æœå‹™ã€‚
      é€™å°±æ˜¯ç‚ºä»€éº¼è§£ç­”åªä¿®æ”¹ nginx1401ï¼Œè€Œä¸è™•ç† ingress-nginx-admission-* é€™å…©å€‹ Podã€‚ 


      Solution: 

      From the doc: "If the probe succeeds, the Pod will be marked as ready and will receive traffic from services. If the **readiness probe fails**, **the pod will be marked unready and will not receive traffic from any services**."

      æ•…æ‡‰å„ªå…ˆæª¢æŸ¥readinessProbeè¨­ç½®!!!!

      æª¢æŸ¥ nginx1401 podé…ç½®:

      k describe pod nginx1401 -n dev1401

      ...
      Events:
      Type     Reason     Age                   From               Message
      ----     ------     ----                  ----               -------
      Normal   Scheduled  4m35s                 default-scheduler  Successfully assigned dev1401/nginx1401 to node01
      Normal   Pulling    4m33s                 kubelet            Pulling image "kodekloud/nginx"
      Normal   Pulled     4m29s                 kubelet            Successfully pulled image "kodekloud/nginx" in 3.969s (3.969s including waiting). Image size: 50986074 bytes.
      Normal   Created    4m29s                 kubelet            Created container nginx
      Normal   Started    4m29s                 kubelet            Started container nginx
      Warning  Unhealthy  97s (x21 over 4m29s)  kubelet            **Readiness probe failed: Get "http://172.17.1.2:8080/": dial tcp 172.17.1.2:8080: connect: connection refused**

      spec:
        containers:
        - image: kodekloud/nginx
          imagePullPolicy: IfNotPresent
          name: nginx
          ports:
          - **containerPort: 9080**
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              **port: 8080**
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}

      **readinessProbeç›£è½çš„ç«¯å£è·ŸcontainerPort(å¯¦éš›ä½¿ç”¨)çš„ç«¯å£ä¸ä¸€è‡´ï¼Œå°è‡´pod ç„¡æ³•æ­£å¸¸å°‡statusä¿®æ­£ç‚ºready**:

      å…ˆå¯¦éš›ç¢ºèªç«¯å£æ˜¯å¦å­˜åœ¨:

      (å€˜è‹¥curl, netstatç­‰æŒ‡ä»¤ä¸å­˜åœ¨ï¼Œç›´æ¥å®‰è£)
      controlplane ~ âœ– kubectl exec -it nginx1401 -n dev1401 -- curl -I http://localhost:8080
      error: Internal error occurred: Internal error occurred: error executing command in container: failed to exec in container: failed to start exec "ae2eaad09b6b6740aa669804cd0de1e940e4f8169c06bc8c640661015b902736": OCI runtime exec failed: exec failed: unable to start container process: exec: "curl": executable file not found in $PATH: unknown


      controlplane ~ âœ– kubectl exec -it nginx1401 -n dev1401 -- sh
      # **apt update && apt install -y net-tools iproute2 curl**
      Get:1 http://security.debian.org/debian-security buster/updates InRelease [34.8 kB]
      Get:2 http://deb.debian.org/debian buster InRelease [122 kB]
      Get:3 http://deb.debian.org/debian buster-updates InRelease [56.6 kB]
      Get:4 http://security.debian.org/debian-security buster/updates/main amd64 Packages [610 kB]
      Get:5 http://deb.debian.org/debian buster/main amd64 Packages [7909 kB]
      Get:6 http://deb.debian.org/debian buster-updates/main amd64 Packages [8788 B]
      Fetched 8741 kB in 2s (4354 kB/s)                         

      # curl -I http://localhost:8080 
      curl: (7) Failed to connect to localhost port 8080: Connection refused
      # curl -I http://localhost:8090         
      curl: (7) Failed to connect to localhost port 8090: Connection refused
      # curl -I http://localhost:9080 
      HTTP/1.1 200 OK
      Server: nginx/1.17.8
      Date: Thu, 06 Feb 2025 17:15:55 GMT
      Content-Type: text/html
      Content-Length: 612
      Last-Modified: Tue, 21 Jan 2020 13:36:08 GMT
      Connection: keep-alive
      ETag: "5e26fe48-264"
      Accept-Ranges: bytes

      spec:
        containers:
        - image: kodekloud/nginx
          imagePullPolicy: IfNotPresent
          name: nginx
          ports:
          - containerPort: 9080
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              **port: 9080** ä¿®æ­£ç‚º9080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1

      controlplane ~ âœ  kc nginx1401.yaml 
      pod/nginx1401 created

      controlplane ~ âœ  k get pod -A
      NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
      default       dev-pod-dind-878516                        3/3     Running   0          21m
      default       pod-xyz1123                                1/1     Running   0          21m
      default       webapp-color                               1/1     Running   0          18m
      dev0403       nginx0403                                  1/1     Running   0          21m
      dev0403       pod-dar85                                  1/1     Running   0          21m
      dev1401       nginx1401                                  1/1     Running   0          3s
      dev1401       pod-kab87                                  1/1     Running   0          21m
      dev2406       nginx2406                                  1/1     Running   0          21m
      dev2406       pod-var2016                                1/1     Running   0          21m
      e-commerce    e-com-1123                                 1/1     Running   0          21m
      kube-system   calico-kube-controllers-5d7d9cdfd8-ql5jv   1/1     Running   0          24m
      kube-system   canal-q9rzw                                2/2     Running   0          24m
      kube-system   canal-rmclb                                2/2     Running   0          23m
      kube-system   coredns-77d6fd4654-99vzv                   1/1     Running   0          24m
      kube-system   coredns-77d6fd4654-jd5sh                   1/1     Running   0          24m
      kube-system   etcd-controlplane                          1/1     Running   0          24m
      kube-system   kube-apiserver-controlplane                1/1     Running   0          24m
      kube-system   kube-controller-manager-controlplane       1/1     Running   0          24m
      kube-system   kube-proxy-b5m4v                           1/1     Running   0          23m
      kube-system   kube-proxy-tdl64                           1/1     Running   0          24m
      kube-system   kube-scheduler-controlplane                1/1     Running   0          24m
      marketing     redis-896d5c767-hbc2j                      1/1     Running   0          21m


      (å»ºè­°æ·»åŠ : initialDelaySeconds: 10  # âœ… ç­‰å¾… 10 ç§’å¾Œå†æª¢æŸ¥ æœ‰æ™‚å€™ Readiness Probe éœ€è¦ä¸€äº›æ™‚é–“ä¾†é€šéï¼Œå»ºè­°å¢åŠ  initialDelaySecondsï¼Œæ¸›å°‘ Readiness Probe éæ—©å¤±æ•—çš„æƒ…æ³ã€‚)

      ç¹¼çºŒä¿®æ”¹æ–‡ä»¶ï¼Œæ·»åŠ livenessProbeå€å¡Š

      controlplane ~ âœ  vim nginx1401.yaml 

      spec:
      containers:
      - image: kodekloud/nginx
        imagePullPolicy: IfNotPresent
        name: nginx
        ports:
        - containerPort: 9080
          protocol: TCP
        **livenessProbe:** å†æ·»åŠ æ­¤block
          exec:
            command:
              - sh
              - -c
              - ls /var/www/html/file_check  # âœ…  **å¦‚æœé€™å€‹å‘½ä»¤å¤±æ•—ï¼Œå®¹å™¨å°±æœƒé‡å•Ÿ**
          initialDelaySeconds: 10  # âœ…  å•Ÿå‹•å¾Œ 10 ç§’æ‰é–‹å§‹æª¢æŸ¥
          periodSeconds: 60        # âœ…  ä¹‹å¾Œæ¯ 60 ç§’åŸ·è¡Œä¸€æ¬¡
          failureThreshold: 1      # âœ…  ä¸€æ¬¡å¤±æ•—å°±é‡å•Ÿ
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /
            port: 9080
            scheme: HTTP
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources: {}

      controlplane ~ âœ  k delete pod nginx1401 -n dev1401 --force
      Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.
      pod "nginx1401" force deleted

      controlplane ~ âœ  kc nginx1401.yaml 
      pod/nginx1401 created

      controlplane ~ âœ  k get pod -A
      NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
      default       dev-pod-dind-878516                        3/3     Running   0          32m
      default       pod-xyz1123                                1/1     Running   0          32m
      default       webapp-color                               1/1     Running   0          29m
      dev0403       nginx0403                                  1/1     Running   0          32m
      dev0403       pod-dar85                                  1/1     Running   0          32m
      dev1401       nginx1401                                  1/1     Running   0          2s
      dev1401       pod-kab87                                  1/1     Running   0          32m
      dev2406       nginx2406                                  1/1     Running   0          32m
      dev2406       pod-var2016                                1/1     Running   0          32m
      e-commerce    e-com-1123                                 1/1     Running   0          32m
      kube-system   calico-kube-controllers-5d7d9cdfd8-ql5jv   1/1     Running   0          35m
      kube-system   canal-q9rzw                                2/2     Running   0          35m
      kube-system   canal-rmclb                                2/2     Running   0          34m
      kube-system   coredns-77d6fd4654-99vzv                   1/1     Running   0          35m
      kube-system   coredns-77d6fd4654-jd5sh                   1/1     Running   0          35m
      kube-system   etcd-controlplane                          1/1     Running   0          35m
      kube-system   kube-apiserver-controlplane                1/1     Running   0          35m
      kube-system   kube-controller-manager-controlplane       1/1     Running   0          35m
      kube-system   kube-proxy-b5m4v                           1/1     Running   0          34m
      kube-system   kube-proxy-tdl64                           1/1     Running   0          35m
      kube-system   kube-scheduler-controlplane                1/1     Running   0          35m
      marketing     redis-896d5c767-hbc2j                      1/1     Running   0          32m




### 05. Services and Networking (20%)
1. Ingress-Network Policy + Service Debugging
  We have deployed a new pod called secure-pod and a service called secure-service. Incoming or Outgoing connections to this pod are not working.
  Troubleshoot why this is happening. 

    Make sure that incoming connection from the pod webapp-color are successful.
    Important: Don't delete any current objects deployed.

    |   æ–¹å¼  |	        è¨­ç½®ä½ç½®        |      	ä½œç”¨       |	                é©ç”¨å ´æ™¯                |
    |:--------|:-----------------------|:-----------------|:----------------------------------------|
    |Ingress|	åœ¨ ç›®æ¨™ Pod (secure-pod)|	é™åˆ¶ã€Œèª°å¯ä»¥é€²ä¾†ã€|	ç•¶ secure-pod å·²ç¶“æœ‰ NetworkPolicyï¼Œæƒ³å…è¨± webapp-color è¨ªå•æ™‚|
    |Egress|	åœ¨ ç™¼é€æµé‡çš„ Pod (webapp-color)|	é™åˆ¶ã€Œèƒ½ç™¼å»å“ªã€|	ç•¶ä½ æƒ³é˜²æ­¢ webapp-color è¨ªå•å…¶ä»– Pod æˆ–å¤–éƒ¨æœå‹™æ™‚|
    âœ” åœ¨é€™å€‹å•é¡Œä¸­ï¼Œsecure-pod å¯èƒ½å·²ç¶“æœ‰ä¸€å€‹ Ingress é™åˆ¶ï¼Œæ‰€ä»¥è¦åœ¨ secure-pod ä¸Šè¨­ç½® Ingress Policyï¼Œå…è¨± webapp-color é€²ä¾†ï¼Œè€Œä¸æ˜¯åœ¨ webapp-color ä¸Šè¨­ç½® Egress Policyï¼
    
    æˆ‘çš„é…ç½®:


    controlplane ~ âœ  k get networkpolicies
    NAME                    POD-SELECTOR     AGE
    default-deny            <none>           38m


    controlplane ~ âœ– cat secure-networkpolicy.yaml 
    apiVersion: networking.k8s.io/v1
    kind: NetworkPolicy
    metadata:
      name: secure-network-policy
      namespace: default
    spec:
      podSelector:
        matchLabels:
          **pod: secure-pod** æœªå…ˆç¢ºèªæ˜¯å¦çœŸæœ‰æ­¤label!!!!
      policyTypes:
      - Ingress
      ingress:
      - from:
        - podSelector:
            matchLabels:
              **pod: webapp-color** æœªå…ˆç¢ºèªæ˜¯å¦çœŸæœ‰æ­¤label!!!!
        ports:
        - protocol: TCP
          port: 80

 
    controlplane ~ âœ– alias kc="k create -f"

    controlplane ~ âœ  kc secure-networkpolicy.yaml 
    networkpolicy.networking.k8s.io/secure-network-policy created


    Usage: netstat [-ral] [-tuwx] [-enWp]

    Display networking information

            -r      Routing table
            -a      All sockets
            -l      Listening sockets
                    Else: connected sockets
            -t      TCP sockets
            -u      UDP sockets
            -w      Raw sockets
            -x      Unix sockets
                    Else: all socket types
            -e      Other/more information
            -n      Don't resolve names
            -W      Wide display
            -p      Show PID/program name for sockets
      /opt # netstat -w 5 secure-service 80
      Active Internet connections (w/o servers)
      Proto Recv-Q Send-Q Local Address           Foreign Address         State  
          
          
      /opt # netstat -w 5 -h
      netstat: unrecognized option: h
      BusyBox v1.28.4 (2018-07-17 15:21:40 UTC) multi-call binary.

      Usage: netstat [-ral] [-tuwx] [-enWp]

      Display networking information

              -r      Routing table
              -a      All sockets
              -l      Listening sockets
                      Else: connected sockets
              -t      TCP sockets
              -u      UDP sockets
              -w      Raw sockets
              -x      Unix sockets
                      Else: all socket types
              -e      Other/more information
              -n      Don't resolve names
              -W      Wide display
              -p      Show PID/program name for sockets
      
      **IMPORTANT**:
      åœ¨é…ç½®podSelectorä¹‹å‰ï¼Œå…ˆç”¨--show-labelsåƒæ•¸æª¢æŸ¥è¨­ç½®æ˜¯å¦æ­£ç¢º!!!!
      kubectl get pods --show-labels
      ç¢ºä¿ secure-pod å’Œ webapp-color çš„æ¨™ç±¤æ­£ç¢ºã€‚

      controlplane ~ âœ  k get networkpolicy
      NAME                    POD-SELECTOR     AGE
      default-deny            <none>           4m35s
      secure-network-policy   pod=secure-pod   7s


      controlplane ~ âœ  k exec -it webapp-color -- sh
      /opt # nc -vz -w 5 secure-pod 80 
      ###! nc -vz -w 5 secure-pod 80 æœƒå¤±æ•—ï¼Œå› ç‚º Pod åç¨±ä¸æœƒè‡ªå‹•è§£æç‚º Pod çš„ IP åœ°å€ã€‚
      ###! Kubernetes å…§éƒ¨ DNS è§£æä¸»è¦é©ç”¨æ–¼ Serviceï¼Œè€Œä¸æ˜¯å–®ç¨çš„ Podã€‚
      nc: bad address 'secure-pod'
      /opt # nc -vz -w 5 secure-service 80
      nc: secure-service (172.20.244.248:80): Operation timed out  (ç”±æ–¼ç¬¬ä¸€å€‹pod selectorè¨­ç½®éŒ¯èª¤ï¼Œå°è‡´ç¶²è·¯ä¸é€š)


      /opt # nc -vz -w 5 secure-service 80 (ä¿®æ”¹pod selectoré…ç½®å¾Œï¼Œæ¸¬è©¦æˆåŠŸ)
      secure-service (172.20.244.248:80) open

      
      Solution:

      Step0: æª¢æŸ¥label!!! (å°å¿ƒæ­¤æ­¥é©Ÿå®¹æ˜“è¢«å¿½ç•¥!!!!)

      controlplane ~ âœ  k get pod --show-labels 
      NAME           READY   STATUS    RESTARTS   AGE    LABELS
      secure-pod     1/1     Running   0          103s   **run=secure-pod**
      webapp-color   1/1     Running   0          2m4s   **name=webapp-color**

      Step1: æª¢æŸ¥ NetworkPolicy
      æª¢æŸ¥ç›®å‰æ˜¯å¦æœ‰å½±éŸ¿ secure-pod çš„ NetworkPolicyï¼ŒåŸ·è¡Œï¼š
      
      k get networkpolicy (-n default)

      Step2: æ‡‰ç”¨ä¿®æ­£çš„ NetworkPolicy
      å»ºç«‹ä»¥ä¸‹ NetworkPolicyï¼Œç¢ºä¿ webapp-color Pod å¯ä»¥è¨ªå• secure-podï¼š
      vim networkpolicy.yaml
      controlplane ~ âœ  cat secure-networkpolicy.yaml 
      apiVersion: networking.k8s.io/v1
      kind: NetworkPolicy
      metadata:
        name: secure-network-policy
        namespace: default
      spec:
        **podSelector**: # é¸æ“‡ è¢«ä¿è­·çš„ Pod(æ‡‰è©²åŒ¹é… secure-pod)
          matchLabels:
            **run: secure-pod** # åŸæœ¬è¨­ç½®ç‚ºpod: secure-pod çµæœwebapp-color ç„¡æ³•è¨ªå•åˆ°secure-pod
        policyTypes:
        - Ingress
        ingress:
        - from:
          - **podSelector**: # é¸æ“‡ å…è¨±è¨ªå•çš„ Podï¼ˆæ‡‰è©²åŒ¹é… webapp-colorï¼‰
              matchLabels:
                **name: webapp-color**
          ports:
          - protocol: TCP
            port: 80


      k create -f networkpolicy.yaml

      controlplane ~ âœ  k get networkpolicies.networking.k8s.io 
      NAME                    POD-SELECTOR     AGE
      default-deny            <none>           4m31s
      secure-network-policy   run=secure-pod   5s

      Step3: é©—è­‰ç¶²çµ¡é€£é€šæ€§
      Then check the connectivity from the webapp-color pod to the secure-pod:-
      æª¢æŸ¥ webapp-color æ˜¯å¦èƒ½å¤ é€£æ¥åˆ° secure-service:
      k exec -it webapp-color --sh
      root@controlplane:~$ kubectl exec -it webapp-color -- sh
      
      ç„¶å¾Œåœ¨ Pod å…§éƒ¨åŸ·è¡Œï¼š**nc -v -z -w 5** <secure-service> <80>
      nc: Netcat å‘½ä»¤ï¼Œç”¨ä¾†æ¸¬è©¦ç¶²è·¯é€£æ¥
      -v: é¡¯ç¤ºè©³ç´°æ¨¡å¼ï¼ˆverbose modeï¼‰ï¼Œæœƒè¼¸å‡ºé€£æ¥éç¨‹çš„è©³ç´°ä¿¡æ¯ã€‚
      -z: åªæª¢æŸ¥é€£æ¥æ˜¯å¦æˆåŠŸï¼Œä¸ç™¼é€æ•¸æ“šï¼Œé©ç”¨æ–¼ç«¯å£æƒæã€‚
      -w: è¨­å®šè¶…æ™‚æ™‚é–“ç‚º 5 ç§’ï¼Œå¦‚æœè¶…é 5 ç§’ç„¡å›æ‡‰å‰‡æ”¾æ£„é€£æ¥
      <ç›®æ¨™æœå‹™åç¨±ï¼ˆKubernetes å…§éƒ¨æœå‹™ï¼‰>
      <ç«¯å£>

      /opt # nc -v -z -w 5 secure-service 80

      å¦‚æœæˆåŠŸï¼Œå°‡é¡¯ç¤º:
      secure-service [10.100.1.23] 80 (http) open

      å¦‚æœå¤±æ•—ï¼Œå¯èƒ½æœƒé¡¯ç¤ºï¼š
      nc: connect to secure-service port 80 (tcp) failed: Connection refused


      **IMPORTANT**:
      å¯ä»¥ä½¿ç”¨ telnet æŒ‡ä»¤å—ï¼Ÿ
      æ˜¯çš„ï¼Œtelnet ä¹Ÿå¯ä»¥ç”¨ä¾†æ¸¬è©¦é€£æ¥ï¼Œä¾‹å¦‚ï¼š
      telnet secure-service 80

      ä½†èˆ‡ ncï¼ˆNetcatï¼‰ç›¸æ¯”ï¼Œtelnet æœ‰å¹¾å€‹å•é¡Œï¼š
      éœ€è¦æ‰‹å‹•è¼¸å…¥ Ctrl+C æ‰èƒ½é€€å‡ºï¼Œç„¡æ³•åƒ nc -z é‚£æ¨£å¿«é€Ÿæ¸¬è©¦ç«¯å£æ˜¯å¦é–‹æ”¾ã€‚
      æŸäº› Linux å®¹å™¨é¡åƒï¼ˆå¦‚ alpineï¼‰ä¸ä¸€å®šå…§å»º telnetï¼Œä½†é€šå¸¸æœƒæœ‰ ncã€‚
      telnet ä¸»è¦ç”¨æ–¼äº¤äº’å¼æ¸¬è©¦ï¼Œå¦‚æœåªéœ€è¦æª¢æŸ¥é€£é€šæ€§ï¼Œnc æœƒæ›´æ–¹ä¾¿ã€‚
      åœ¨ CKAD è€ƒè©¦æˆ– Kubernetes ç’°å¢ƒä¸­ï¼Œä¸€èˆ¬ æ¨è–¦ä½¿ç”¨ ncï¼Œå› ç‚ºå®ƒç°¡å–®é«˜æ•ˆ

      ç‚ºä½•ä½¿ç”¨ netcat (nc)ï¼Ÿ
      **ncï¼ˆNetcatï¼‰é€šå¸¸å…§å»ºæ–¼è¨±å¤šå®¹å™¨é¡åƒ**ï¼Œä¾‹å¦‚ï¼š

      busyboxï¼ˆè¼•é‡å®¹å™¨ï¼ŒCKAD è€ƒè©¦å¸¸è¦‹ï¼‰
      alpineï¼ˆè¼•é‡ç´š Linux ç‰ˆæœ¬ï¼‰
      debianã€ubuntuï¼ˆè¼ƒå®Œæ•´çš„ Linux ç’°å¢ƒï¼‰
      å¾ˆå¤šå®˜æ–¹çš„ Kubernetes å®¹å™¨æ˜ åƒä¸æœƒå…§å»º telnetï¼Œä½†å¤§éƒ¨åˆ†æœƒæœ‰ ncï¼Œå› æ­¤ nc æ˜¯è¼ƒé€šç”¨çš„é¸æ“‡ã€‚

      å¦‚æœ Pod å…§ä»€éº¼æŒ‡ä»¤éƒ½æ²’æœ‰ï¼Œä¾‹å¦‚ï¼š
        kubectl exec -it mypod -- sh
        sh: nc: not found

      **æ³• 1ï¼šé€²å…¥å…¶ä»–æœ‰ nc çš„ Pod æ¸¬è©¦**
      å¯ä»¥ä½¿ç”¨ä¸€å€‹å·²æœ‰ nc æŒ‡ä»¤çš„ Podï¼Œä¾‹å¦‚ï¼š
      kubectl run debug-pod --rm -it --image=busybox -- sh
      ç„¶å¾Œåœ¨é€™å€‹ Pod å…§åŸ·è¡Œï¼š
      nc -v -z -w 5 secure-service 80
      é€™æ¨£å¯ä»¥é¿å…å®‰è£æ–°è»Ÿé«”ï¼Œé©ç”¨æ–¼ CKAD è€ƒè©¦å ´æ™¯ã€‚

      **å¦‚æœ Pod å…§æ²’æœ‰ ncï¼Œé¦¬ä¸Šå‰µå»ºä¸€å€‹ busybox æˆ– alpine Pod ä¾†æ¸¬è©¦**
      kubectl run debug-pod --rm -it --image=busybox -- sh
      ä½¿ç”¨ nc -z -v ä¾†å¿«é€Ÿæ¸¬è©¦ç«¯å£é–‹æ”¾æƒ…æ³
      nc -z -v secure-service 80


      æ³• 2ï¼šæ‰‹å‹•å®‰è£ nc
      å¦‚æœ Pod å…è¨±å®‰è£ï¼ˆéœ€ root æ¬Šé™ï¼‰ï¼Œå¯ä»¥è©¦è‘—å®‰è£ï¼š

      Alpine Linuxï¼ˆå¸¸è¦‹æ–¼ CKAD è€ƒè©¦ç’°å¢ƒï¼‰
      
          apk add --no-cache netcat-openbsd

      Debian / Ubuntu
        
          apt update && apt install -y netcat

      ä½† CKAD è€ƒè©¦æ™‚é–“æœ‰é™ï¼Œé€šå¸¸ æ–¹å¼ 1 æœƒæ›´å¿«ï¼

      ä¸è¦æµªè²»æ™‚é–“å®‰è£ telnet æˆ– ncï¼Œç›´æ¥ç”¨ busybox æ¸¬è©¦æœƒæ›´å¿«ã€‚




2. Ingress + Virtual Host Routing
    Create a single ingress resource called ingress-vh-routing. The resource should route HTTP traffic to multiple hostnames as specified below:
    The service video-service should be accessible on http://watch.ecom-store.com:30093/video
    The service apparels-service should be accessible on http://apparels.ecom-store.com:30093/wear
    To ensure that the path is correctly rewritten for the backend service, add the following annotation to the resource:
    nginx.ingress.kubernetes.io/rewrite-target: /
    Here 30093 is the port used by the Ingress Controller

    controlplane ~ âœ  k get deployment -o wide
    NAME              READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS      IMAGES                         SELECTOR
    webapp-apparels   1/1     1            1           109s   simple-webapp   kodekloud/ecommerce:apparels   app=webapp-apparels
    webapp-video      1/1     1            1           109s   simple-webapp   kodekloud/ecommerce:video      app=webapp-video

    controlplane ~ âœ– vim ingress-vh-routing.yaml
    apiVersion: networking.k8s.io/v1
    kind: Ingress
    metadata:
      name: ingress-vh-routing
      annotations:
        nginx.ingress.kubernetes.io/rewrite-target: /
    spec:
      rules:
      - host: watch.ecom-store.com
        http:
          paths:
          - path: /video
            pathType: Prefix
            backend:
              service:
                name: video-service
                port:
                  ~~number: 30093~~
      - host: apparels.ecom-store.com
        http:
          paths:
          - path: /wear
            pathType: Prefix
            backend:
              service:
                name: apparels-service
                port:
                  number: ~~30093~~

    controlplane ~ âœ  k create -f  ingress-vh-routing.yaml
    ingress.networking.k8s.io/ingress-vh-routing created


    ingress.networking.k8s.io/ingress-vh-routing edited

    controlplane ~ âœ  k get ingress
    NAME                 CLASS    HOSTS                                          ADDRESS          PORTS   AGE
    ingress-vh-routing   <none>   watch.ecom-store.com,apparels.ecom-store.com   172.20.103.190   80      4m40s



    Solution:

    controlplane ~ âœ  k get svc 
    NAME               TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
    apparels-service   ClusterIP   172.20.14.91   <none>        8080/TCP   14m
    kubernetes         ClusterIP   172.20.0.1     <none>        443/TCP    62m
    video-service      ClusterIP   172.20.61.90   <none>        8080/TCP   14m

    controlplane ~ âœ– vim ingress-vh-routing.yaml

    ---
    kind: Ingress
    apiVersion: networking.k8s.io/v1
    metadata:
      name: ingress-vh-routing
      annotations:
        nginx.ingress.kubernetes.io/rewrite-target: /
    spec:
      rules:
      - host: watch.ecom-store.com
        http:
          paths:
          - pathType: Prefix
            path: "/video"
            backend:
              service:
                name: video-service
                port:
                  number: **8080**
      - host: apparels.ecom-store.com
        http:
          paths:
          - pathType: Prefix
            path: "/wear"
            backend:
              service:
                name: apparels-service
                port:
                  number: **8080**



    controlplane ~ âœ  k get ingress
    NAME                 CLASS    HOSTS                                          ADDRESS          PORTS   AGE
    ingress-vh-routing   <none>   watch.ecom-store.com,apparels.ecom-store.com   172.20.103.190   80      11m

    controlplane ~ âœ  curl -I http://172.20.103.190
    HTTP/1.1 404 Not Found
    Date: Fri, 07 Feb 2025 11:40:09 GMT
    Content-Type: text/html
    Content-Length: 146
    Connection: keep-alive


    controlplane ~ âœ  curl -H "Host: watch.ecom-store.com" http://172.20.103.190/video

    <!doctype html>
    <title>Hello from Flask</title>
    <body style="background: #30336b;">

    <div style="color: #e4e4e4;
        text-align:  center;
        height: 90px;
        vertical-align:  middle;">
        <img src="https://res.cloudinary.com/cloudusthad/image/upload/v1547052431/video.jpg">

    </div>

    </body>
    controlplane ~ âœ  

    controlplane ~ âœ  curl -H "Host: apparels.ecom-store.com" http://172.20.103.190/wear
    <!doctype html>
    <title>Hello from Flask</title>
    <body style="background: #2980b9;">

    <div style="color: #e4e4e4;
        text-align:  center;
        height: 90px;
        vertical-align:  middle;">
        <img src="https://res.cloudinary.com/cloudusthad/image/upload/v1547052428/apparels.jpg">

    </div>

    </body>


    Ingress ä¾è³´ Host ä¾†åŒ¹é…æµé‡
    Ingress è¨­å®šäº† åŸºæ–¼ Host çš„è·¯ç”±ï¼ˆwatch.ecom-store.com å’Œ apparels.ecom-store.comï¼‰ã€‚
    ç•¶ä½ ç›´æ¥è¨ªå• http://172.20.103.190/videoï¼Œç€è¦½å™¨çš„è«‹æ±‚ ä¸æœƒå¸¶æœ‰ Host æ¨™é ­ï¼ŒKubernetes Ingress Controller ç„¡æ³•è­˜åˆ¥è©²è«‹æ±‚æ‡‰è©²ç™¼é€åˆ°å“ªå€‹ Serviceï¼Œå°è‡´ 404 Not Foundã€‚
    2ï¸âƒ£ Ingress Controller é è¨­æ‹’çµ•æœªåŒ¹é…çš„è«‹æ±‚
    ç•¶è«‹æ±‚çš„ Host èˆ‡ Ingress rules ä¸åŒ¹é…æ™‚ï¼ŒIngress Controller ä¸æœƒè‡ªå‹•å°‡æµé‡è½‰ç™¼ï¼Œè€Œæ˜¯è¿”å› 404 Not Foundã€‚
    è§£æ±ºæ–¹æ³•ï¼šä½ éœ€è¦å‘Šè¨´ç€è¦½å™¨ä½¿ç”¨ watch.ecom-store.com ä¾†ç™¼é€è«‹æ±‚ï¼ˆè¦‹ä¸‹æ–¹è§£æ±ºæ–¹æ¡ˆï¼‰ã€‚
    3ï¸âƒ£ DNS è§£æå•é¡Œ
    watch.ecom-store.com ä¸¦ä¸æ˜¯ä¸€å€‹å…¬å…±åŸŸåï¼Œç€è¦½å™¨ç„¡æ³•è§£æå®ƒåˆ° 172.20.103.190ã€‚
    è§£æ±ºæ–¹æ³•ï¼šæ‰‹å‹•é…ç½® /etc/hosts ä¾†è®“æœ¬æ©ŸçŸ¥é“é€™å€‹åŸŸåæ‡‰è©²è§£æåˆ° Ingress Controller çš„ IPã€‚



        


