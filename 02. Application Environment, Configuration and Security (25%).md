1.  Docker Define, build and modify container images (ÈÄô‰∏çÊúÉÂá∫ÁèæÂú®CKADËÄÉË©¶‰∏≠)
    1)  How many images are available on this host?

            docker images

        ~ ‚ûú  docker images
        REPOSITORY                      TAG       IMAGE ID       CREATED        SIZE
        alpine                          latest    91ef0af61f39   3 months ago   7.79MB
        nginx                           alpine    c7b4f26a7d93   4 months ago   43.2MB
        nginx                           latest    39286ab8a5e1   4 months ago   188MB
        postgres                        latest    b781f3a53e61   4 months ago   432MB
        ubuntu                          latest    edbfe74c41f8   4 months ago   78MB
        redis                           latest    590b81f2fea1   5 months ago   117MB
        mysql                           latest    a82a8f162e18   5 months ago   586MB
        kodekloud/simple-webapp-mysql   latest    129dd9f67367   6 years ago    96.6MB
        kodekloud/simple-webapp         latest    c6e3cd9aae36   6 years ago    84.8MB

        Answer: 9 

    2)  We just pulled a new image. What is the tag on the newly pulled NGINX image? Êñ∞ÊãâÂèñÁöÑ‚Äã‚ÄãNGINXÈè°ÂÉè‰∏äÁöÑÊ®ôÁ±§ÊòØ‰ªÄÈ∫º  (Ê≠§È°å‰∏çÈáçË¶Å)

        Answer: 1.14-alpine

        ~ ‚ûú  docker images
        REPOSITORY                      TAG           IMAGE ID       CREATED        SIZE
        alpine                          latest        91ef0af61f39   3 months ago   7.79MB
        nginx                           alpine        c7b4f26a7d93   4 months ago   43.2MB
        nginx                           latest        39286ab8a5e1   4 months ago   188MB
        postgres                        latest        b781f3a53e61   4 months ago   432MB
        ubuntu                          latest        edbfe74c41f8   4 months ago   78MB
        redis                           latest        590b81f2fea1   5 months ago   117MB
        mysql                           latest        a82a8f162e18   5 months ago   586MB
        nginx                           1.14-alpine   8a2fb25a19f5   5 years ago    16MB
        kodekloud/simple-webapp-mysql   latest        129dd9f67367   6 years ago    96.6MB
        kodekloud/simple-webapp         latest        c6e3cd9aae36   6 years ago    84.8MB

        È°åÁõÆÈô∑Èò±ÔºöTAG ÁöÑÂØ¶ÈöõÂê´Áæ©
        È°åÁõÆ‰∏¶Ê≤íÊúâÊòéÁ¢∫Ë¶ÅÊ±ÇÊâæÂà∞ "ÂâõÂâõÊãâÂèñÁöÑÊò†ÂÉè"„ÄÇÈ°åÁõÆÁöÑÈáçÈªûÂú®Êñº NGINX Êò†ÂÉèÁöÑÊâÄÊúâ TAGÔºåËÄå 1.14-alpine ÊòØÊòéÁ¢∫Â≠òÂú®ÁöÑ TAG„ÄÇ

        ÈÅ∏È†ÖËß£ÈáãÔºö
        Ê†πÊìöÈÅ∏È†ÖÔºö

        11cd0b38bc3c ‚Üí ‰∏çÊòØ TAGÔºåÊéíÈô§„ÄÇ
        1.14-alpine ‚Üí ÊòØ‰∏ÄÂÄãÊòéÁ¢∫ÁöÑ TAG„ÄÇ
        perl ‚Üí ‰∏çÊòØ TAGÔºåÊéíÈô§„ÄÇ
        1.10.1 ‚Üí ‰∏çÂ≠òÂú®Êñº docker images ÁöÑËº∏Âá∫‰∏≠ÔºåÊéíÈô§„ÄÇ
        1.14 ‚Üí ‰∏çÂ≠òÂú®Êñº docker images ÁöÑËº∏Âá∫‰∏≠ÔºåÊéíÈô§„ÄÇ
        Âõ†Ê≠§ÔºåÂîØ‰∏ÄÁöÑÂêàÁêÜÁ≠îÊ°àÊòØ 1.14-alpine„ÄÇ

        ÁÇ∫‰ªÄÈ∫ºÁ≠îÊ°à‰∏çÊòØ latestÔºü
        È°åÁõÆÊ≤íÊúâÊòéÁ¢∫Ë™™Ë¶ÅÊâæÊñ∞ÊãâÂèñÁöÑ TAGÔºåËÄåÊòØË¶Å„ÄåÊ™¢Êü• TAG Âàó„ÄçÔºå‰∏¶ÂæûÁµ¶ÂÆöÁöÑÈÅ∏È†Ö‰∏≠ÈÅ∏ÊìáÊ≠£Á¢∫Á≠îÊ°à„ÄÇÂæûËº∏Âá∫ÁöÑ TAG Âàó‰∏≠ÔºåÂîØ‰∏ÄÂåπÈÖçÁöÑÈÅ∏È†ÖÊòØ 1.14-alpine„ÄÇ

        È°åÁõÆË®≠Ë®àÊÑèÂúñ
        È°åÁõÆÂèØËÉΩË®≠Ë®à‰∫Ü‰∏ÄÂÄãÊ∑∑Ê∑ÜÈªûÔºö
        ÊÇ®ÂèØËÉΩÊúÉ‰∏ãÊÑèË≠òË™çÁÇ∫ÂâõÊãâÂèñÁöÑ nginx:latest ÊòØÁ≠îÊ°àÔºå‰ΩÜÂæûÈÅ∏È†ÖÂíåËº∏Âá∫‰æÜÁúãÔºå1.14-alpine ÊâçÊòØËàáÈ°åÁõÆÊèêÁ§∫‰∏ÄËá¥ÁöÑÊ≠£Á¢∫ÈÅ∏È†Ö„ÄÇ

        ÁµêË´ñ
        Á≠îÊ°àÊòØ 1.14-alpineÔºåÂéüÂõ†Âú®ÊñºÔºö
        docker images Ëº∏Âá∫‰∏≠Ôºå1.14-alpine ÊòéÁ¢∫Â≠òÂú®Êñº TAG Âàó„ÄÇ
        ÈÅ∏È†Ö‰∏≠Ôºå1.14-alpine ÊòØÂîØ‰∏ÄÁ¨¶ÂêàÁöÑÁ≠îÊ°à„ÄÇ
        È°åÁõÆÈáçÈªûÊòØÊéíÊü• TAGÔºåËÄå‰∏çÊòØÂñÆÁ¥î‰æùË≥¥Êñ∞ÊãâÂèñÁöÑÁâàÊú¨„ÄÇ

    3)  We just downloaded the code of an application. What is the base image used in the Dockerfile?
        Inspect the Dockerfile in the webapp-color directory.
        
        Hint: You can either open the file using vi /root/webapp-color/Dockerfile (or using commands such as cat/more/less/vim) and look for the FROM instruction or search for it directly using grep -i FROM /root/webapp-color/Dockerfile.

            vi /root/webapp-color/Dockerfile

            FROM python:3.6

            RUN pip install flask

            COPY . /opt/

            EXPOSE 8080

            WORKDIR /opt

            ENTRYPOINT ["python", "app.py"]
            ~                               
            "~/webapp-color/Dockerfile" 11L, 113B
        
        Ê™¢Êü• Dockerfile Êñá‰ª∂ÁÑ∂ÂæåÂ∞ãÊâæ FROM Êåá‰ª§ÁöÑÂÖßÂÆπ (Âõ†Âü∫Â∫ïÊò†ÂÉèÊòØÂú® FROM Êåá‰ª§ÂæåÊåáÂÆö)

        OR 

            grep -i FROM /root/webapp-color/Dockerfile

        ‰ΩøÁî® grep Âø´ÈÄüÂÆö‰ΩçÔºåÂèØ‰ª•Áõ¥Êé•Âü∑Ë°å‰ª•‰∏äÊåá‰ª§ÔºåÊúÉÂø´ÈÄüÊâæÂà∞ FROM Êåá‰ª§ÂèäÂÖ∂ÊâÄ‰ΩøÁî®ÁöÑÊò†ÂÉè

        Answer: python:3.6

    4)  To what location within the container is the application code copied to during a Docker build?
        Inspect the Dockerfile in the webapp-color directory.
        
        Hint: Open the Dockerfile and look for COPY command.

            cat /root/webapp-color/Dockerfile
            grep -i COPY /root/webapp-color/Dockerfile
            COPY . /usr/src/app
        Ê™¢Êü• Dockerfile Êñá‰ª∂Ôºå‰ΩøÁî®‰ª•‰∏äÂëΩ‰ª§Êü•ÁúãÊñá‰ª∂ÂÖßÂÆπ

        OR:
            
            grep -i COPY /root/webapp-color/Dockerfile

        Êü•Êâæ COPY Êåá‰ª§Ôºå‰ΩøÁî®‰ª•‰∏äÊåá‰ª§Âø´ÈÄüÊâæÂà∞ COPY Êåá‰ª§

        Answer: /opt/

    5)  When a container is created using the image built with this Dockerfile, what is the command used to RUN the application inside it.
        Inspect the Dockerfile in the webapp-color directory.

        WRONG ANSWER: pip install flask (ÊßãÂª∫ Docker Êò†ÂÉèÊôÇÂü∑Ë°åÁöÑÂëΩ‰ª§„ÄÇ)

        ~ ‚ûú  grep -i RUN webapp-color/Dockerfile
        RUN pip install flask

        Á≠îÊ°à‰∏çÊòØ pip install flask ÁöÑÂéüÂõ†Âú®Êñº È°åÁõÆË©¢ÂïèÁöÑÊòØÂÆπÂô®ÂïüÂãïÊôÇÂü∑Ë°åÁöÑÂëΩ‰ª§ÔºåËÄåÈùûÊßãÂª∫Êò†ÂÉèÊôÇÂü∑Ë°åÁöÑÂëΩ‰ª§„ÄÇ
        Hint: Open the Dockerfile and look for ENTRYPOINT command.

            cat webapp-color/Dockerfile
        
        ~ ‚úñ cat webapp-color/Dockerfile         
        FROM python:3.6

        RUN pip install flask

        COPY . /opt/

        EXPOSE 8080

        WORKDIR /opt

        ENTRYPOINT ["python", "app.py"] (ENTRYPOINT ÂÆöÁæ©ÂÆπÂô®ÂïüÂãïÊôÇÂü∑Ë°åÁöÑÂëΩ‰ª§)

        OR 

            grep -i ENTRYPOINT webapp-color/Dockerfile  

        ~ ‚úñ grep -i ENTRYPOINT webapp-color/Dockerfile
        ENTRYPOINT ["python", "app.py"]

        Answer: python app.py



    6)  What port is the web application run within the container?
        Inspect the Dockerfile in the webapp-color directory.

            grep -i EXPOSE webapp-color/Dockerfile

        ~ ‚ûú  grep -i EXPOSE webapp-color/Dockerfile
        EXPOSE 8080


        Answer: 8080

    7)  Build a docker image using the Dockerfile and name it webapp-color. No tag to be specified.
        Hint: Run the command docker build -t webapp-color 

            pwd
            cd webapp-color
            docker build -t webapp-color .
            (-t: ÊßãÂª∫ÁöÑÊò†ÂÉèÊåáÂÆöÂêçÁ®±ÂíåÊ®ôÁ±§ ; webapp-color: ÊßãÂª∫ÁöÑÊò†ÂÉèÁöÑÂêçÁ®±Ôºå‰æøÊñºÂæåÁ∫åÁÆ°ÁêÜÂíåÂºïÁî®)

        
        ~ ‚ûú  pwd
        /root

        ~ ‚ûú  docker build -t webapp-color . (You have to move to the specified container directory )

        ~ ‚úñ cd webapp-color               

        webapp-color on ÓÇ† docker-images via üêç ‚ûú  docker build -t webapp-color .
        DEPRECATED: The legacy builder is deprecated and will be removed in a future release.
                    BuildKit is currently disabled; enable it by removing the DOCKER_BUILDKIT=0
                    environment-variable.

        Sending build context to Docker daemon  65.02kB
        Step 1/6 : FROM python:3.6
        ---> 54260638d07c

        (Ignore)

        Step 6/6 : ENTRYPOINT ["python", "app.py"]
        ---> Using cache
        ---> 1dc6f5bc94e8
        Successfully built 1dc6f5bc94e8
        Successfully tagged webapp-color:latest

    8) Run an instance of the image webapp-color and publish port 8080 on the container to 8282 on the host.
        Container with image 'webapp-color'
        Container Port: 8080
        Host Port: 8282

            docker run --help
            docker run -p 8282:8080 webapp-color

        ~ ‚ûú  docker run -p 8282:8080 webapp-color 
        This is a sample web application that displays a colored background. 
        (...Ignore...)
        WARNING: This is a development server. Do not use it in a production deployment.
        * Running on http://172.12.0.2:8080/ (Press CTRL+C to quit)

    9)  What is the base Operating System used by the python:3.6 image?
        If required, run an instance of the image to figure it out.

            docker run python:3.6 cat /etc/*release*
        
        WRONG: (run the docker run python:3.6 & cat /etc/*release* seperately)

        ~ ‚ûú  docker run python:3.6 

        ~ ‚ûú  ls
        webapp-color

        ~ ‚ûú  cd webapp-color 
        webapp-color on ÓÇ† docker-images via üêç ‚úñ docker run python:3.6

        webapp-color on ÓÇ† docker-images via üêç ‚ûú  cat /etc/*release*         
        3.18.9
        NAME="Alpine Linux"
        ID=alpine
        VERSION_ID=3.18.9
        PRETTY_NAME="Alpine Linux v3.18"
        HOME_URL="https://alpinelinux.org/"
        BUG_REPORT_URL="https://gitlab.alpinelinux.org/alpine/aports/-/issues"

        CORRECT:

        webapp-color on ÓÇ† docker-images via üêç ‚ûú  docker run python:3.6 cat /etc/*release*
        cat: /etc/alpine-release: No such file or directory
        PRETTY_NAME="**Debian** GNU/Linux 11 (bullseye)"
        NAME="Debian GNU/Linux"
        VERSION_ID="11"
        VERSION="11 (bullseye)"
        VERSION_CODENAME=bullseye
        ID=debian
        HOME_URL="https://www.debian.org/"
        SUPPORT_URL="https://www.debian.org/support"
        BUG_REPORT_URL="https://bugs.debian.org/"

        Answer: Debian


    10)  The size of webapp-color image is 920 MB which is really BIG for a Docker Image. 
            
            Docker images are supposed to be small and light weight. Let us try to trim it down.
            
            webapp-color on ÓÇ† docker-images via üêç ‚úñ docker images                           
            REPOSITORY                      TAG           IMAGE ID       CREATED          SIZE
            webapp-color                    latest        ab486d9553af   14 minutes ago   913MB
            alpine                          latest        91ef0af61f39   3 months ago     7.79MB
            nginx                           alpine        c7b4f26a7d93   4 months ago     43.2MB
            nginx                           latest        39286ab8a5e1   4 months ago     188MB

            Build a new smaller docker image by modifying the same Dockerfile and name it webapp-color and tag it lite.
            Hint: Find a smaller base image for python:3.6. Make sure the final image is less than 150MB.

            Name: webapp-color:lite
            Image size less than 150MB

                vim Dockerfile 

            webapp-color on ÓÇ† docker-images via üêç ‚ûú  docker build -t webapp-color:lite (X you need to modify the Dockerfile first !!)     

            webapp-color on ÓÇ† docker-images via üêç ‚úñ ls -l (Check where's the Dockerfile)
            total 16
            -rw-r--r--    1 root     root           113 Dec 29 10:57 Dockerfile
            -rw-r--r--    1 root     root          2259 Dec 29 10:57 app.py
            -rw-r--r--    1 root     root             5 Dec 29 10:57 requirements.txt
            drwxr-xr-x    2 root     root          4096 Dec 29 10:57 templates

            webapp-color on ÓÇ† docker-images via üêç ‚ûú  vim Dockerfile 

            FROM python:3.6-alpine (Change from python:3.6 to python:3.6-alpine)

            RUN pip install flask

            COPY . /opt/

            EXPOSE 8080

            WORKDIR /opt

            ENTRYPOINT ["python", "app.py"]

            webapp-color on ÓÇ† docker-images [!] via üêç ‚ûú  docker build -t webapp-color:lite
            DEPRECATED: The legacy builder is deprecated and will be removed in a future release.
                        BuildKit is currently disabled; enable it by removing the DOCKER_BUILDKIT=0
                        environment-variable.

            "docker build" requires exactly 1 argument.
            See 'docker build --help'.

            Usage:  docker build [OPTIONS] PATH | URL | -

            Build an image from a Dockerfile

            webapp-color on ÓÇ† docker-images [!] via üêç ‚úñ docker images (Check the result)                
            REPOSITORY                      TAG           IMAGE ID       CREATED          SIZE
            webapp-color                    lite          daf613b80c93   4 minutes ago    51.9MB  (The size has been downsized)
            webapp-color                    latest        ab486d9553af   33 minutes ago   913MB

    11) Run an instance of the new image webapp-color:lite and publish port 8080 on the container to 8383 on the host.
            Container with image 'webapp-color:lite'
            Container to publish port 8080 to 8383

            webapp-color on ÓÇ† docker-images [!] via üêç ‚ûú  ddocker run -p 8080:8383 webapp-color:lite (-d: run in backgroun)
            docker: Error response from daemon: driver failed programming external connectivity on endpoint exciting_bose (0ebd8baaeeaa47238b96f4e4d1144f1ef1a4931ff280d269058242edaa24a368): Error starting userland proxy: listen tcp4 0.0.0.0:8080: bind: address already in use.
            ERRO[0000] error waiting for container: context canceled 

            (port 8080 is accupied)

            webapp-color on ÓÇ† docker-images [!] via üêç ‚úñ docker run -p 9090:8383 webapp-color:lite  (Change to another port)

            This is a sample web application that displays a colored background. 
            (...Ignore...)
            WARNING: This is a development server. Do not use it in a production deployment.
            * Running on http://172.12.0.2:8080/ (Press CTRL+C to quit)



2. A quick note on editing Pods and Deployments
    1)  Edit a POD
        you **CANNOT** edit specifications of an existing POD other than the below.

        spec.containers[*].image
        spec.initContainers[*].image
        spec.activeDeadlineSeconds
        spec.tolerations
        
        For example you **cannot edit** the **environment variables, service accounts, resource limits** (all of which we will discuss later) of a running pod. But if you really want to, you have 2 options:

         a. Run the 
                    
                kubectl edit pod <pod name> command.  
            This will open the pod specification in an editor (vi editor). Then edit the required properties. When you try to save it, you will be denied. This is because you are attempting to edit a field on the pod that is not editable.

            A copy of the file with your changes is saved in a temporary location as shown above.
            You can then delete the existing pod by running the command:

            kubectl delete pod webapp

            Then create a new pod with your changes using the temporary file
            
                kubectl create -f /tmp/kubectl-edit-ccvrq.yaml

        b.  The second option is to extract the pod definition in YAML format to a file using the command 
            
                kubectl get pod webapp -o yaml > my-new-pod.yaml

            Then make the changes to the exported file using an editor (vi editor). Save the changes

            vi my-new-pod.yaml

            Then delete the existing pod

                kubectl delete pod webapp

            Then create a new pod with the edited file

                kubectl create -f my-new-pod.yaml


    2)  Edit Deployments
        With Deployments you can easily edit any field/property of the POD template. Since **the pod template is a child of the deployment specification**,  with every change the deployment will automatically delete and create a new pod with the new changes. So if you are asked to edit a property of a POD part of a deployment you may do that simply by running the 
        
            command:

                kubectl edit deployment my-deployment

            Examples:
            a. What is the command used to run the pod ubuntu-sleeper?
            
                k edit pod ubuntu-sleeper

                    controlplane ~ ‚ûú  k get pods
            NAME             READY   STATUS    RESTARTS   AGE
            ubuntu-sleeper   1/1     Running   0          7s

            controlplane ~ ‚ûú  k edit pod ubuntu-sleeper 

            (...Ignore...)
            spec:
            containers:
            - command:
                - sleep
                - "4800"
            (...Ignore...)

            OR

                k describe pod ubuntu-sleeper
            
            (...Ignore...)
            Command:
                sleep
                4800
            (...Ignore...)

            Answer: *sleep* (X)  sleep 4800 (O) 
            ÈÄôË£°ÁöÑ command Ê¨Ñ‰ΩçÂÆöÁæ©ÁÇ∫‰∏ÄÂÄãÂàóË°®ÔºåÂåÖÂê´ÂÖ©ÂÄãÂÖÉÁ¥†Ôºö
            sleepÔºö‰∏ªÁ®ãÂ∫èÔºàENTRYPOINTÔºâË∑ü "4800"ÔºöÂèÉÊï∏ÔºàÈªòË™çÊÉÖÊ≥Å‰∏ãÊúÉ‰ΩúÁÇ∫Á¨¨‰∏ÄÂÄãÂëΩ‰ª§Ë°åÂèÉÊï∏ÂÇ≥ÈÅûÁµ¶ sleep ÂëΩ‰ª§Ôºâ„ÄÇ
            Âõ†Ê≠§ÔºåÊúÄÁµÇÁîüÊàêÁöÑÂÆåÊï¥ÂëΩ‰ª§ÊòØÔºösleep 4800


        b.  Create a pod with the ubuntu image to run a container to sleep for 5000 seconds. Modify the file ubuntu-sleeper-2.yaml.
        Note: Only make the necessary changes. Do not modify the name.
        
            Pod Name: ubuntu-sleeper-2
            Command: sleep 5000

                vim ubuntu-sleeper-2.yaml
                k create -f ubuntu-sleeper-2.yaml 

            controlplane ~ ‚úñ vim ubuntu-sleeper-2.yaml 

            controlplane ~ ‚ûú  k create -f ubuntu-sleeper-2.yaml 
            pod/ubuntu-sleeper-2 created

            apiVersion: v1 
            kind: Pod 
            metadata:
            name: ubuntu-sleeper-2 
            spec:
            containers:
            - name: ubuntu
                image: ubuntu
                command:  (add command column)
                - "sleep" 
                - "5000"
            (OR:
            spec:
            containers:
            - name: ubuntu
                image: ubuntu
                command: ["sleep","5000"] )

            controlplane ~ ‚ûú  k get pods
            NAME               READY   STATUS    RESTARTS   AGE
            ubuntu-sleeper     1/1     Running   0          20m
            ubuntu-sleeper-2   1/1     Running   0          107s

            c. Create a pod using the file named ubuntu-sleeper-3.yaml. There is something wrong with it. Try to fix it!
            Note: Only make the necessary changes. Do not modify the name
            
            Pod Name: ubuntu-sleeper-3
            Command: sleep 1200

            controlplane ~ ‚ûú  k create -f ubuntu-sleeper-3.yaml  (run first to check the error)
            Error from server (BadRequest): error when creating "ubuntu-sleeper-3.yaml": Pod in version "v1" cannot be handled as a Pod: json: cannot unmarshal number into Go struct field Container.spec.containers.command of type string

            controlplane ~ ‚úñ vim ubuntu-sleeper-3.yaml 
            apiVersion: v1
            kind: Pod 
            metadata:
            name: ubuntu-sleeper-3
            spec:
            containers:
            - name: ubuntu
                image: ubuntu
                command:
                - "sleep"
                - 1200 -> The command in command column should be String formatL "1200"
            ~              

            controlplane ~ ‚ûú  k create -f ubuntu-sleeper-3.yaml 
            pod/ubuntu-sleeper-3 created

        d. Inspect the file Dockerfile given at /root/webapp-color directory. What command is **run at container startup**?
        
            controlplane ~ ‚ûú  cat webapp-color/Dockerfile
            FROM python:3.6-alpine

            RUN pip install flask  (**RUN: executed at build time**. It is used to build the Docker image and is not part of the container's runtime behavior.)

            COPY . /opt/

            EXPOSE 8080

            WORKDIR /opt

            ENTRYPOINT ["python", "app.py"]

            Answer: python app.py

        e. Inspect the file Dockerfile2 given at /root/webapp-color directory. What command is run at container startup?
            option1: python app.py
            option2: python app.py --color red

            controlplane ~ ‚ûú  cat webapp-color/Dockerfile2
            FROM python:3.6-alpine

            RUN pip install flask

            COPY . /opt/

            EXPOSE 8080

            WORKDIR /opt

            ENTRYPOINT ["python", "app.py"] (ENTRYPOINT: Specifies the base command that **will always be executed when the container starts**.)

            CMD ["--color", "red"] (CMD: Provides default arguments or options to the command specified in ENTRYPOIN)

            Answer: python app.py --color red

        f. Inspect the two files under directory webapp-color-2. What command is run at container startup?
        Assume the image was created from the Dockerfile in this directory.

            option1: --color green
            option2: python app.py --color red

            Hint: The ENTRYPOINT in the Dockerfile is overridden by the command in the pod definition file.

                vim webapp-color-2/Dockerfile
                vim webapp-color-2/webapp-color-pod.yaml 

            controlplane ~ ‚ûú  vim webapp-color-2/
            Dockerfile             webapp-color-pod.yaml  

            controlplane ~ ‚ûú  cat webapp-color-2/Dockerfile

            FROM python:3.6-alpine

            RUN pip install flask

            COPY . /opt/

            EXPOSE 8080

            WORKDIR /opt

            ENTRYPOINT ["python", "app.py"]

            CMD ["--color", "red"]

            controlplane ~ ‚ûú  cat webapp-color-2/webapp-color-pod.yaml 
            apiVersion: v1
            kind: Pod 
            metadata:
            name: webapp-green
            labels:
                name: webapp-green
            spec:
            containers:
            - name: simple-webapp
                image: kodekloud/webapp-color
                command: ["--color","green"]    (Defult pod command)


            Solution: Since the entrypoint is overridden in the pod definition, the command that will be run is just --color green.

            Answer: --color green
        

        g. Inspect the two files under directory webapp-color-3. What command is run at container startup?
        Assume the image was created from the Dockerfile in this directory.
        
            option1: python app.py --color pink
            option2: --color pink
            option3: python app.py

                cat webapp-color-3/Dockerfile 
                cat webapp-color-3/webapp-color-pod-2.yaml

            controlplane ~ ‚ûú  cat webapp-color-3/Dockerfile 
            FROM python:3.6-alpine

            RUN pip install flask

            COPY . /opt/

            EXPOSE 8080

            WORKDIR /opt

            ENTRYPOINT ["python", "app.py"] (won't overried by defult pod command definition)

            CMD ["--color", "red"]

            controlplane ~ ‚ûú  cat webapp-color-3/webapp-color-pod-2.yaml 
            apiVersion: v1 
            kind: Pod 
            metadata:
            name: webapp-green
            labels:
                name: webapp-green 
            spec:
            containers:
            - name: simple-webapp
                image: kodekloud/webapp-color
                command: ["python", "app.py"] (Defult command & arguments)
                args: ["--color", "pink"]

            Answer: python app.py --color pink

        h. Create a pod with the given specifications. By default it displays a blue background. Set the given command line arguments to change it to green.

            Pod Name: webapp-green
            Image: kodekloud/webapp-color
            Command line arguments: --color=green
            controlplane ~ ‚ûú  vim webapp-green.yaml

            controlplane ~ ‚ûú  k create -f webapp-green.yaml 
            pod/webapp-green created

            controlplane ~ ‚úñ k get pods
            NAME               READY   STATUS    RESTARTS   AGE
            ubuntu-sleeper     1/1     Running   0          48m
            ubuntu-sleeper-2   1/1     Running   0          30m
            ubuntu-sleeper-3   1/1     Running   0          21m
            webapp-green       1/1     Running   0          5s


            # Start the nginx pod using the default command, but use custom arguments (arg1 .. argN) for that command
                
                kubectl run nginx --image=nginx -- <arg1> <arg2> ... <argN>  (We choose this approach)
        
            # Start the nginx pod using a different command and custom arguments
                
                kubectl run nginx --image=nginx --command -- <cmd> <arg1> ... <argN>

            controlplane ~ ‚ûú  kubectl run webapp-green --image=kodekloud/webapp-color -- --color=green
            pod/webapp-green created

            controlplane ~ ‚ûú  k get pods
            NAME               READY   STATUS    RESTARTS   AGE
            ubuntu-sleeper     1/1     Running   0          20m
            ubuntu-sleeper-2   1/1     Running   0          16m
            ubuntu-sleeper-3   1/1     Running   0          13m
            webapp-green       1/1     Running   0          2s


3. Environment Variables & ConfigMaps   
    Why use ConfigMap: simplify the declaration file. 
    Imperative way (ÂëΩ‰ª§ÂºèÊñπÊ≥ï): without using the ConfigMap
        
        k create configmap
            <config-name> --from-literal=<key>=<value>    (You can directly declare your key-value in configmap)

        k create configmap \
            app-config --from-literal=APP_COLOR=blue \
                       --from-literal=APP_MOD=prod

        k create configmap
            <config-name> --from-file=<path-to-the-file>

        k create configmap \
            app-config --from -file=app_config.properties

        k create config -h (to see more options)

    Declarative way (ÂÆ£ÂëäÂºèÊñπÊ≥ï): using the ConfigMap (Create ConfigMap then inject the map in delaration file)

        k create -f

    Example: Create 2 files: ConfigMap & config-map.yaml
        
        ConfigMap:
            APP_COLOR: blue
            APP_MODE: prod
        
        config-map.yaml:
            apiVersion: v1
            kind: ConfigMap
            metadata:
                name: app-config
            data:
                APP_COLOR: blue
                APP_MODE: prod
            
        k create -f config-map.yaml
    
    There are several enviornment configuraion in the ConfigMap column. (eg. ENV: envFrom, SINGLE ENV: env and VOLUMN: volumes)
    See more: https://kubernetes.io/docs/concepts/configuration/configmap/

    Practice:

    1) What is the environment variable name set on the container in the pod?

        controlplane ~ ‚ûú  k get pods
        NAME           READY   STATUS    RESTARTS   AGE
        webapp-color   1/1     Running   0          6s 

            k describe pod webapp-color

        controlplane ~ ‚ûú  k describe pod webapp-color
        ...
        (Find the column: Enviornment)

        OR 

            k describe pod webapp-color | grep -A1 "Environment:"
            (-A1 Ë°®Á§∫ÊâæÂà∞‰∏ã‰∏ÄË°å)
        controlplane ~ ‚ûú  k describe pod webapp-color |grep Environment
        Environment:

        controlplane ~ ‚ûú  k describe pod webapp-color | grep -A1 "Environment:" 
            Environment:
            APP_COLOR:  pink
        
        Answer: APP_COLOR

        What is the value set on the environment variable APP_COLOR on the container in the pod?

        Carry On, Answer: pink
    
    2) Update the environment variable on the POD to display a green background. (Open the UI of the botton of "webapp", it displays "purple" on the background)

        Note: Delete and recreate the POD. Only make the necessary changes. Do not modify the name of the Pod.

        Pod Name: webapp-color
        Label Name: webapp-color
        Env: APP_COLOR=green
    
        WRONG STEPS: Directly delete the pods and there's no definition file of this pod.

        controlplane ~ ‚ûú  k get pods
        NAME           READY   STATUS    RESTARTS   AGE
        webapp-color   1/1     Running   0          10m

        controlplane ~ ‚ûú  k delete pod webapp-color 
        pod "webapp-color" deleted

        controlplane ~ ‚ûú  ls

        Since there's no definition file of this pod, we only can use:

            k edit pod webapp-color
        
        Then try to change the env.name column:
        controlplane ~ ‚ûú  k edit pod webapp-color 
        It says:

        # Please edit the object below. Lines beginning with a '#' will be ignored,
    
        # and an empty file will abort the edit. If an error occurs while saving this file will be
        # reopened with the relevant failures.
        #
        # pods "webapp-color" was not valid:
        # * spec: Forbidden: pod updates may not change fields other than spec.containers[*].image,spec.initContainers[*].image,spec.activeDeadlineSeconds,spec.tolerations (only additions to existing tolerations),spec.terminationGracePeriodSeconds (allow it to be set to 1 if it was previously negative)

        error: pods "webapp-color" is invalid
        A copy of your changes has been stored to "/tmp/kubectl-edit-803466170.yaml"
        error: Edit cancelled, no valid changes were saved.

    **WARNING!!!**:
        Since Pods are immutable resources, which means you cannot directly modify most properties of a Pod (including environment variables). The following steps can be taken to change the environment variables of a Pod:
        Export the Pod's YAML file and edit it, then delete the existing pod and recreate it

            k get pod webapp-color -o yaml > webapp-color.yaml

        controlplane ~ ‚úñ kubectl get pod webapp-color -o yaml > webapp-color.yaml (Export the YAML file)

        controlplane ~ ‚ûú  vim webapp-color.yaml  (Change the env column)

        controlplane ~ ‚ûú  k delete pod webapp-color  (Delete the current existing pod)
        pod "webapp-color" deleted

        controlplane ~ ‚ûú  k create pod -f webapp-color.yaml  (Recreate the pod)
        error: Unexpected args: [pod]
        See 'kubectl create -h' for help and examples

        controlplane ~ ‚úñ k create -f webapp-color.yaml 
        pod/webapp-color created

        controlplane ~ ‚ûú  k get pods
        NAME           READY   STATUS    RESTARTS   AGE
        webapp-color   1/1     Running   0          6s

        (View the changes to the web application UI by clicking on the Webapp Color Tab above your terminal.
        If you already have it open, simply refresh the browser.  --This displays green on  the background)

    3) How many ConfigMaps exists in the default namespace?

            k get cm
            k describe db-config
        
        controlplane ~ ‚ûú  k get configmap
        NAME               DATA   AGE
        db-config          3      18s
        kube-root-ca.crt   1      41m

        controlplane ~ ‚ûú  k get cm
        NAME               DATA   AGE
        db-config          3      86s
        kube-root-ca.crt   1      43m

        Identify the database host from the config map db-config

        option1: SQL01.example.com
        option2: SQL01

        controlplane ~ ‚ûú  k describe cm db-config 

        Name:         db-config
        Namespace:    default
        Labels:       <none>
        Annotations:  <none>

        Data
        ====
        DB_PORT:
        ----
        3306

        DB_HOST:
        ----
        SQL01.example.com

        DB_NAME:
        ----
        SQL01


        BinaryData
        ====

        Events:  <none>

        Answer: SQL01.example.com (option1)

    4) Create a new ConfigMap for the webapp-color POD. Use the spec given below.

        ConfigMap Name: webapp-config-map
        Data: APP_COLOR=darkblue
        Data: APP_OTHER=disregard

        2 ways: (Imperative vs. Declarative)

        a. Imperative Way:
                
                k get cm
                k create cm <cm-name> --from-literal=<key>=<value> (data1) --from -literal=<key>=<value> (data2)

            controlplane ~ ‚ûú  k get cm
            NAME               DATA   AGE
            db-config          3      5m57s
            kube-root-ca.crt   1      47m

            controlplane ~ ‚úñ kubectl create configmap  webapp-config-map --from-literal=APP_COLOR=darkblue --from-literal=APP_OTHER=disregard
            configmap/webapp-config-map created

        b. Declarative Way:

            controlplane ~ ‚ûú  k get cm db-config -o yaml > webapp-config-map.yaml
            controlplane ~ ‚ûú  vim webapp-config-map.yaml

            apiVersion: v1
            data:
            #DB_HOST: SQL01.example.com (Delete)
            #DB_NAME: SQL01 (Delete)
            #DB_PORT: "3306" (Delete)
            APP_COLOR: darkblue (Add)
            APP_OTHER: disregard (Add)
            kind: ConfigMap
            metadata:
            creationTimestamp: "2025-01-05T04:54:57Z"
            name: webapp-config-map (Add) #db-config 
            namespace: default
            resourceVersion: "1469"
            uid: e13c1f81-bc49-4fe6-9d6b-b5fe8ef4d4a3

            k apply -f webapp-config-map.yaml

            controlplane ~ ‚ûú  k apply -f new-db-config.yaml 
            Warning: resource configmaps/webapp-config-map is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
            (Â∑≤Â≠òÂú®‰∏ÄÂÄãConfigMap:

            i. Êõ¥ÊîπÂéüÊú¨ÁöÑwebapp-config-map.yaml ‰πãÂæå‰ΩøÁî® k apply -f  webapp-config-map.yaml
            ii. Âº∑Âà∂Ë≥áÊ∫êÊõ¥Êñ∞: k replace -f webapp-config-map.yaml

            (ÊàëÂÄë‰ΩøÁî®replace)

    5) Update the environment variable on the POD to use only the APP_COLOR key from the newly created ConfigMap.
       Note: Delete and recreate the POD. Only make the necessary changes. Do not modify the name of the Pod.    
       
        Hint: Set the environment option of env from value to valueFrom and use the configMapKeyRef to further specify the specs such as name and key for webapp-config-map configmap.

        Pod Name: webapp-color
        ConfigMap Name: webapp-config-map  

            k get cm
            k delete cm webapp-config-map
            vim webapp-color-config.yaml 
            k apply -f webapp-color-config.yaml 
            k get cm

        **WRONG APROACH:** 
        controlplane ~ ‚ûú  k get cm
        NAME                DATA   AGE
        db-config           3      5m33s
        kube-root-ca.crt    1      19m
        webapp-config-map   2      4s

        controlplane ~ ‚ûú  k delete cm webapp-config-map 
        configmap "webapp-config-map" deleted

        controlplane ~ ‚ûú  vim webapp-color-config.yaml 
        **This practice aim to infect the ConfigMap into Pod definition file, Using the COLOR property from ConfigMap.**
        apiVersion: v1
        data:
        APP_COLOR: darkblue
        #APP_OTHER: disregard (X Âà™Èô§Ê≠§Ë°å) **WRONG!!**
        kind: ConfigMap
        metadata:
        creationTimestamp: "2025-01-05T05:42:43Z"
        name: webapp-config-map
        namespace: default
        resourceVersion: "1019"
        uid: 8867b6fe-4982-4956-a6a7-54d374f64bda

        controlplane ~ ‚ûú  k get cm 
        NAME                DATA   AGE
        db-config           3      8m45s
        kube-root-ca.crt    1      23m
        webapp-config-map   2      3m16s  

        controlplane ~ ‚ûú  k apply -f webapp-color-config.yaml 
        configmap/webapp-config-map created

        controlplane ~ ‚ûú  k get cm
        NAME                DATA   AGE
        db-config           3      9m13s
        kube-root-ca.crt    1      23m
        webapp-config-map   1      3s


        **CORRECT„ÄÄAPROACH:**


        Check the configMap yaml file: webapp-color-config.yaml

            vim webapp-color-config.yaml

            apiVersion: v1
            data:
            APP_COLOR: darkblue
            #APP_OTHER: disregard
            kind: ConfigMap
            metadata:
            creationTimestamp: "2025-01-05T05:42:43Z"
            name: webapp-config-map (Check the name)
            namespace: default
            resourceVersion: "1019"
            uid: 8867b6fe-4982-4956-a6a7-54d374f64bda
        
        Edit the pod definition file: webapp-color.yaml

            vim webapp-color.yaml
        
        controlplane ~ ‚ûú  vim webapp-color.yaml 
        spec:
        containers:
        - env:
            - name: APP_COLOR
            #value: green (Delete this lineÔºå‰Ωøcolor ÈÄôÂÄãÂ±¨ÊÄßÁî®ConfigMap ÈÄèÈÅévalueFrom Ë£°ÁöÑconfigMaoKeyRef‰æÜÂÆöÁæ©)
            valueFrom: (Add valueFrom property) (Define the color property should refer from the ConfigMap)
                configMapKeyRef:
                name: webapp-config-map
                key: APP_COLOR
        
        :wq

        controlplane ~ ‚ûú  k delete pod webapp-color 
        pod "webapp-color" deleted

        controlplane ~ ‚ûú  k apply -f webapp-color.yaml 
        pod/webapp-color created

        controlplane ~ ‚ûú  k get pods
        NAME           READY   STATUS    RESTARTS   AGE
        webapp-color   1/1     Running   0          2s

        View the changes to the web application UI by clicking on the Webapp Color Tab above your terminal.
        If you already have it open, simply refresh the browser. (-- This displays darkblue on the background)

4. Secrets
    - There're also 2 ways (Imperative & Declarative) to create Secrers.
    - Secrets are not Encrypted but only Encoded in base64 format.
      - Anyone with the base64 encoded secret can easily decode it
      - Do not check-in Secret objects to SCM (eg. Github) along with code

            controlplane ~ ‚úñ echo "c3VwZXJzZWNyZXQ=" |base64 --decode
            base64: unrecognized option: decode
            BusyBox v1.35.0 (2022-08-01 15:14:44 UTC) multi-call binary.

            Usage: base64 [-d] [-w COL] [FILE]

            Base64 encode or decode FILE to standard output

                    -d      Decode data
                    -w COL  Wrap lines at COL (default 76, 0 disables)

            controlplane ~ ‚úñ echo "c3VwZXJzZWNyZXQ=" |base64 -d
            supersecret

    - Secrets are not encrypted in ETCD
      - Enable encryption at rest. 
    - Anyone able to crete pod/deployments in the same namespace can acess the secrets. 
      - Configure least-privilege access to Secrets - RBAC.
    - Consider thrid-party secrets store providers.
      - eg. AWA, Azure, GCP and Valt...

    Check how to make the practice of using Secrets to be safer:
    https://kubernetes.io/docs/concepts/configuration/secret/

    Having said that, there are other better ways of handling sensitive data like passwords in Kubernetes, such as using tools like Helm Secrets, HashiCorp Vault.

    1) How many secrets are defined in the dashboard-token secret?
       Hint: kubectl describe secrets dashboard-token and look at the data field.

        controlplane ~ ‚ûú  k get secrets
        NAME              TYPE                                  DATA   AGE
        dashboard-token   kubernetes.io/service-account-token   3      19s

        controlplane ~ ‚ûú  k describe secrets dashboard-token 
        Name:         dashboard-token
        Namespace:    default
        Labels:       <none>
        Annotations:  kubernetes.io/service-account.name: dashboard-sa
                    kubernetes.io/service-account.uid: c20548ab-83ee-43db-97fb-35fb24ec2ddb

        Type:  kubernetes.io/service-account-token

        Data
        ====
        ca.crt:     570 bytes
        namespace:  7 bytes
        token:      eyJhbGciOiJSUzI1NiIsImtpZCI6InhKRFl0SS1ETTA2QWRHaUFTRDBRd0ktZWJWSXF0b1FTMmltdHZlWVp4SnMifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRhc2hib2FyZC10b2tlbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJkYXNoYm9hcmQtc2EiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJjMjA1NDhhYi04M2VlLTQzZGItOTdmYi0zNWZiMjRlYzJkZGIiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6ZGVmYXVsdDpkYXNoYm9hcmQtc2EifQ.MY8fSJQXeQ3V_g0_kTohAxBHYgea_I678p9FLCOuY2uU5JIDBuU2bmJpE_hxJhYjjuYw-Eu-_8M0EPorP3_VdqvvhZ0xatQCJH4mZzT2smn3-cavJx8ZPVuYX_ZCI-4Nvk_vAX-3xX2onPTnwmHe4DuxLmzmGC54zVLuylnyKbu3gLUO5OeZ_LjXXvifxPbrTm5NcLqwes9pfpgCV5I1BEwEGV47sLG-JKDrUGTuws4JlV2eZ2KYiBQnVFFhYUKQDxj6NRqdRoakBmxdNXDT-hAlp15nwd_pREXqg9rUpbPru12BbfbSy-j1yFu36yNrcogLeph1ZoN7_k3pVBzNUQ

        Answer: 3 Secrers (ca.crt, namspace and token)

        What is the type of the dashboard-token secret?
        Carry on: Type:  kubernetes.io/service-account-token

        Answer: kubernetes.io/service-account-token

    2) We are going to deploy an application with the below architecture
        We have already deployed the required pods and services. Check out the pods and services created. Check out the web application using the Webapp MySQL link above your terminal, next to the Quiz Portal Link.
        ![Archetacture](images/Secrets01.png "Secrets Image")

        The reason the application is failed is because we have not created the secrets yet. Create a new secret named db-secret with the data given below.
        ![Current Display](images/Secrets02.png "Secrets Image")

        Secret Name: db-secret
        Secret 1: DB_Host=sql01
        Secret 2: DB_User=root
        Secret 3: DB_Password=password123

        Imperative way: (Recommanded, more simple)
            controlplane ~ ‚ûú  k create secret generic -h
            ...

            Examples:
            ...
            # Create a new secret named my-secret with key1=supersecret and key2=topsecret
            kubectl create secret generic my-secret --from-literal=key1=supersecret
            --from-literal=key2=topsecret  (pick this one)
            
            Options:
                ...
                --from-literal=[]:
                    Specify a key and literal value to insert in secret (i.e.
                    mykey=somevalue)
                    
                    (pick this one)

            controlplane ~ ‚ûú  k create secret generic db-secret --from-literal=DB_Host=sql01 --from-literal=DB_User=root --from-literal=DB_Password=password123
            secret/db-secret created

            controlplane ~ ‚ûú  k get secrets 
            NAME              TYPE                                  DATA   AGE
            dashboard-token   kubernetes.io/service-account-token   3      25m
            db-secret         Opaque                                3      8s
                

        Declarative way:
            
            vim db-secret.yaml
            
            apiVersion: v1
            kind: Secret
            metadata:
                name: db-secret
                type: Opaque
            data:
                DB_Host: c3FsMDE=
                DB_User: cm9vdA==
                DB_Password: cGFzc3dvcmQxMjM=


            controlplane ~ ‚ûú  k delete secrets db-secret 
            secret "db-secret" deleted

            controlplane ~ ‚ûú  k apply -f db-secret.yaml 
            Error from server (BadRequest): error when creating "db-secret.yaml": Secret in version "v1" cannot be handled as a Secret: illegal base64 data at input byte 4

        **WARNING:** Since Secret's data field requires that the value be stored in Base64 encoded form. 
        Use echo -n to encode:

            controlplane ~ ‚úñ echo -n 'sql01' | base64 
            c3FsMDE=

            controlplane ~ ‚ûú  echo -n 'root' | base64
            cm9vdA==

            controlplane ~ ‚ûú  echo -n 'password123' | base64
            cGFzc3dvcmQxMjM=

            Replace the above encoded fields back into the file:
            
            apiVersion: v1
            kind: Secret
            metadata:
                name: db-secret
            type: Opaque
            data:
                DB_Host: c3FsMDE=
                DB_User: cm9vdA==
                DB_Password: cGFzc3dvcmQxMjM=

            controlplane ~ ‚ûú  k apply -f db-secret.yaml 
            secret/db-secret created

            controlplane ~ ‚ûú  k get secrets 
            NAME              TYPE                                  DATA   AGE
            dashboard-token   kubernetes.io/service-account-token   3      40m
            db-secret         Opaque      

    3) Configure webapp-pod to load environment variables from the newly created secret.
        Delete and recreate the pod if required.

        Pod name: webapp-pod
        Image name: kodekloud/simple-webapp-mysql
        Env From: Secret=db-secret

            k get pod webapp-pod -o yaml > webap-pod.yaml

        controlplane ~ ‚ûú  k get pod webapp-pod -o yaml > webap-pod.yaml

            spec:
            containers:
            - image: kodekloud/simple-webapp-mysql
                imagePullPolicy: Always
                name: webapp
                envFrom:
                - secretRef:
                    name: db-secret


        controlplane ~ ‚ûú  k delete pod webapp-pod 
        pod "webapp-pod" deleted

        controlplane ~ ‚ûú  k apply -f webapp-pod.yaml 
        pod/webapp-pod created

        controlplane ~ ‚úñ k get pods
        NAME         READY   STATUS    RESTARTS   AGE
        mysql        1/1     Running   0          48m
        webapp-pod   1/1     Running   0          2m4s

        View the web application to verify it can successfully connect to the database
        (Connect to the db sucessfully, Shown below:)
        ![Sucess Display](images/Secrets03.png "Secrets Image")










(See more Supplementary Content on Docker Security)
   
5. Security Contexts
    1) What is the user used to execute the sleep process within the ubuntu-sleeper pod?
        In the current(default) namespace.

        Hint: Run the command: 
            
            kubectl exec ubuntu-sleeper -- whoami
        and check the user that is running the container.

        controlplane ~ ‚úñ k exec ubuntu-sleeper -- whoami
        root        

    2) Edit the pod ubuntu-sleeper to run the sleep process with user ID 1010.
        Pod Name: ubuntu-sleeper
        Image Name: ubuntu
        SecurityContext: User 1010

        Note: Only make the necessary changes. Do not modify the name or image of the pod.

            k get pods
            k edit pod pod ubuntu-sleeper

        controlplane ~ ‚ûú  k get pods
        NAME             READY   STATUS    RESTARTS   AGE
        ubuntu-sleeper   1/1     Running   0          47m

        controlplane ~ ‚ûú  k edit pod ubuntu-sleeper (You cannot directly edit the pod when it's running)
        error: pods "ubuntu-sleeper" is invalid
        A copy of your changes has been stored to "/tmp/kubectl-edit-2275419055.yaml"
        error: Edit cancelled, no valid changes were saved.

            k get pod ubuntu-sleeper -o YAML > ubuntu-sleeper.yaml (save the pod yaml file first)
            k delete pod ubuntu-sleeper (--force) 
            vim ubuntu-sleeper.yaml
            (--force: delete the pod faster)
            k apply -f ubuntu-sleeper.yaml
            k get pods

        controlplane ~ ‚ûú  k get pod ubuntu-sleeper -o YAML > ubuntu-sleeper.yaml

        controlplane ~ ‚ûú  k delete pod ubuntu-sleeper --force
        pod "ubuntu-sleeper" deleted

        controlplane ~ ‚ûú  vim ubuntu-sleeper.yaml 
            kind: Pod
            metadata:
            creationTimestamp: "2025-01-06T05:35:52Z"
            **name: ubuntu-sleeper** (checked)
            namespace: default
            resourceVersion: "777"
            uid: 2d8cbe9a-05ba-4066-ae7f-6e93c462334a
            spec:
            containers:
            - command:
                - sleep
                - "4800"
                **image: ubuntu** (checked)
                imagePullPolicy: Always
                name: ubuntu
                resources: {}
                terminationMessagePath: /dev/termination-log
                terminationMessagePolicy: File
                volumeMounts:
                - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
                name: kube-api-access-5q9nk
                readOnly: true
            dnsPolicy: ClusterFirst
            enableServiceLinks: true
            nodeName: controlplane
            - effect: NoExecute
                key: node.kubernetes.io/unreachable
                imagePullPolicy: Always
                name: ubuntu
                resources: {}
                terminationMessagePath: /dev/termination-log
                terminationMessagePolicy: File
                volumeMounts:
                - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
                name: kube-api-access-5q9nk
                readOnly: true
            dnsPolicy: ClusterFirst
            enableServiceLinks: true
            nodeName: controlplane
            preemptionPolicy: PreemptLowerPriority
            priority: 0
            restartPolicy: Always
            schedulerName: default-scheduler
            **securityContext:** #{}
                **runAsUser: 1010** (Add this column)
            serviceAccount: default
            terminationGracePeriodSeconds: 30
            tolerations:
            ...

        controlplane ~ ‚ûú  k apply -f ubuntu-sleeper.yaml 
        pod/ubuntu-sleeper created

        controlplane ~ ‚ûú  k get pods
        NAME             READY   STATUS    RESTARTS   AGE
        ubuntu-sleeper   1/1     Running   0          4s

    3) A Pod definition file named multi-pod.yaml is given. With what user are the processes in the web container started?
        The pod is created with multiple containers and security contexts defined at the Pod and Container level.

        Hint: The User ID defined in the securityContext of the container overrides the User ID in the POD.

        option1: 1001
        option2: 1002

        check th yaml file:

            vim multi-pod.yaml

        apiVersion: v1
        kind: Pod
        metadata:
        name: multi-pod
        spec:
        securityContext:
            **runAsUser**: 1001 # By default, all containers will run under user ID 1001 unless an individual container has a special securityContext override
        containers:
        -  image: ubuntu
            name: web
            command: ["sleep", "5000"]
            securityContext:
            **runAsUser**: 1002 # In the web container, you define your own securityContext and set runAsUser: 1002, which overrides the default settings at the Pod level.

        -  image: ubuntu
            name: sidecar
            command: ["sleep", "5000"]


        Answer: option2:1002

        With what user are the processes in the sidecar container started?

        The pod is created with multiple containers and security contexts defined at the Pod and Container level.

        Carry On, the sidecar container does not specify a securityContext, so it will inherit the Pod level runAsUser: 1001

        Answer: 1001

    4) Update pod ubuntu-sleeper to run as Root user and with the SYS_TIME capability.
        Note: Only make the necessary changes. Do not modify the name of the pod.

        - Pod Name: ubuntu-sleeper
        - Image Name: ubuntu
        - SecurityContext: Capability SYS_TIME
        - Is run as a root user?

                k get pod ubuntu-sleeper -o YAML > ubuntu-sleeper.yaml
                k delete pod ubuntu-sleeper (--force) 
                vim ubuntu-sleeper.yaml
                k apply -f ubuntu-sleeper.yaml
                k get pods
        
        See settings for Secrety Context: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/

        controlplane ~ ‚ûú  vim ubuntu-sleeper.yaml
            apiVersion: v1
            kind: Pod
            metadata:
            creationTimestamp: "2025-01-06T06:39:44Z"
            name: ubuntu-sleeper
            namespace: default
            resourceVersion: "749"
            uid: 927aa6ed-4013-4091-a3ab-cfe42ad46b20
            spec:
            containers:
            - command:
                - sleep
                - "4800"
                image: ubuntu
                name: ubuntu
                securityContext:
                # runAsUser: 0 (UID=0=root)  by default, it run as root 
                **capabilities:**
                    **add: ["SYS_TIME"]**

        Verify:
            controlplane ~ ‚ûú  k exec ubuntu-sleeper -- whoami
            root

        Now update the pod to also make use of the NET_ADMIN capability.
        Note: Only make the necessary changes. Do not modify the name of the pod.

        Pod Name: ubuntu-sleeper
        Image Name: ubuntu
        SecurityContext: Capability SYS_TIME
        SecurityContext: Capability NET_ADMIN

        controlplane ~ ‚úñ k delete pod ubuntu-sleeper --force

        Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.
        pod "ubuntu-sleeper" force deleted

        controlplane ~ ‚úñ vim ubuntu-sleeper.yaml 
        spec:
        containers:
        - command:
            - sleep
            - "4800"
            image: ubuntu
            name: ubuntu
            securityContext:
            runAsUser: 0
            capabilities:
                add: ["NET_ADMIN","SYS_TIME"]  (Add: NET_ADMIN; See more settins in the document)

        controlplane ~ ‚ûú  k apply -f  ubuntu-sleeper.yaml 
        pod/ubuntu-sleeper created

6. Service Account
    (In CKAD scope, Security Accont is under Configuration field and examnee only need to understand Security Acount. While in CKA scope, examnee need to understand the configure, primitives of it. )

    User Account: used by human (eg. Admin, Developer)
    Server Account: used by machine (eg. Prometheous, Jenkins)

    If one developer develop a service to query the kubernetes api, the service has to be authenticated. To create a service account:

        k create serviceaccount dashboard-sa
    
    When the service account has been created, it generates a token automatically. The token is stored as a object in Secret.

    The Secret object is then liked to the service account. To see the details of the token:
        
        k describe secret dashboard-sa-token-kbbdm
    
    If we put token inside the pod, we can easielly use the token to query the kubernetes api in the cluster, without provide it manullay.

    Eg. Each namespace has its own default service account. Whenever a pod is created, the default service account and its token are automatically mounted to that pod as a volumn mount.

    Location the token mount inside the pod:

        /var/run/secrets/kubernetes.io/serviceaccount/
        k exec -it my-kubernetes-dashboard -- ls /var/run/secrets/kubernetes.io/serviceaccount/
        ca.crt namespace token
        (the secret mounted as 3 seperate files. token files store the actual token)

    1)  What is the secret token used by the default service account?
        Hint: Run the command kubectl describe serviceaccount default and look at the Tokens field.

        option1: none
        option2: default-token-86x8w

            k describe sa default

        k get serviceaccount/sa
        controlplane ~ ‚ûú  k get sa
        NAME      SECRETS   AGE
        default   0         4m56s
        dev       0         54s

        controlplane ~ ‚ûú  k describe sa default 
        Name:                default
        Namespace:           default
        Labels:              <none>
        Annotations:         <none>
        Image pull secrets:  <none>
        Mountable secrets:   <none>
        Tokens:              <none>
        Events:              <none>

        Answer: option1 none
    
    2) We just deployed the Dashboard application. Inspect the deployment. What is the image used by the deployment?
        
        option1: kodekloud/my-kubernetes-dashboard
        option2: gcr.io/kodekloud/customimage/my-kubernetes-dashboard


        controlplane ~ ‚ûú  k describe deploy web-dashboard 

        Name:                   web-dashboard
        Namespace:              default
        CreationTimestamp:      Mon, 06 Jan 2025 07:58:50 +0000
        Labels:                 <none>
        Annotations:            deployment.kubernetes.io/revision: 1
        Selector:               name=web-dashboard
        Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
        StrategyType:           RollingUpdate
        MinReadySeconds:        0
        RollingUpdateStrategy:  25% max unavailable, 25% max surge
        Pod Template:
        Labels:  name=web-dashboard
        Containers:
        web-dashboard:
            Image:      gcr.io/kodekloud/customimage/my-kubernetes-dashboard
            Port:       8080/TCP
            Host Port:  0/TCP
        ... 

        Answer: option2 gcr.io/kodekloud/customimage/my-kubernetes-dashboard

    3)  Wait for the deployment to be ready. Access the custom-dashboard by clicking on the link to dashboard portal.
        ![Display](images/Service_Account.png "Secrvice Account")

        What is the state of the dashboard? Have the pod details loaded successfully?
        Answer: Failed

        What type of account does the Dashboard application use to query the Kubernetes API?
        Answer: Service Account

        Which account does the Dashboard application use to query the Kubernetes API?
        By the error message: "pods is forbidden: User "system:serviceaccount:default:default" cannot list resource "pods" in API group "" in the namespace "default""
        (Dashboard ÊáâÁî®Á®ãÂ∫è‰ΩøÁî®‰∫Ü default ÂëΩÂêçÁ©∫Èñì‰∏≠ÁöÑ default ÊúçÂãôÂ∏≥Êà∂Ôºàsystem:serviceaccount:default:defaultÔºâ,Ë©≤ÊúçÂãôÂ∏≥Êà∂Áº∫Â∞ëÂàóÂá∫ÔºàlistÔºâpods Ë≥áÊ∫êÁöÑÊ¨äÈôêÔºåÂõ†Ê≠§ÁÑ°Ê≥ïÂæû Kubernetes API Áç≤Âèñ pods Ë≥áÊ∫ê‰ø°ÊÅØ„ÄÇ)

        Answer: default

    4)  Inspect the Dashboard Application POD and identify the Service Account mounted on it.
        Hint: Run the command kubectl describe pod and look for volume mount path.

        option1: service-account
        option2: default
        option3: dashboard-sa

            k describe pod web-dashboard-66d58fc7b8-k4kzq |grep "Service Account"

        controlplane ~ ‚úñ k describe pod web-dashboard-66d58fc7b8-k4kzq |grep "Service Account"
        Service Account:  default
        
        At what location is the ServiceAccount credentials available within the pod?

            k describe pod web-dashboard-66d58fc7b8-k4kzq

        controlplane ~ ‚ûú  k describe pod web-dashboard-66d58fc7b8-k4kzq |grep -A1 "Mount"
        Mounts:
        /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7klhg (ro)

        OR 

        controlplane ~ ‚úñ k get pod web-dashboard-66d58fc7b8-k4kzq -o yaml > tp.text
        controlplane ~ ‚ûú  vim tp.text 

            spec:
            containers:
            - env:
                - name: PYTHONUNBUFFERED
                value: "1"
                image: gcr.io/kodekloud/customimage/my-kubernetes-dashboard
                imagePullPolicy: Always
                name: web-dashboard
                ports:
                - containerPort: 8080
                protocol: TCP
                resources: {}
                terminationMessagePath: /dev/termination-log
                terminationMessagePolicy: File
                volumeMounts:
                - **mountPath**: /var/run/secrets/kubernetes.io/serviceaccount

        Answer: option1 /var/run/secrets
    
    5)  The application needs a ServiceAccount with the Right permissions to be created to authenticate to Kubernetes. The default ServiceAccount has limited access.   
        Create a new ServiceAccount named dashboard-sa.

            k create sa/serviceaccount dashboard-sa
        
        We just added additional permissions for the newly created dashboard-sa account using RBAC.
        If you are interested checkout the files used to configure RBAC at /var/rbac. We will discuss RBAC in a separate section.

        controlplane ~ ‚ûú  cat /var/rbac/pod-reader-role.yaml 
        ---
        kind: Role
        apiVersion: rbac.authorization.k8s.io/v1
        metadata:
        namespace: default
        name: pod-reader
        rules:
        - apiGroups:
        - ''
        resources:
        - pods
        verbs:
        - get
        - watch
        - list

    6)  Enter the access token in the UI of the dashboard application. Click Load Dashboard button to load Dashboard
        Create an authorization token for the newly created service account, copy the generated token and paste it into the token field of the UI.
        To do this, run kubectl create token dashboard-sa for the dashboard-sa service account, copy the token and paste it in the UI. 

                k create token dashboard-sa (creat a token for dashboard-sa service)

        controlplane ~ ‚ûú  k create token dashboard-sa 
        eyJhbGciOiJSUzI1NiIsImtpZCI6InJyU0NZRC16TUsyTnhFWUdjcmRzQnhiZUprT1hDWXV5cmlOYjJwMTk5RjAifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiLCJrM3MiXSwiZXhwIjoxNzM2MTU2MDQ4LCJpYXQiOjE3MzYxNTI0NDgsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwianRpIjoiZDliZGQxYjgtMmE3Mi00MzEwLWI1YWUtNTZlZjU4OWIyYmYyIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJkZWZhdWx0Iiwic2VydmljZWFjY291bnQiOnsibmFtZSI6ImRhc2hib2FyZC1zYSIsInVpZCI6ImUzYTViODM2LTMyN2UtNDgxMS1iMmZlLTM3NTBlNjE0OWJhNyJ9fSwibmJmIjoxNzM2MTUyNDQ4LCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6ZGVmYXVsdDpkYXNoYm9hcmQtc2EifQ.BeA-SZRJ9WvbuYt2fFBUv4DQXweHis79h84rl0culJB6W8IfoEIQuxSydvSakgAFkMQI2Ew_L4PbDwwPQ4LSDkIcrA1WXGjQq3UQRKWTFaZKI8LpTrCDqeS1Vv2QcWzNJ3HjaP1xukpKT93iYjI9Wj7vL-8leGIxllS_XXbz2TIW-38b65ht-PHEoPbpQYr-FzQuw_tSS58U_czx7OmQyAfUmW3Em75wOBpiIxMKAQBNH-j6e9gaYIHnwrRqSrUyycZZG7Ei1tPI3V-i2rL1_VbDcfkyJ7nkgRsMM_CHmBuHK3ohLzpUxSz7sVZDw5pjTUelOHwuEb601qWzBfEPZQ

        ![Acess the service](images/Service_Account02.png "Secrvice Account")  
        ![Running](images/Service_Account03.png "Secrvice Account")  


        You shouldn't have to copy and paste the token each time. The Dashboard application is programmed to read token from the secret mount location. However currently, the default service account is mounted. Update the deployment to use the newly created ServiceAccount
        
        Edit the deployment to change ServiceAccount from default to dashboard-sa.

        Deployment name: web-dashboard
        Service Account: dashboard-sa
        Deployment Ready

        Hint (2 way): 
        
        Use the **kubectl edit command** for the deployment and specify the serviceAccountName field inside the pod spec.

        OR

        Make use of the **kubectl set command**. Run the following command to use the newly created service account: - kubectl set serviceaccount deploy/web-dashboard dashboard-sa

        Way 1: kubectl edit command

            k edit deploy web-dashboard

        controlplane ~ ‚ûú  k get deploy
        NAME            READY   UP-TO-DATE   AVAILABLE   AGE
        web-dashboard   1/1     1            1           46m

        controlplane ~ ‚úñ k edit deploy web-dashboard    # **Remember deployment can directly use k edit command !!**
        deployment.apps/web-dashboard edited

        apiVersion: apps/v1
        kind: Deployment
        metadata:
        annotations:
            deployment.kubernetes.io/revision: "1"
        creationTimestamp: "2025-01-06T07:58:50Z"
        generation: 1
        name: web-dashboard
        namespace: default
        resourceVersion: "840"
        uid: 1df8a546-3dcd-4b5e-95e9-8e68fd5b009f
        spec:
        #serviceAccountName: dashboard-sa **Service Account Name should be configure under template field**
        progressDeadlineSeconds: 600
        replicas: 1
        revisionHistoryLimit: 10
        selector:
            matchLabels:
            name: web-dashboard
        strategy:
            rollingUpdate:
            maxSurge: 25%
            maxUnavailable: 25%
            type: RollingUpdate
        template:
            metadata:
            creationTimestamp: null
            labels:
                name: web-dashboard
            spec:
            **serviceAccountName: dashboard-sa**  # Add serviceAccountName
            containers:
            - env:
               ...
        
        Way 2: kubectl set command

            k set sa deploy/web-dashboard dashboard-sa
            deploy/web-dashboard: ÊåáÂÆöË¶ÅÊõ¥Êñ∞ÁöÑ Deployment ÂêçÁ®±ÊòØ web-dashboard
            dashboard-sa: Êñ∞ÁöÑ Service Account ÂêçÁ®±ÔºåÂç≥‰πãÂâçÂâµÂª∫ÁöÑ dashboard-sa

        controlplane ~ ‚ûú  kubectl set serviceaccount deploy/web-dashboard dashboard-sa
        deployment.apps/web-dashboard serviceaccount updated

        controlplane ~ ‚ûú  kubectl describe deploy web-dashboard | grep "Service Account"
        Service Account:  dashboard-sa

        controlplane ~ ‚ûú  kubectl get pods
        NAME                             READY   STATUS    RESTARTS   AGE
        web-dashboard-6f64cd9d8d-85kpz   1/1     Running   0          7m38s (Ê≠§ÁÇ∫Êõ¥Êñ∞ÂæåÊñ∞ÁöÑpod)

        controlplane ~ ‚ûú  kubectl get pods
        NAME                             READY   STATUS    RESTARTS   AGE
        web-dashboard-6f64cd9d8d-85kpz   1/1     Running   0          10m

        controlplane ~ ‚ûú  kubectl set serviceaccount deploy/web-dashboard default
        deployment.apps/web-dashboard serviceaccount updated

        controlplane ~ ‚ûú  kubectl get pods
        NAME                             READY   STATUS        RESTARTS   AGE
        web-dashboard-5b4846788d-nlnzg   1/1     Running       0          3s (Ê≠§ÁÇ∫Êõ¥Êñ∞ÂæåÊñ∞ÁöÑpod)
        web-dashboard-6f64cd9d8d-85kpz   1/1     Terminating   0          11m (Ê≠§ÁÇ∫ËàäÁöÑpod)
        

        Refresh the Dashboard application UI and you should now see the PODs listed automatically.
        This time you shouldn't have to put in the token manually.

        ![Service After Refresh](images/Service_Account04.png "Secrvice Account") 


7. Resource Requirements
   1) A pod called rabbit is deployed. Identify the CPU requirements set on the Pod in the current(default) namespace     

        controlplane ~ ‚ûú  k describe pod rabbit
        Containers:
            cpu-stress:
                Container ID:  containerd://078e512fa4fd27ca3d34807dfd7217ca648e59023cf2269081fae7c39b85bb72
                Image:         ubuntu
                Image ID:      docker.io/library/ubuntu@sha256:80dd3c3b9c6cecb9f1667e9290b3bc61b78c2678c02cbdae5f0fea92cc6734ab
                Port:          <none>
                Host Port:     <none>
                Args:
                    sleep
                    1000
                State:          Running
                    Started:      Mon, 06 Jan 2025 09:59:09 +0000
                Ready:          True
                Restart Count:  0
                Limits:
                    cpu:  1
                **Requests:** (Requests represent the minimum resource requirements of the container)
                    cpu:        500m
                Environment:  <none>
        
        Answer: 0.5 (500m = 0.5 cpu)

   2) Another pod called elephant has been deployed in the default namespace. It fails to get to a running state. Inspect this pod and identify the Reason why it is not running.

        option1: OOMKilled
        option2: CrashLoopBackOff

            k describe pod elephant  | grep -A5 State

        controlplane ~ ‚ûú  k describe pod elephant  | grep -A5 State
        State:          Waiting
        **Reason:       CrashLoopBackOff**
        **Last State:     Terminated**
        **Reason:       OOMKilled**
        Exit Code:    1
        Started:      Mon, 06 Jan 2025 10:06:55 +0000
        Finished:     Mon, 06 Jan 2025 10:06:55 +0000
        Ready:          False 

        The reason for the pod not running is OOMKilled. This means that the pod ran out of memory.
        
        Answer: option1 OOMKilled
    
        It's changing the status frequently so make use of the watch command to get the output every two seconds: -

        root@controlplane ~ ‚ûú  watch kubectl get pods

        Every 2.0s: kubectl get pods                                                               2025-01-06 10:10:09

        NAME       READY   STATUS             RESTARTS      AGE
        elephant   0/1     CrashLoopBackOff   5 (61s ago)   4m1s

        Every 2.0s: kubectl get pods                                                               2025-01-06 10:10:29

        NAME       READY   STATUS             RESTARTS      AGE
        elephant   0/1     CrashLoopBackOff   5 (82s ago)   4m22s
    
        (CTL + C to exit)

        The status OOMKilled indicates that it is failing because the pod ran out of memory. Identify the memory limit set on the POD.

   3)  The elephant pod runs a process that consumes 15Mi of memory. Increase the limit of the elephant pod to 20Mi.
        Delete and recreate the pod if required. Do not modify anything other than the required fields.

            k get pod elephant -o yaml >elephant.yaml
            vim elephant.yaml 
            k delete pod elephant 
            k apply -f elephant.yaml            

        controlplane ~ ‚ûú  k get pod elephant -o yaml >elephant.yaml

        controlplane ~ ‚ûú  vim elephant.yaml 

        controlplane ~ ‚ûú  k delete pod elephant 
        pod "elephant" deleted

        controlplane ~ ‚ûú  k apply -f elephant.yaml 
        pod/elephant created

        controlplane ~ ‚ûú  k get pods
        NAME       READY   STATUS    RESTARTS   AGE
        elephant   1/1     Running   0          3s

        apiVersion: v1
        kind: Pod
        metadata:
        creationTimestamp: "2025-01-06T10:06:08Z"
        name: elephant
        namespace: default
        resourceVersion: "1013"
        uid: 4320f461-a3ef-4a81-8046-c9c02f5ef737
        spec:
        containers:
        - args:
            - --vm
            - "1"
            - --vm-bytes
            - 15M
            - --vm-hang
            - "1"
            command:
            - stress
            image: polinux/stress
            imagePullPolicy: Always
            name: mem-stress
            resources:
            limits:
                memory: 20Mi #10Mi
            requests:
                memory: 5Mi


8.  Taints and Tolerations  
    Taints are set on Nodes and Tolerations are set on Pods.
    
    Taints-Nodes:
        Applied to nodes to repel certain pods.
        Restrict pod scheduling on the node.
        
        k taint nodes node-name key=value:taint-effect (ÊïàÊûú)
                      (3 types of effect: NoSchedule | PreferNoSchedule | NoExecute)

        k taint nodes node1 app=blue:NoSchedule

    Tolerations-Pods:
        Added to pods to allow them to tolerate specific taints.

        k taint node node1
    
    **Taint and tolerance can only limit which Pods cannot enter the node, and cannot specify that the Pod must enter this node.**
    **Special tains on the Master node: Kubernetes will set up taints on the Master node by default to prevent ordinary applications from being deployed to the Master node to ensure system stability.**


    To see a Taint:

        k describe node kubemaster | grep Taint
    
    1)  Do any taints exist on node01 node?

        option1: No
        option2: NoSchedule
        option3: NoExecute

            k describe node node01 | grep -i taints

        controlplane ~ ‚ûú  k get nodes
        NAME           STATUS   ROLES           AGE     VERSION
        controlplane   Ready    control-plane   2m40s   v1.31.0
        node01         Ready    <none>          2m10s   v1.31.0

        controlplane ~ ‚ûú  k describe node node01 
        Name:               node01
        Roles:              <none>
        Labels:             beta.kubernetes.io/arch=amd64
                            beta.kubernetes.io/os=linux
                            kubernetes.io/arch=amd64
                            kubernetes.io/hostname=node01
                            kubernetes.io/os=linux
        Annotations:        flannel.alpha.coreos.com/backend-data: {"VNI":1,"VtepMAC":"d6:73:02:97:ac:14"}
                            flannel.alpha.coreos.com/backend-type: vxlan
                            flannel.alpha.coreos.com/kube-subnet-manager: true
                            flannel.alpha.coreos.com/public-ip: 192.168.212.184
                            kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock
                            node.alpha.kubernetes.io/ttl: 0
                            volumes.kubernetes.io/controller-managed-attach-detach: true
        CreationTimestamp:  Mon, 06 Jan 2025 12:56:41 +0000
        **Taints:             <none>**
        Unschedulable:      false

        controlplane ~ ‚ûú  k describe node node01 | grep -i taints
        Taints:             <none>

        Answer: option1 No 
    
    2)  Create a taint on node01 with key of spray, value of mortein and effect of NoSchedule
        Key = spray
        Value = mortein
        Effect = NoSchedule

            kubectl taint nodes node01 spray=mortein:NoSchedule
            k describe nodes node01 | grep -i "Taint"

        controlplane ~ ‚ûú  kubectl taint nodes node01 spray=mortein:NoSchedule
        node/node01 tainted

        ~~controlplane ~ ‚úñ k get taints~~
        ~~error: the server doesn't have a resource type "taints"~~


        controlplane ~ ‚ûú  k describe nodes node01 | grep -i "Taint"
        Taints:             spray=mortein:NoSchedule


        In Kubernetes, **Taint is an attribute attached to Node**, rather than an independent resource type. 
        Therefore, **you cannot query them directly** with kubectl get taints or kubectl get taint. 
        Instead, you need to view the node details to see the taint.
    
    3)  What is the state of pod mosquito?
        controlplane ~ ‚ûú  k get pods
        NAME       READY   STATUS    RESTARTS   AGE
        mosquito   0/1     Pending   0          14s

        Answer: Pending

        Why do you think the pod is in a pending state?

        option1: image is not available
        option2: application error
        option3: pod mosquito can not tolerate taint Mortein

            k describe pod mosquito |grep -A3 "Events"

        controlplane ~ ‚ûú  k describe pod mosquito |grep -A3 "Events"
        Events:
        Type     Reason            Age    From               Message
        ----     ------            ----   ----               -------
        Warning  FailedScheduling  2m33s  default-scheduler  0/2 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 1 node(s) had untolerated taint {spray: mortein}. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.

        Answer: option3 pod mosquito can not tolerate taint Mortein

    4)  Create another pod named bee with the nginx image, which has a toleration set to the taint mortein.
        Image name: nginx

        Key: spray
        Value: mortein
        Effect: NoSchedule
        Status: Running

        Way 1: Create Pod then add Toleration
        
        i. Since we cannot directly use k edit on Pod:
                
                k get pod bee -o yaml > bee.yaml

        ii. Check whether the Taints have been set:

                k describe node node01 | grep -i "Taint"

            controlplane ~ ‚ûú  kubectl describe node node01 |grep -i "Taint"
            Taints:             spray=mortein:NoSchedule
        
        iii. Check whether the Tolerations have been set :

                vim bee.yaml
                k apply -f bee.yaml
            
            controlplane ~ ‚ûú  vim bee.yaml
            Taints:             spray=mortein:NoSchedule
            Check whether the Toleration have been set:
            vim bee.yaml
            (Add the toleration under spec:) 
                
            tolerations:
            - key: "spray"
                (operator: "Equal")
                value: "mortein"
                effect: "NoSchedule"
            
            (See more in: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/)

            :wq
            
            controlplane ~ ‚ûú  k apply -f bee.yaml 
            pod/bee created

            controlplane ~ ‚ûú  k get pods
            NAME       READY   STATUS              RESTARTS   AGE
            bee        0/1     ContainerCreating   0          6s

            controlplane ~ ‚ûú  k get pods
            NAME       READY   STATUS    RESTARTS   AGE
            bee        1/1     Running   0          35s

        Way 2: Add tolerance directly when creating Pod (Recommanded)

            controlplane ~ ‚úñ vim bee.yaml
            ---
            apiVersion: v1
            kind: Pod
            metadata:
            name: bee
            spec:
            containers:
            - image: nginx
                name: bee
            tolerations:
            - key: spray
                value: mortein
                effect: NoSchedule
                operator: Equal

            controlplane ~ ‚ûú  k apply -f bee.yaml
            pod/bee created

            controlplane ~ ‚ûú  k get pods
            NAME       READY   STATUS    RESTARTS   AGE
            bee        1/1     Running   0          6s

    5)  Do you see any taints on controlplane node?

        controlplane ~ ‚úñ k describe nodes controlplane |grep -i "Taint"
        Taints:             node-role.kubernetes.io/control-plane:NoSchedule

        Answer: Yes-NoSchedule

        Remove the taint on controlplane, which currently has the taint effect of NoSchedule.

        2 Ways:
        
        i. Declarative: 
        controlplane ~ ‚ûú  k get nodes controlplane -o yaml > controplane.yaml

        controlplane ~ ‚ûú  vim controplane.yaml 
        
            spec:
                podCIDR: 172.17.0.0/24
                podCIDRs:
                - 172.17.0.0/24
                    #  taints:  (Delete)
                    #  - effect: NoSchedule
                    #   key: node-role.kubernetes.io/control-plane

        controlplane ~ ‚ûú  k delete nodes controlplane 
        node "controlplane" deleted

        controlplane ~ ‚ûú  k apply -f controplane.yaml 
        node/controlplane created                

        ii. Imperative: run the command to untaint the node: (**Recommanded**)
            
            kubectl taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule-

        controlplane ~ ‚ûú  kubectl taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule-
        node/controlplane untainted

        controlplane ~ ‚úñ k describe nodes controlplane |grep -i "Taint"
        Taints:             <none>

        Which node is the POD mosquito on now?
        Hint: Run the command kubectl get pods -o wide and look at the Node column.

            kubectl get pods -o wide

        controlplane ~ ‚ûú  kubectl get pods -o wide
        NAME       READY   STATUS    RESTARTS   AGE   IP           NODE           NOMINATED NODE   READINESS GATES
        bee        1/1     Running   0          23m   172.17.1.2   node01         <none>           <none>
        mosquito   1/1     Running   0          96s   172.17.0.7   controlplane   <none>           <none>

        Answer: controplane


    ÁµêË´ñ:

    1.  ÁØÄÈªûÊúâÊ±ôÈªûÔºåPod ÂÆπÂøçË©≤Ê±ôÈªû
        ÊÉÖÊ≥ÅÔºö ÁØÄÈªûÊúâÊ±ôÈªû ‰∏î PodÁöÑtolerations Â∑≤Ê≠£Á¢∫ÈÖçÁΩÆ
        ÁµêÊûúÔºö Kubernetes Ë™øÂ∫¶Âô® (scheduler)ÊúÉÂ∞áË©≤ Pod ÂÆâÊéíÂà∞ÈÄôÂÄãÁØÄÈªû‰∏äÔºåÂõ†ÁÇ∫ Pod Â∑≤Á∂ìÂÆπÂøç‰∫ÜÈÄôÂÄãÊ±ôÈªûÔºåÁ¨¶ÂêàÊ¢ù‰ª∂„ÄÇ
    
    2. ÁØÄÈªûÊúâÊ±ôÈªûÔºåPod Ê≤íÊúâÂÆπÂøçË©≤Ê±ôÈªû
        ÊÉÖÊ≥ÅÔºö ÁØÄÈªûÊúâÊ±ôÈªûÔºå‰ΩÜ Pod Ê≤íÊúâÈÖçÁΩÆÂ∞çÊáâÁöÑÂÆπÂøçÂ∫¶„ÄÇ
        ÁµêÊûúÔºö Kubernetes schedulerÊúÉË∑≥ÈÅéÈÄôÂÄãÁØÄÈªûÔºåPod ÁÑ°Ê≥ïË¢´Ë™øÂ∫¶Âà∞Ê≠§ÁØÄÈªû‰∏ä„ÄÇ
    
    3. Pod ËôïÊñº Pending ÁãÄÊÖãÁöÑÂéüÂõ†
        Â¶ÇÊûúÈõÜÁæ§‰∏≠ÁöÑ ÊâÄÊúâÁØÄÈªû ÈÉΩÊúâÊú™Ë¢´ Pod ÂÆπÂøçÁöÑÊ±ôÈªûÔºåPod Â∞±ÊúÉÈÄ≤ÂÖ• Pending ÁãÄÊÖã„ÄÇ
        ÂÖ∑È´îÂéüÂõ†ÔºöKubernetes schedulerÁÑ°Ê≥ïÂú®‰ªª‰ΩïÁØÄÈªû‰∏äÊâæÂà∞Á¨¶ÂêàÊ¢ù‰ª∂ÁöÑÁ©∫Èñì‰æÜÈÅãË°å Pod„ÄÇ

    4. ÁØÄÈªûÊ≤íÊúâÊ±ôÈªûÔºåPod ÊúÉË¢´Ë™øÂ∫¶Âà∞Ê≠§ÁØÄÈªû
        ÊÉÖÊ≥ÅÔºö Â¶ÇÊûúÈõÜÁæ§‰∏≠ÁöÑ Ëá≥Â∞ë‰∏ÄÂÄãÁØÄÈªû Ê≤íÊúâÊ±ôÈªûÔºåPod Âú®Ê≤íÊúâÂÆπÂøçÂ∫¶ÈôêÂà∂ÁöÑÊÉÖÊ≥Å‰∏ãÊúÉË¢´Ë™øÂ∫¶Âà∞ÈÄôÂÄãÁØÄÈªûÔºåÂâçÊèêÊòØË©≤ÁØÄÈªûÊúâË∂≥Â§†ÁöÑË≥áÊ∫ê„ÄÇ
        ÁµêÊûúÔºö Kubernetes Ë™øÂ∫¶Âô®ÊúÉÈÅ∏ÊìáÈÄôÂÄãÊ≤íÊúâÊ±ôÈªûÁöÑÁØÄÈªû‰æÜÈÅãË°å Pod„ÄÇ

    5.  Pod ÂøÖÈ†àÂåπÈÖçÂà∞‰∏ÄÂÄãÁØÄÈªû
        Kubernetes scheduler ÁöÑÂ∑•‰ΩúÂéüÂâáÊòØÂ∞áÊØèÂÄã Pod Ë™øÂ∫¶Âà∞ÊüêÂÄãÁØÄÈªû‰∏äÔºåÂâçÊèêÊòØÊúâÁ¨¶ÂêàÊ¢ù‰ª∂ÁöÑÁØÄÈªû„ÄÇ
        Â¶ÇÊûúÊâÄÊúâÁØÄÈªûÈÉΩ‰∏çÁ¨¶ÂêàÊ¢ù‰ª∂ÔºåPod Â∞±ÊúÉÈÄ≤ÂÖ• Pending ÁãÄÊÖãÔºåÁ≠âÂæÖÁØÄÈªûÊ¢ù‰ª∂ÁôºÁîüËÆäÂåñÔºà‰æãÂ¶ÇÁßªÈô§Ê±ôÈªû„ÄÅÊ∑ªÂä†ÂÆπÂøçÂ∫¶ÔºåÊàñËÄÖÊñ∞Â¢ûÁ¨¶ÂêàÊ¢ù‰ª∂ÁöÑÁØÄÈªûÔºâ„ÄÇ


9.  Node Selectors & Affinity   
    In a cluster with two small nodes (low resources) and one large node (high resources), we need to restrict high-resource workloads (Pod C) to the large node to prevent resource exhaustion. By default, Pods can be scheduled on any node, which may lead to suboptimal placements. NodeSelectors can solve this problem.

    - Functionality:
        Restricts Pods to specific nodes with matching labels.
        Add the nodeSelector field in the Pod's spec section and specify a key-value pair (e.g., size: large).
    
    - Implementation:
        Label the target node (e.g., size=large).
        Set the nodeSelector in the Pod definition file.
        Create the Pod, and it will be scheduled on the labeled node (e.g., the large node).
    
    - Limitations:
        NodeSelectors support only a single key-value pair for matching.
        Cannot handle complex conditions. (eg. OR, NOT...)
    
    To label a node:
        
        k label nodes <node-name> <label-key>=<label-value>
        k label nodes node-1 size=large
    
    
    - Advanced Features:
        Node Affinity and Anti-Affinity address NodeSelectors' limitations.
        Allow for complex expressions, such as multiple label matches or exclusions.

    2 Types of Node Affinity:

        - Available:
            - requiredDuringSchedulingIgnoredDuringExecution
            - preferredDuringSchedulingIgnoredDuringExecution
        - Planned:
          - requiredDuringSchedulingRequiredDuringExecution 

        (See more on: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/)

    1) How many Labels exist on node node01?

        controlplane ~ ‚ûú  k describe nodes node01 | grep -A10 (set whatever you whant to display the column) "Labels"
        Labels:             beta.kubernetes.io/arch=amd64
                            beta.kubernetes.io/os=linux
                            kubernetes.io/arch=amd64
                            kubernetes.io/hostname=node01
                            kubernetes.io/os=linux (**5 labels in total**)
        Annotations:        flannel.alpha.coreos.com/backend-data: {"VNI":1,"VtepMAC":"9e:ab:7e:00:43:77"}
                            flannel.alpha.coreos.com/backend-type: vxlan
                            flannel.alpha.coreos.com/kube-subnet-manager: true
                            flannel.alpha.coreos.com/public-ip: 192.168.212.145
                            kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock
                            node.alpha.kubernetes.io/ttl: 0


        Answer: 5

        What is the value set to the label key beta.kubernetes.io/arch on node01?

        Carry On, beta.kubernetes.io/arch=amd64

        Answer: amd4

    2)  Apply a label color=blue to node node01
        Hint: Runt kubectl label node node01 color=blue

            kubectl label node node01 color=blue
        
        controlplane ~ ‚ûú  kubectl label node node01 color=blue
        node/node01 labeled

        controlplane ~ ‚ûú  k describe node node01 |grep -10 "Labels"
        ... 
        Name:               node01
        Roles:              <none>
        Labels:             beta.kubernetes.io/arch=amd64
                            beta.kubernetes.io/os=linux
                            color=blue (ADDED
                            kubernetes.io/arch=amd64
                            kubernetes.io/hostname=node01
                            kubernetes.io/os=linux
        ...

    3)  Create a new deployment named blue with the nginx image and 3 replicas.
        Name: blue
        Replicas: 3
        Image: nginx

            kubectl create deployment my-dep --image=nginx --replicas=3

        controlplane ~ ‚ûú  kubectl create deployment blue --image=nginx --replicas=3
        deployment.apps/blue created

        controlplane ~ ‚ûú  k get deploy
        NAME   READY   UP-TO-DATE   AVAILABLE   AGE
        blue   0/3     3            0           5s

    4)  Which nodes can the pods for the blue deployment be placed on?
        Make sure to check taints on both nodes!

        option1: node01
        option2: controlplane
        option3: controplane and node01

        Hint: Check if controlplane and node01 have any taints on them that will prevent the pods to be scheduled on them. If there are no taints, the pods can be scheduled on either node.

        So run the following command to check the taints on both nodes.
            
            kubectl describe node controlplane | grep -i taints
            kubectl describe node node01 | grep -i taints

        controlplane ~ ‚ûú  k get nodes
        NAME           STATUS   ROLES           AGE     VERSION
        controlplane   Ready    control-plane   3m48s   v1.31.0
        node01         Ready    <none>          3m17s   v1.31.0

        controlplane ~ ‚ûú  k describe nodes controlplane |grep -i "Taints"
        Taints:             <none>

        controlplane ~ ‚ûú  k describe nodes node01 |grep -i "Taints"
        Taints:             <none>

        Answer: controplane and node01

    5)  Set Node Affinity to the deployment to place the pods on node01 only.
        
        Name: blue
        Replicas: 3
        Image: nginx
        NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution
        Key: color
        value: blue

        Hint: Edit the deployment blue and add the Node Affinity with specified key and value.

        controlplane ~ ‚ûú  k get deployment blue -o yaml > blue.yaml 

        controlplane ~ ‚ûú  vim blue.yaml 

            ...
              template:
                metadata:
                creationTimestamp: null
                labels:
                    app: blue
                spec:
                    affinity:
                        nodeAffinity:
                        requiredDuringSchedulingIgnoredDuringExecution:
                            nodeSelectorTerms:
                            - matchExpressions:
                            - key: color
                                operator: In
                                values:
                                - blue
                    containers:
                    - image: nginx
                        imagePullPolicy: Always
                        name: nginx
                        resources: {}
            ...
        
        **affinity ÊòØ Pod Â±§Á¥öÁöÑÂ±¨ÊÄß**ÔºåÂøÖÈ†àÊîæÂú® **template.spec** ‰∏ã
        **È†ÇÂ±§ spec Âè™ÈÅ©Áî®Êñº Deployment ÁöÑÂÖ®Â±ÄÂ±¨ÊÄß**Ôºå‰æãÂ¶ÇÂâØÊú¨Êï∏ÂíåÊõ¥Êñ∞Á≠ñÁï•„ÄÇ

        (See more settings on: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/)

        controlplane ~ ‚ûú  k get deployments
        NAME   READY   UP-TO-DATE   AVAILABLE   AGE
        blue   3/3     3            3           19m

        controlplane ~ ‚ûú  k delete deploy blue 
        deployment.apps "blue" deleted

        controlplane ~ ‚ûú  k apply -f blue.yaml 
        deployment.apps/blue created

        controlplane ~ ‚ûú  k get deployments
        NAME   READY   UP-TO-DATE   AVAILABLE   AGE
        blue   3/3     3            3           9s

    6)  Which nodes are the pods placed on now?

             k get pods -o wide

        controlplane ~ ‚ûú  k get pods -o wide
        NAME                    READY   STATUS    RESTARTS   AGE     IP           NODE     NOMINATED NODE   READINESS GATES
        blue-5659879b55-pmzh4   1/1     Running   0          3m14s   172.17.1.4   node01   <none>           <none>
        blue-5659879b55-ptkmb   1/1     Running   0          3m14s   172.17.1.6   node01   <none>           <none>
        blue-5659879b55-rr789   1/1     Running   0          3m14s   172.17.1.5   node01   <none>           <none>

        Answer: node01

    7)  Create a new deployment named red with the nginx image and 2 replicas, and ensure it gets placed on the controlplane node only.
        Use the label key - node-role.kubernetes.io/control-plane - which is already set on the controlplane node.

            k get deploy red -o yaml > red.yaml
            vim red.yaml
            k apply -f red.yaml
            k get deploy

        Name: red
        Replicas: 2
        Image: nginx
        NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution
        Key: node-role.kubernetes.io/control-plane
        Use the right operator

        controlplane ~ ‚ûú  k get deploy red -o yaml > red.yaml

        controlplane ~ ‚ûú  vim red.yaml
        (Copy the below content from the doc then replace with required values)
           
            ...
            template:
                metadata:
                creationTimestamp: null
                labels:
                    app: red
                spec:
                    affinity:
                        nodeAffinity:
                        requiredDuringSchedulingIgnoredDuringExecution:
                            nodeSelectorTerms:
                            - matchExpressions:
                            - key: node-role.kubernetes.io/control-plane
                                operator: Exists (Exists: A label with this key exists on the object)
                                #values:
                                #- #linux
                    containers:
                    - image: nginx
                ... 

        Why operator: Exixts dont' need values? How to Choose Exists or In?
           
            Â¶ÇÊûú‰Ω†Âè™ÊÉ≥Á¢∫Ë™çÊ®ôÁ±§ÊòØÂê¶Â≠òÂú®Ôºö
            ‰ΩøÁî® operator: ExistsÔºå‰∏çÈúÄË¶Å values„ÄÇ

            Â¶ÇÊûú‰Ω†ÈúÄË¶ÅÂåπÈÖçÁâπÂÆöÁöÑÊ®ôÁ±§ÂÄºÔºö
            ‰ΩøÁî® operator: InÔºå‰∏¶ÊåáÂÆö values„ÄÇ

            
         (See more settings on: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/)

        controlplane ~ ‚ûú  k apply -f red.yaml 
        deployment.apps/red created

        controlplane ~ ‚ûú  k get deploy
        NAME   READY   UP-TO-DATE   AVAILABLE   AGE
        blue   3/3     3            3           11m
        red    0/2     2            0           8s

       


10. Taints & Tolerations vs. Node Affinity

    How to command Taints/Tolerations and Node Affinity ?
    
    GoalÔºö
        Put the Blue pod on Blue Node, Red Pod on Red Node and Green pod on Green Node
        Prevent other group' Pods from being placed on our nodes.
        Prevent our Pods from being scheduled to other grpous‚Äô nodes.
    
    Attemp 1ÔºöUsing Taints and Tolerations

        Setting methodÔºö
        Áµ¶ÁØÄÈªûÊ∑ªÂä†ËàáÈ°èËâ≤ÂåπÈÖçÁöÑÊ±ôÈªûÔºàtaintsÔºâÔºåÂ¶Ç color=blue:NoSchedule„ÄÇ
        Áµ¶ Pod Ê∑ªÂä†Áõ∏ÊáâÁöÑÂÆπÂøçÂ∫¶ÔºàtolerationsÔºâÔºåÂ¶Ç color=blue„ÄÇ
        
        EffectÔºö
        Á¢∫‰øùÁ∂†Ëâ≤ Pod ÊîæÁΩÆÂú®Á∂†Ëâ≤ÁØÄÈªûÔºåËóçËâ≤ Pod ÊîæÁΩÆÂú®ËóçËâ≤ÁØÄÈªû„ÄÇ
        
        RestrictionÔºö
        ‰∏ç‰øùË≠â Pod Âè™ËÉΩÊîæÁΩÆÂà∞ÊåáÂÆöÁöÑÁØÄÈªû‰∏ä„ÄÇ
        Â¶ÇÊûúÊúâÂÖ∂‰ªñÊú™Ë®≠ÁΩÆÊ±ôÈªûÁöÑÁØÄÈªûÔºåPod ÂèØËÉΩË¢´Ë™øÂ∫¶Âà∞ÈÄô‰∫õÁØÄÈªû„ÄÇ (=> pod ÂèØËÉΩË¢´ÊîæÂà∞ÂÖ∂‰ªñnodes, ÈúÄÁî®node affinity Ëß£Ê±∫)
    
    Attemp 2ÔºöUsing Node Affinity

        Setting methodÔºö
        ÁÇ∫ÁØÄÈªûÊ∑ªÂä†Ê®ôÁ±§ÔºàÂ¶Ç color=blueÔºâ„ÄÇ
        ÁÇ∫ Pod Ë®≠ÁΩÆ Node AffinityÔºåÁ¢∫‰øù Pod ÂÉÖË™øÂ∫¶Âà∞ËàáÊ®ôÁ±§ÂåπÈÖçÁöÑÁØÄÈªû„ÄÇ
        
        EffectÔºö
        Á¢∫‰øù Pod Ë¢´Ë™øÂ∫¶Âà∞ËàáÊ®ôÁ±§ÂåπÈÖçÁöÑÁØÄÈªû„ÄÇ
        
        RestrictionÔºö
        ‰∏ç‰øùË≠âÁØÄÈªûÂÉÖÊé•Êî∂ÊåáÂÆöÁöÑ Pod„ÄÇ
        ÂÖ∂‰ªñ Pod ÂèØËÉΩË¢´Ë™øÂ∫¶Âà∞ÈÄô‰∫õÁØÄÈªû„ÄÇ
        ÊúÄ‰Ω≥Ëß£Ê≥ïÔºöÁµêÂêà Taints/Tolerations Âíå Node Affinity
        
    StrategyÔºö

        Taints Âíå TolerationsÔºöÈò≤Ê≠¢ÂÖ∂‰ªñ Pod ÊîæÁΩÆÂà∞ÊàëÂÄëÁöÑÁØÄÈªû„ÄÇ
        Node AffinityÔºöÈò≤Ê≠¢ÊàëÂÄëÁöÑ Pod ÊîæÁΩÆÂà∞ÂÖ∂‰ªñÁØÄÈªû„ÄÇ
        
        ÊïàÊûúÔºö
        Á¢∫‰øùÁØÄÈªûÂ∞àÁî®ÊñºÁâπÂÆöÁöÑ PodÔºå‰∏î Pod ÂÉÖË™øÂ∫¶Âà∞ÊåáÂÆöÁöÑÁØÄÈªû„ÄÇ
        Á∏ΩÁµê
    
    Taints Âíå TolerationsÔºö
        Used to limit which Pods can be scheduled on nodes.
    
    Node AffinityÔºö
        Used to control which nodes Pods should be scheduled to.
    
    Combination of UsageÔºö
        Ensure nodes are dedicated to specific Pods and protect Pod scheduling policies.

11. Volumns

    doc: https://kubernetes.io/docs/concepts/storage/volumes/

    Persistent Volume (PV)
    ÂÆöÁæ©ÔºöPV ÊòØÈõÜÁæ§Â±§Á¥öÁöÑÂ≠òÂÑ≤Ë≥áÊ∫êÔºåÁî±ÁÆ°ÁêÜÂì°ÊèêÂâçÈÖçÁΩÆ„ÄÇÂÆÉÊòØ‰∏ÄÁ®ÆÊäΩË±°Ôºå‰ª£Ë°®‰∏ÄÂ°äÂ≠òÂÑ≤ÔºåÂØ¶ÈöõÂèØ‰ª•Êò†Â∞ÑÂà∞Â§öÁ®ÆÂ≠òÂÑ≤ÂæåÁ´ØÔºå‰æãÂ¶Ç NFS„ÄÅiSCSI„ÄÅÈõ≤Â≠òÂÑ≤ÔºàÂ¶Ç AWS EBS„ÄÅGCP Persistent DiskÔºâÁ≠â„ÄÇ
    ÁâπÊÄßÔºö
    Â≠òÂú®ÊñºÈõÜÁæ§‰∏≠ÔºåÊòØ‰∏ÄÂÄãË≥áÊ∫êÁâ©‰ª∂„ÄÇ
    ÂèØ‰ª•Ë¢´Â§öÂÄã PVC Áî≥Ë´ãÔºå‰ΩÜÈÄöÂ∏∏Âè™ËÉΩË¢´‰∏ÄÂÄã Pod ‰ΩøÁî®Ôºà‰æùÊìöÂ≠òÂèñÊ®°ÂºèÔºâ„ÄÇ
    ÂåÖÂê´Ë©≥Á¥∞ÁöÑÂ≠òÂÑ≤Â±¨ÊÄßÔºå‰æãÂ¶ÇÂ§ßÂ∞è„ÄÅÂ≠òÂèñÊ®°ÂºèÔºàÂ¶Ç ReadWriteOnce„ÄÅReadOnlyManyÔºâ‰ª•ÂèäÂ≠òÂÑ≤È°ûÂûã„ÄÇ

    Persistent Volume Claim (PVC)
    ÂÆöÁæ©ÔºöPVC ÊòØÁî®Êà∂Áî®‰æÜÁî≥Ë´ãÂ≠òÂÑ≤Ë≥áÊ∫êÁöÑ‰∏ÄÁ®ÆÊñπÂºè„ÄÇPod ÈÄöÈÅé PVC Ë´ãÊ±ÇÂ≠òÂÑ≤ÔºåËÄå PVC ÂÜçÂéªÂåπÈÖçÁ¨¶ÂêàÊ¢ù‰ª∂ÁöÑ PV„ÄÇ
    ÁâπÊÄßÔºö
    Áî®Êà∂ÂÆöÁæ©ÊâÄÈúÄÁöÑÂ≠òÂÑ≤Â§ßÂ∞èÂíåÁâπÊÄßÔºàÂ¶ÇÂ≠òÂèñÊ®°ÂºèÔºâ„ÄÇ
    PVC Âíå PV ÁöÑÁ∂ÅÂÆöÊòØÂãïÊÖãÁöÑÔºåÁï∂‰∏ÄÂÄã PVC Ë¢´ÂâµÂª∫ÊôÇÔºåKubernetes ÊúÉÂ∞ãÊâæÂåπÈÖçÁöÑ PV„ÄÇ
    ÂèØ‰ª•ÂíåÂ≠òÂÑ≤È°ûÂà•ÔºàStorageClassÔºâÁµêÂêà‰ΩøÁî®ÔºåÊîØÊåÅÂãïÊÖãÂâµÂª∫ PV„ÄÇ

    ![PV vs. Volumn](images/PV.png "PV vs. Volumn")

    PV Ëàá Volume ÁöÑÈóú‰øÇ
    Volume ÊòØ PV ÁöÑÊ∂àË≤ªËÄÖÔºö
        Volume ÂèØ‰ª•ÈÄöÈÅé Persistent Volume Claim (PVC) ‰æÜË´ãÊ±Ç‰∏ÄÂÄã PV„ÄÇ
        Áï∂ Pod ÂÆöÁæ©‰∏≠ÂºïÁî® PVC ÊôÇÔºåKubernetes ÊúÉÂ∞áÂ∞çÊáâÁöÑ PV ÊåÇËºâÁÇ∫Ë©≤ Pod ÁöÑ Volume„ÄÇ
        Volume ÁöÑÊåÅ‰πÖÊÄßÂèñÊ±∫ÊñºÊòØÂê¶‰ΩøÁî® PVÔºö
        Â¶ÇÊûú Volume ‰ΩøÁî®‰∫Ü PVÔºàÈÄöÈÅé PVCÔºâÔºåÂâáÊï∏ÊìöÊòØÊåÅ‰πÖÁöÑ„ÄÇ
        Â¶ÇÊûú Volume ÊòØÂÖ∂‰ªñÈ°ûÂûãÔºàÂ¶Ç emptyDir Êàñ configMapÔºâÔºåÂâáÊï∏ÊìöÊòØËá®ÊôÇÁöÑ„ÄÇ
        Pod Ëàá PV ÁöÑÊ©ãÊ®ëÔºö
        Pod ÂÆöÁæ©‰∏≠ÔºåÈÄöÈÅé PVC ÂºïÁî®‰∫Ü PVÔºåPV ÁöÑÂ≠òÂÑ≤Ë≥áÊ∫êÂ∞±ÊúÉÊéõËºâÂà∞ Pod ÁöÑ Volume ‰∏≠Ôºå‰æõÂÆπÂô®‰ΩøÁî®„ÄÇ

    PVÔºöÁõ∏Áï∂ÊñºÂ≠òÂÑ≤Á≥ªÁµ±‰∏≠ÁöÑÁ°¨Á¢üÁ©∫ÈñìÔºåÂèØËÉΩÊòØÊüêÂÄã NAS Á≥ªÁµ±ÊàñÈõ≤Â≠òÂÑ≤Ë≥áÊ∫ê„ÄÇ
    PVCÔºöÁî®Êà∂ÁöÑË´ãÊ±ÇÔºåÊèèËø∞ÈúÄË¶ÅÁöÑÂ≠òÂÑ≤Â§ßÂ∞èËàáÁâπÊÄß„ÄÇ
    VolumeÔºöÊúÄÁµÇÊéõËºâÂà∞ Pod ÁöÑÂ≠òÂÑ≤Á©∫ÈñìÔºåÂèØËÉΩ‰æÜËá™ PVÔºå‰πüÂèØËÉΩÊòØËá®ÊôÇÂ≠òÂÑ≤„ÄÇ

    1)  We have deployed a POD. Inspect the POD and wait for it to start running.
        In the current(default) namespace.

        controlplane ~ ‚ûú  k get pod
        NAME     READY   STATUS    RESTARTS   AGE
        webapp   1/1     Running   0          32s


        The application stores logs at location /log/app.log. View the logs.
        You can exec in to the container and open the file:

            kubectl exec webapp -- cat /log/app.log

        controlplane ~ ‚ûú  k exec pods/webapp -- cat /log/app.log
        [2025-01-24 03:46:53,329] INFO in event-simulator: USER3 is viewing page1
        [2025-01-24 03:46:54,330] INFO in event-simulator: USER2 logged out
        [2025-01-24 03:46:55,332] INFO in event-simulator: USER2 is viewing page1
        [2025-01-24 03:46:56,333] INFO in event-simulator: USER2 is viewing page3
        [2025-01-24 03:46:57,334] INFO in event-simulator: USER2 is viewing page1
        [2025-01-24 03:46:58,335] WARNING in event-simulator: USER5 Failed to Login as the account is locked due to MANY FAILED ATTEMPTS.
        [2025-01-24 03:46:58,336] INFO in event-simulator: USER1 logged out
        [2025-01-24 03:46:59,337] INFO in event-simulator: USER1 is viewing page2
        [2025-01-24 03:47:00,338] INFO in event-simulator: USER3 is viewing page3
        [2025-01-24 03:47:01,339] WARNING in event-simulator: USER7 Order failed as the item is OUT OF STOCK.
        [2025-01-24 03:47:01,339] INFO in event-simulator: USER4 logged out
        [2025-01-24 03:47:02,341] INFO in event-simulator: USER1 is viewing page2
        [2025-01-24 03:47:03,342] WARNING in event-simulator: USER5 Failed to Login as the account is locked due to MANY FAILED ATTEMPTS.
        ...



        If the POD was to get deleted now, would you be able to view these logs.

        controlplane ~ ‚ûú  k delete pod webapp --force
        Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.
        pod "webapp" force deleted

        controlplane ~ ‚ûú  k exec pods/webapp -- cat /log/app.log
        Error from server (NotFound): pods "webapp" not found

        Answer: No


        Configure a volume to store these logs at /var/log/webapp on the host.
        Use the spec provided below.
            Name: webapp
            Image Name: kodekloud/event-simulator
            Volume HostPath: /var/log/webapp
            Volume Mount: /log
                
                vim webapp-volumn.yaml
                k replace -f webapp-volumn.yaml --force  (Delete + Create)

            apiVersion: v1
            kind: Pod
            metadata:
            name: webapp
            namespace: default
            spec:
            containers:
            - name: webapp
                image: kodekloud/event-simulator
                volumeMounts: (Êõ¥ÊîπvolumeMounts)
                - name: log-volume
                mountPath: /log
                readOnly: true
            volumes: (Êñ∞Â¢ûvolumes ÂÆöÁæ©)
            - name: log-volume
                hostPath:
                path: /var/log/webapp
                type: Directory

        controlplane ~ ‚úñ k delete pod webapp --force 
        Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.
        pod "webapp" force deleted

        controlplane ~ ‚ûú  k create -f webapp-volumn.yaml 
        pod/webapp created

        OR:

        controlplane ~ ‚ûú  kubectl replace -f webapp-volumn.yaml --force
        pod "webapp" deleted
        pod/webapp replaced

    2)  Create a Persistent Volume with the given specification.
            Volume Name: pv-log
            Storage: 100Mi
            Access Modes: ReadWriteMany
            Host Path: /pv/log
            Reclaim Policy: Retain
        

        controlplane ~ ‚ûú  k create -f webapp-pv.yaml 
        persistentvolume/pv-log created

        controlplane ~ ‚ûú  vim webapp-pv.yaml
            
            apiVersion: v1
            kind: PersistentVolume
            metadata:
            name: pv-log
            spec:
            capacity:
                storage: 100Mi
            volumeMode: Filesystem (Ê≠§Á≠ÜÂèØÊúâÂèØÁÑ°)
            accessModes:
            - ReadWriteMany
            persistentVolumeReclaimPolicy: Retain
            hostPath:
                path: /pv/log

        According to the doc: https://kubernetes.io/docs/concepts/storage/volumes/

        "A local volume represents a mounted local storage device such as a disk, partition or directory.

        Local volumes can only be used as a statically created PersistentVolume. Dynamic provisioning is not supported.

        Compared to hostPath volumes, local volumes are used in a durable and portable manner without manually scheduling pods to nodes. The system is aware of the volume's node constraints by looking at the node affinity on the PersistentVolume.

        However, local volumes are subject to the availability of the underlying node and are not suitable for all applications. If a node becomes unhealthy, then the local volume becomes inaccessible by the pod. The pod using this volume is unable to run. Applications using local volumes must be able to tolerate this reduced availability, as well as potential data loss, depending on the durability characteristics of the underlying disk."

        ÊïÖÊàëÂÄëÈÅ∏ÊìáË®≠ÁΩÆhostPathÔºåËÄåÈùûlocal.

        controlplane ~ ‚úñ k replace -f webapp-pv.yaml --force
        persistentvolume "pv-log" deleted
        persistentvolume/pv-log replaced



        Let us claim some of that storage for our application. Create a Persistent Volume Claim with the given specification.
            Persistent Volume Claim: claim-log-1
            Storage Request: 50Mi
            Access Modes: ReadWriteOnce

        controlplane ~ ‚ûú  vim webapp-pvc.yaml
            
            apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
            name: claim-log-1
            spec:
            accessModes:
                - ReadWriteOnce
            volumeMode: Filesystem
            resources:
                requests:
                storage: 50Mi
            storageClassName: slow
            selector: 
                matchLabels:
                name: webapp

            Tutor's Answer:
            kind: PersistentVolumeClaim
            apiVersion: v1
            metadata:
            name: claim-log-1
            spec:
            accessModes:
                - ReadWriteOnce
            resources:
                requests:
                storage: 50Mi

        controlplane ~ ‚ûú  k create -f webapp-pvc.yaml 
        persistentvolumeclaim/claim-log-1 created

        controlplane ~ ‚ûú  k get pv
        NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE
        pv-log   100Mi      RWX            Retain           Available                          <unset>                          7m20s

        controlplane ~ ‚ûú  k get pvc
        NAME          STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
        claim-log-1   Pending                                      slow           <unset>                 7s



        What is the state of the Persistent Volume Claim?
        Answer: Pending


        What is the state of the Persistent Volume?
        Answer: Available


        Why is the claim not bound to the available Persistent Volume?
            option1: PV and PVC name mismatch
            option2: Reclaim Policy not set correctly
            option3: Capacity Mismatch
            option4: Access Modes Mismatch
        
        Run the command: kubectl get pv,pvc and look at the Access Modes.
        
        controlplane ~ ‚ûú  k get pv
        NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE
        pv-log   100Mi      **RWX**         Retain           Available                          <unset>                          7m20s

        controlplane ~ ‚ûú  k get pvc
        NAME          STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
        claim-log-1   Pending                                      slow           <unset>                 7s

        From the doc: https://kubernetes.io/docs/concepts/storage/persistent-volumes/
        RWO - ReadWriteOnce
        ROX - ReadOnlyMany
        RWX - ReadWriteMany
        RWOP - ReadWriteOncePod


        Answer: option4 Access Modes Mismatch


        Update the Access Mode on the claim to bind it to the PV.

        Delete and recreate the claim-log-1.
            Persistent Volume Claim: claim-log-1
            Storage Request: 50Mi
            PVol: pv-log
            Status: Bound

        Hint: Set the Access Mode on the PVC to ReadWriteMany                 

        controlplane ~ ‚ûú  vim webapp-pvc.yaml 

            apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
            name: claim-log-1
            spec:
            accessModes:
                - ReadWriteMany (Chnage from ReadWriteOnce to ReadWriteMany)
            volumeMode: Filesystem
            resources:
                requests:
                storage: 50Mi
            ~ 

        controlplane ~ ‚ûú  k replace -f webapp-pvc.yaml 
        The PersistentVolumeClaim "claim-log-1" is invalid: spec: Forbidden: spec is immutable after creation except resources.requests and volumeAttributesClassName for bound claims
        core.PersistentVolumeClaimSpec{
        -       AccessModes: []core.PersistentVolumeAccessMode{"ReadWriteOnce"},
        +       AccessModes: []core.PersistentVolumeAccessMode{"ReadWriteMany"},
                Selector:    nil,
                Resources:   {Requests: {s"storage": {i: {...}, s: "50Mi", Format: "BinarySI"}}},
                ... // 6 identical fields
        }


        controlplane ~ ‚úñ k delete pvc claim-log-1 
        persistentvolumeclaim "claim-log-1" deleted

        controlplane ~ ‚ûú  k create -f webapp-pvc.yaml 
        persistentvolumeclaim/claim-log-1 created

        controlplane ~ ‚ûú  k get pvc
        NAME          STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
        claim-log-1   Bound    pv-log   100Mi      RWX                           <unset>                 6s


        
    3)  You requested for 50Mi, how much capacity is now available to the PVC?
        controlplane ~ ‚ûú  k get pvc
        NAME          STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
        claim-log-1   Bound    pv-log   100Mi      RWX                           <unset>                 6s

        Answer: 100Mi



        Update the webapp pod to use the persistent volume claim as its storage.
        Replace hostPath configured earlier with the newly created PersistentVolumeClaim.
            Name: webapp
            Image Name: kodekloud/event-simulator
            Volume: PersistentVolumeClaim=claim-log-1
            Volume Mount: /log
        
        controlplane ~ ‚ûú  vim webapp.yaml 
        
            apiVersion: v1
            kind: Pod
            metadata:
            name: webapp
            namespace: default
            spec:
            containers:
            - name: test
                image: kodekloud/event-simulator
                imagePullPolicy: Always
                env:
                - name: LOG_HANDLERS
                value: file
                volumeMounts:
                - mountPath: /log
                name: log-volume
                readOnly: true

            volumes: 
            - name: log-volume
                #hostPath:  (Remove the loca local hostPath)
                #path: /var/log/webapp
                #type: Directory
                
                persistentVolumeClaim: (Add claim)
                claimName: claim-log-1

        controlplane ~ ‚ûú  k delete pod webapp --force
        Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.
        pod "webapp" force deleted

        controlplane ~ ‚ûú  k create -f webapp.yaml 
        pod/webapp created

        controlplane ~ ‚ûú  k get pod
        NAME     READY   STATUS    RESTARTS   AGE
        webapp   1/1     Running   0          2s


       
        What is the Reclaim Policy set on the Persistent Volume pv-log?
        Hint: Run the command: kubectl get pv and look under the Reclaim Policy column.

        controlplane ~ ‚ûú  k get pv
        NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                 STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE
        pv-log   100Mi      RWX            Retain           Bound    default/claim-log-1                  <unset>                          19m

        Answer: Retain


        What would happen to the PV if the PVC was destroyed?
            option1: The PV is deleted.
            option2: The PV is not deleted but available.

        Answer: option2: The PV is not deleted but available


        Try deleting the PVC and notice what happens.
            option1: The PVC is deleted
            option2: The PVC is stuck in 'terminating' state
        
        If the command hangs, you can use CTRL + C to get back to the bash prompt OR check the status of the pvc from another terminal

        controlplane ~ ‚úñ k delete pvc claim-log-1 
        persistentvolumeclaim "claim-log-1" deleted
        ^C
        controlplane ~ ‚úñ k get pvc
        NAME          STATUS        VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
        claim-log-1   Terminating   pv-log   100Mi      RWX                           <unset>                 19m

        Answer: option2:  The PVC is stuck in 'terminating' state


        Why is the PVC stuck in Terminating state?
        option1: The PVC is wating for the PV to be deleted
        option2: The PVC is being used by a Pod
        option3: The PVC is in the process of scrubbing

        Hint: The PVC was still being used by the webapp pod when we issued the delete command. Until the pod is deleted, the PVC will remain in a terminating state.

        Answer: option2 The PVC is being used by a Pod


        Let us now delete the webapp Pod.
        Once deleted, wait for the pod to fully terminate.

        controlplane ~ ‚ûú  k delete pod webapp 
        pod "webapp" deleted

        controlplane ~ ‚ûú  k get pvc
        No resources found in default namespace.


        What is the state of the PVC now?
        Answer: deleted


        What is the state of the Persistent Volume now?
        Hint: Run the command: kubectl get pv and look under the status section.

        Answer: Released

Optional Topics on Udemy: (Not int the CKAD field)
Storage Classess
StatefulSets
Headless Service

